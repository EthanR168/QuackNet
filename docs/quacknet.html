<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="pdoc 15.0.4"/>
    <title>quacknet API documentation</title>

    <style>/*! * Bootstrap Reboot v5.0.0 (https://getbootstrap.com/) * Copyright 2011-2021 The Bootstrap Authors * Copyright 2011-2021 Twitter, Inc. * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE) * Forked from Normalize.css, licensed MIT (https://github.com/necolas/normalize.css/blob/master/LICENSE.md) */*,::after,::before{box-sizing:border-box}@media (prefers-reduced-motion:no-preference){:root{scroll-behavior:smooth}}body{margin:0;font-family:system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans","Liberation Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-size:1rem;font-weight:400;line-height:1.5;color:#212529;background-color:#fff;-webkit-text-size-adjust:100%;-webkit-tap-highlight-color:transparent}hr{margin:1rem 0;color:inherit;background-color:currentColor;border:0;opacity:.25}hr:not([size]){height:1px}h1,h2,h3,h4,h5,h6{margin-top:0;margin-bottom:.5rem;font-weight:500;line-height:1.2}h1{font-size:calc(1.375rem + 1.5vw)}@media (min-width:1200px){h1{font-size:2.5rem}}h2{font-size:calc(1.325rem + .9vw)}@media (min-width:1200px){h2{font-size:2rem}}h3{font-size:calc(1.3rem + .6vw)}@media (min-width:1200px){h3{font-size:1.75rem}}h4{font-size:calc(1.275rem + .3vw)}@media (min-width:1200px){h4{font-size:1.5rem}}h5{font-size:1.25rem}h6{font-size:1rem}p{margin-top:0;margin-bottom:1rem}abbr[data-bs-original-title],abbr[title]{-webkit-text-decoration:underline dotted;text-decoration:underline dotted;cursor:help;-webkit-text-decoration-skip-ink:none;text-decoration-skip-ink:none}address{margin-bottom:1rem;font-style:normal;line-height:inherit}ol,ul{padding-left:2rem}dl,ol,ul{margin-top:0;margin-bottom:1rem}ol ol,ol ul,ul ol,ul ul{margin-bottom:0}dt{font-weight:700}dd{margin-bottom:.5rem;margin-left:0}blockquote{margin:0 0 1rem}b,strong{font-weight:bolder}small{font-size:.875em}mark{padding:.2em;background-color:#fcf8e3}sub,sup{position:relative;font-size:.75em;line-height:0;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}a{color:#0d6efd;text-decoration:underline}a:hover{color:#0a58ca}a:not([href]):not([class]),a:not([href]):not([class]):hover{color:inherit;text-decoration:none}code,kbd,pre,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;font-size:1em;direction:ltr;unicode-bidi:bidi-override}pre{display:block;margin-top:0;margin-bottom:1rem;overflow:auto;font-size:.875em}pre code{font-size:inherit;color:inherit;word-break:normal}code{font-size:.875em;color:#d63384;word-wrap:break-word}a>code{color:inherit}kbd{padding:.2rem .4rem;font-size:.875em;color:#fff;background-color:#212529;border-radius:.2rem}kbd kbd{padding:0;font-size:1em;font-weight:700}figure{margin:0 0 1rem}img,svg{vertical-align:middle}table{caption-side:bottom;border-collapse:collapse}caption{padding-top:.5rem;padding-bottom:.5rem;color:#6c757d;text-align:left}th{text-align:inherit;text-align:-webkit-match-parent}tbody,td,tfoot,th,thead,tr{border-color:inherit;border-style:solid;border-width:0}label{display:inline-block}button{border-radius:0}button:focus:not(:focus-visible){outline:0}button,input,optgroup,select,textarea{margin:0;font-family:inherit;font-size:inherit;line-height:inherit}button,select{text-transform:none}[role=button]{cursor:pointer}select{word-wrap:normal}select:disabled{opacity:1}[list]::-webkit-calendar-picker-indicator{display:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]:not(:disabled),[type=reset]:not(:disabled),[type=submit]:not(:disabled),button:not(:disabled){cursor:pointer}::-moz-focus-inner{padding:0;border-style:none}textarea{resize:vertical}fieldset{min-width:0;padding:0;margin:0;border:0}legend{float:left;width:100%;padding:0;margin-bottom:.5rem;font-size:calc(1.275rem + .3vw);line-height:inherit}@media (min-width:1200px){legend{font-size:1.5rem}}legend+*{clear:left}::-webkit-datetime-edit-day-field,::-webkit-datetime-edit-fields-wrapper,::-webkit-datetime-edit-hour-field,::-webkit-datetime-edit-minute,::-webkit-datetime-edit-month-field,::-webkit-datetime-edit-text,::-webkit-datetime-edit-year-field{padding:0}::-webkit-inner-spin-button{height:auto}[type=search]{outline-offset:-2px;-webkit-appearance:textfield}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-color-swatch-wrapper{padding:0}::file-selector-button{font:inherit}::-webkit-file-upload-button{font:inherit;-webkit-appearance:button}output{display:inline-block}iframe{border:0}summary{display:list-item;cursor:pointer}progress{vertical-align:baseline}[hidden]{display:none!important}</style>
    <style>/*! syntax-highlighting.css */pre{line-height:125%;}span.linenos{color:inherit; background-color:transparent; padding-left:5px; padding-right:20px;}.pdoc-code .hll{background-color:#ffffcc}.pdoc-code{background:#f8f8f8;}.pdoc-code .c{color:#3D7B7B; font-style:italic}.pdoc-code .err{border:1px solid #FF0000}.pdoc-code .k{color:#008000; font-weight:bold}.pdoc-code .o{color:#666666}.pdoc-code .ch{color:#3D7B7B; font-style:italic}.pdoc-code .cm{color:#3D7B7B; font-style:italic}.pdoc-code .cp{color:#9C6500}.pdoc-code .cpf{color:#3D7B7B; font-style:italic}.pdoc-code .c1{color:#3D7B7B; font-style:italic}.pdoc-code .cs{color:#3D7B7B; font-style:italic}.pdoc-code .gd{color:#A00000}.pdoc-code .ge{font-style:italic}.pdoc-code .gr{color:#E40000}.pdoc-code .gh{color:#000080; font-weight:bold}.pdoc-code .gi{color:#008400}.pdoc-code .go{color:#717171}.pdoc-code .gp{color:#000080; font-weight:bold}.pdoc-code .gs{font-weight:bold}.pdoc-code .gu{color:#800080; font-weight:bold}.pdoc-code .gt{color:#0044DD}.pdoc-code .kc{color:#008000; font-weight:bold}.pdoc-code .kd{color:#008000; font-weight:bold}.pdoc-code .kn{color:#008000; font-weight:bold}.pdoc-code .kp{color:#008000}.pdoc-code .kr{color:#008000; font-weight:bold}.pdoc-code .kt{color:#B00040}.pdoc-code .m{color:#666666}.pdoc-code .s{color:#BA2121}.pdoc-code .na{color:#687822}.pdoc-code .nb{color:#008000}.pdoc-code .nc{color:#0000FF; font-weight:bold}.pdoc-code .no{color:#880000}.pdoc-code .nd{color:#AA22FF}.pdoc-code .ni{color:#717171; font-weight:bold}.pdoc-code .ne{color:#CB3F38; font-weight:bold}.pdoc-code .nf{color:#0000FF}.pdoc-code .nl{color:#767600}.pdoc-code .nn{color:#0000FF; font-weight:bold}.pdoc-code .nt{color:#008000; font-weight:bold}.pdoc-code .nv{color:#19177C}.pdoc-code .ow{color:#AA22FF; font-weight:bold}.pdoc-code .w{color:#bbbbbb}.pdoc-code .mb{color:#666666}.pdoc-code .mf{color:#666666}.pdoc-code .mh{color:#666666}.pdoc-code .mi{color:#666666}.pdoc-code .mo{color:#666666}.pdoc-code .sa{color:#BA2121}.pdoc-code .sb{color:#BA2121}.pdoc-code .sc{color:#BA2121}.pdoc-code .dl{color:#BA2121}.pdoc-code .sd{color:#BA2121; font-style:italic}.pdoc-code .s2{color:#BA2121}.pdoc-code .se{color:#AA5D1F; font-weight:bold}.pdoc-code .sh{color:#BA2121}.pdoc-code .si{color:#A45A77; font-weight:bold}.pdoc-code .sx{color:#008000}.pdoc-code .sr{color:#A45A77}.pdoc-code .s1{color:#BA2121}.pdoc-code .ss{color:#19177C}.pdoc-code .bp{color:#008000}.pdoc-code .fm{color:#0000FF}.pdoc-code .vc{color:#19177C}.pdoc-code .vg{color:#19177C}.pdoc-code .vi{color:#19177C}.pdoc-code .vm{color:#19177C}.pdoc-code .il{color:#666666}</style>
    <style>/*! theme.css */:root{--pdoc-background:#fff;}.pdoc{--text:#212529;--muted:#6c757d;--link:#3660a5;--link-hover:#1659c5;--code:#f8f8f8;--active:#fff598;--accent:#eee;--accent2:#c1c1c1;--nav-hover:rgba(255, 255, 255, 0.5);--name:#0066BB;--def:#008800;--annotation:#007020;}</style>
    <style>/*! layout.css */html, body{width:100%;height:100%;}html, main{scroll-behavior:smooth;}body{background-color:var(--pdoc-background);}@media (max-width:769px){#navtoggle{cursor:pointer;position:absolute;width:50px;height:40px;top:1rem;right:1rem;border-color:var(--text);color:var(--text);display:flex;opacity:0.8;z-index:999;}#navtoggle:hover{opacity:1;}#togglestate + div{display:none;}#togglestate:checked + div{display:inherit;}main, header{padding:2rem 3vw;}header + main{margin-top:-3rem;}.git-button{display:none !important;}nav input[type="search"]{max-width:77%;}nav input[type="search"]:first-child{margin-top:-6px;}nav input[type="search"]:valid ~ *{display:none !important;}}@media (min-width:770px){:root{--sidebar-width:clamp(12.5rem, 28vw, 22rem);}nav{position:fixed;overflow:auto;height:100vh;width:var(--sidebar-width);}main, header{padding:3rem 2rem 3rem calc(var(--sidebar-width) + 3rem);width:calc(54rem + var(--sidebar-width));max-width:100%;}header + main{margin-top:-4rem;}#navtoggle{display:none;}}#togglestate{position:absolute;height:0;opacity:0;}nav.pdoc{--pad:clamp(0.5rem, 2vw, 1.75rem);--indent:1.5rem;background-color:var(--accent);border-right:1px solid var(--accent2);box-shadow:0 0 20px rgba(50, 50, 50, .2) inset;padding:0 0 0 var(--pad);overflow-wrap:anywhere;scrollbar-width:thin; scrollbar-color:var(--accent2) transparent; z-index:1}nav.pdoc::-webkit-scrollbar{width:.4rem; }nav.pdoc::-webkit-scrollbar-thumb{background-color:var(--accent2); }nav.pdoc > div{padding:var(--pad) 0;}nav.pdoc .module-list-button{display:inline-flex;align-items:center;color:var(--text);border-color:var(--muted);margin-bottom:1rem;}nav.pdoc .module-list-button:hover{border-color:var(--text);}nav.pdoc input[type=search]{display:block;outline-offset:0;width:calc(100% - var(--pad));}nav.pdoc .logo{max-width:calc(100% - var(--pad));max-height:35vh;display:block;margin:0 auto 1rem;transform:translate(calc(-.5 * var(--pad)), 0);}nav.pdoc ul{list-style:none;padding-left:0;}nav.pdoc > div > ul{margin-left:calc(0px - var(--pad));}nav.pdoc li a{padding:.2rem 0 .2rem calc(var(--pad) + var(--indent));}nav.pdoc > div > ul > li > a{padding-left:var(--pad);}nav.pdoc li{transition:all 100ms;}nav.pdoc li:hover{background-color:var(--nav-hover);}nav.pdoc a, nav.pdoc a:hover{color:var(--text);}nav.pdoc a{display:block;}nav.pdoc > h2:first-of-type{margin-top:1.5rem;}nav.pdoc .class:before{content:"class ";color:var(--muted);}nav.pdoc .function:after{content:"()";color:var(--muted);}nav.pdoc footer:before{content:"";display:block;width:calc(100% - var(--pad));border-top:solid var(--accent2) 1px;margin-top:1.5rem;padding-top:.5rem;}nav.pdoc footer{font-size:small;}</style>
    <style>/*! content.css */.pdoc{color:var(--text);box-sizing:border-box;line-height:1.5;background:none;}.pdoc .pdoc-button{cursor:pointer;display:inline-block;border:solid black 1px;border-radius:2px;font-size:.75rem;padding:calc(0.5em - 1px) 1em;transition:100ms all;}.pdoc .alert{padding:1rem 1rem 1rem calc(1.5rem + 24px);border:1px solid transparent;border-radius:.25rem;background-repeat:no-repeat;background-position:.75rem center;margin-bottom:1rem;}.pdoc .alert > em{display:none;}.pdoc .alert > *:last-child{margin-bottom:0;}.pdoc .alert.note{color:#084298;background-color:#cfe2ff;border-color:#b6d4fe;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23084298%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8%2016A8%208%200%201%200%208%200a8%208%200%200%200%200%2016zm.93-9.412-1%204.705c-.07.34.029.533.304.533.194%200%20.487-.07.686-.246l-.088.416c-.287.346-.92.598-1.465.598-.703%200-1.002-.422-.808-1.319l.738-3.468c.064-.293.006-.399-.287-.47l-.451-.081.082-.381%202.29-.287zM8%205.5a1%201%200%201%201%200-2%201%201%200%200%201%200%202z%22/%3E%3C/svg%3E");}.pdoc .alert.tip{color:#0a3622;background-color:#d1e7dd;border-color:#a3cfbb;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%230a3622%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M2%206a6%206%200%201%201%2010.174%204.31c-.203.196-.359.4-.453.619l-.762%201.769A.5.5%200%200%201%2010.5%2013a.5.5%200%200%201%200%201%20.5.5%200%200%201%200%201l-.224.447a1%201%200%200%201-.894.553H6.618a1%201%200%200%201-.894-.553L5.5%2015a.5.5%200%200%201%200-1%20.5.5%200%200%201%200-1%20.5.5%200%200%201-.46-.302l-.761-1.77a2%202%200%200%200-.453-.618A5.98%205.98%200%200%201%202%206m6-5a5%205%200%200%200-3.479%208.592c.263.254.514.564.676.941L5.83%2012h4.342l.632-1.467c.162-.377.413-.687.676-.941A5%205%200%200%200%208%201%22/%3E%3C/svg%3E");}.pdoc .alert.important{color:#055160;background-color:#cff4fc;border-color:#9eeaf9;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23055160%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M2%200a2%202%200%200%200-2%202v12a2%202%200%200%200%202%202h12a2%202%200%200%200%202-2V2a2%202%200%200%200-2-2zm6%204c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%204.995A.905.905%200%200%201%208%204m.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2%22/%3E%3C/svg%3E");}.pdoc .alert.warning{color:#664d03;background-color:#fff3cd;border-color:#ffecb5;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23664d03%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8.982%201.566a1.13%201.13%200%200%200-1.96%200L.165%2013.233c-.457.778.091%201.767.98%201.767h13.713c.889%200%201.438-.99.98-1.767L8.982%201.566zM8%205c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%205.995A.905.905%200%200%201%208%205zm.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2z%22/%3E%3C/svg%3E");}.pdoc .alert.caution{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M11.46.146A.5.5%200%200%200%2011.107%200H4.893a.5.5%200%200%200-.353.146L.146%204.54A.5.5%200%200%200%200%204.893v6.214a.5.5%200%200%200%20.146.353l4.394%204.394a.5.5%200%200%200%20.353.146h6.214a.5.5%200%200%200%20.353-.146l4.394-4.394a.5.5%200%200%200%20.146-.353V4.893a.5.5%200%200%200-.146-.353zM8%204c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%204.995A.905.905%200%200%201%208%204m.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2%22/%3E%3C/svg%3E");}.pdoc .alert.danger{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M5.52.359A.5.5%200%200%201%206%200h4a.5.5%200%200%201%20.474.658L8.694%206H12.5a.5.5%200%200%201%20.395.807l-7%209a.5.5%200%200%201-.873-.454L6.823%209.5H3.5a.5.5%200%200%201-.48-.641l2.5-8.5z%22/%3E%3C/svg%3E");}.pdoc .visually-hidden{position:absolute !important;width:1px !important;height:1px !important;padding:0 !important;margin:-1px !important;overflow:hidden !important;clip:rect(0, 0, 0, 0) !important;white-space:nowrap !important;border:0 !important;}.pdoc h1, .pdoc h2, .pdoc h3{font-weight:300;margin:.3em 0;padding:.2em 0;}.pdoc > section:not(.module-info) h1{font-size:1.5rem;font-weight:500;}.pdoc > section:not(.module-info) h2{font-size:1.4rem;font-weight:500;}.pdoc > section:not(.module-info) h3{font-size:1.3rem;font-weight:500;}.pdoc > section:not(.module-info) h4{font-size:1.2rem;}.pdoc > section:not(.module-info) h5{font-size:1.1rem;}.pdoc a{text-decoration:none;color:var(--link);}.pdoc a:hover{color:var(--link-hover);}.pdoc blockquote{margin-left:2rem;}.pdoc pre{border-top:1px solid var(--accent2);border-bottom:1px solid var(--accent2);margin-top:0;margin-bottom:1em;padding:.5rem 0 .5rem .5rem;overflow-x:auto;background-color:var(--code);}.pdoc code{color:var(--text);padding:.2em .4em;margin:0;font-size:85%;background-color:var(--accent);border-radius:6px;}.pdoc a > code{color:inherit;}.pdoc pre > code{display:inline-block;font-size:inherit;background:none;border:none;padding:0;}.pdoc > section:not(.module-info){margin-bottom:1.5rem;}.pdoc .modulename{margin-top:0;font-weight:bold;}.pdoc .modulename a{color:var(--link);transition:100ms all;}.pdoc .git-button{float:right;border:solid var(--link) 1px;}.pdoc .git-button:hover{background-color:var(--link);color:var(--pdoc-background);}.view-source-toggle-state,.view-source-toggle-state ~ .pdoc-code{display:none;}.view-source-toggle-state:checked ~ .pdoc-code{display:block;}.view-source-button{display:inline-block;float:right;font-size:.75rem;line-height:1.5rem;color:var(--muted);padding:0 .4rem 0 1.3rem;cursor:pointer;text-indent:-2px;}.view-source-button > span{visibility:hidden;}.module-info .view-source-button{float:none;display:flex;justify-content:flex-end;margin:-1.2rem .4rem -.2rem 0;}.view-source-button::before{position:absolute;content:"View Source";display:list-item;list-style-type:disclosure-closed;}.view-source-toggle-state:checked ~ .attr .view-source-button::before,.view-source-toggle-state:checked ~ .view-source-button::before{list-style-type:disclosure-open;}.pdoc .docstring{margin-bottom:1.5rem;}.pdoc section:not(.module-info) .docstring{margin-left:clamp(0rem, 5vw - 2rem, 1rem);}.pdoc .docstring .pdoc-code{margin-left:1em;margin-right:1em;}.pdoc h1:target,.pdoc h2:target,.pdoc h3:target,.pdoc h4:target,.pdoc h5:target,.pdoc h6:target,.pdoc .pdoc-code > pre > span:target{background-color:var(--active);box-shadow:-1rem 0 0 0 var(--active);}.pdoc .pdoc-code > pre > span:target{display:block;}.pdoc div:target > .attr,.pdoc section:target > .attr,.pdoc dd:target > a{background-color:var(--active);}.pdoc *{scroll-margin:2rem;}.pdoc .pdoc-code .linenos{user-select:none;}.pdoc .attr:hover{filter:contrast(0.95);}.pdoc section, .pdoc .classattr{position:relative;}.pdoc .headerlink{--width:clamp(1rem, 3vw, 2rem);position:absolute;top:0;left:calc(0rem - var(--width));transition:all 100ms ease-in-out;opacity:0;}.pdoc .headerlink::before{content:"#";display:block;text-align:center;width:var(--width);height:2.3rem;line-height:2.3rem;font-size:1.5rem;}.pdoc .attr:hover ~ .headerlink,.pdoc *:target > .headerlink,.pdoc .headerlink:hover{opacity:1;}.pdoc .attr{display:block;margin:.5rem 0 .5rem;padding:.4rem .4rem .4rem 1rem;background-color:var(--accent);overflow-x:auto;}.pdoc .classattr{margin-left:2rem;}.pdoc .decorator-deprecated{color:#842029;}.pdoc .decorator-deprecated ~ span{filter:grayscale(1) opacity(0.8);}.pdoc .name{color:var(--name);font-weight:bold;}.pdoc .def{color:var(--def);font-weight:bold;}.pdoc .signature{background-color:transparent;}.pdoc .param, .pdoc .return-annotation{white-space:pre;}.pdoc .signature.multiline .param{display:block;}.pdoc .signature.condensed .param{display:inline-block;}.pdoc .annotation{color:var(--annotation);}.pdoc .view-value-toggle-state,.pdoc .view-value-toggle-state ~ .default_value{display:none;}.pdoc .view-value-toggle-state:checked ~ .default_value{display:inherit;}.pdoc .view-value-button{font-size:.5rem;vertical-align:middle;border-style:dashed;margin-top:-0.1rem;}.pdoc .view-value-button:hover{background:white;}.pdoc .view-value-button::before{content:"show";text-align:center;width:2.2em;display:inline-block;}.pdoc .view-value-toggle-state:checked ~ .view-value-button::before{content:"hide";}.pdoc .inherited{margin-left:2rem;}.pdoc .inherited dt{font-weight:700;}.pdoc .inherited dt, .pdoc .inherited dd{display:inline;margin-left:0;margin-bottom:.5rem;}.pdoc .inherited dd:not(:last-child):after{content:", ";}.pdoc .inherited .class:before{content:"class ";}.pdoc .inherited .function a:after{content:"()";}.pdoc .search-result .docstring{overflow:auto;max-height:25vh;}.pdoc .search-result.focused > .attr{background-color:var(--active);}.pdoc .attribution{margin-top:2rem;display:block;opacity:0.5;transition:all 200ms;filter:grayscale(100%);}.pdoc .attribution:hover{opacity:1;filter:grayscale(0%);}.pdoc .attribution img{margin-left:5px;height:35px;vertical-align:middle;width:70px;transition:all 200ms;}.pdoc table{display:block;width:max-content;max-width:100%;overflow:auto;margin-bottom:1rem;}.pdoc table th{font-weight:600;}.pdoc table th, .pdoc table td{padding:6px 13px;border:1px solid var(--accent2);}</style>
    <style>/*! custom.css */</style></head>
<body>
    <nav class="pdoc">
        <label id="navtoggle" for="togglestate" class="pdoc-button"><svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 30 30'><path stroke-linecap='round' stroke="currentColor" stroke-miterlimit='10' stroke-width='2' d='M4 7h22M4 15h22M4 23h22'/></svg></label>
        <input id="togglestate" type="checkbox" aria-hidden="true" tabindex="-1">
        <div>

            <input type="search" placeholder="Search..." role="searchbox" aria-label="search"
                   pattern=".+" required>

            <h2>Contents</h2>
            <ul>
  <li><a href="#quacknet">QuackNet</a>
  <ul>
    <li><a href="#key-features">Key Features</a></li>
    <li><a href="#installation">Installation</a></li>
    <li><a href="#usage-example">Usage Example</a></li>
    <li><a href="#examples">Examples</a></li>
    <li><a href="#highlights">Highlights</a></li>
    <li><a href="#code-structure">Code structure</a></li>
    <li><a href="#related-projects">Related Projects</a></li>
    <li><a href="#license">License</a></li>
  </ul></li>
</ul>


            <h2>Submodules</h2>
            <ul>
                    <li><a href="quacknet/Transformer.html">Transformer</a></li>
            </ul>

            <h2>API Documentation</h2>
                <ul class="memberlist">
            <li>
                    <a class="class" href="#Network">Network</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#Network.__init__">Network</a>
                        </li>
                        <li>
                                <a class="variable" href="#Network.layers">layers</a>
                        </li>
                        <li>
                                <a class="variable" href="#Network.weights">weights</a>
                        </li>
                        <li>
                                <a class="variable" href="#Network.biases">biases</a>
                        </li>
                        <li>
                                <a class="variable" href="#Network.learningRate">learningRate</a>
                        </li>
                        <li>
                                <a class="variable" href="#Network.lossFunction">lossFunction</a>
                        </li>
                        <li>
                                <a class="variable" href="#Network.optimisationFunction">optimisationFunction</a>
                        </li>
                        <li>
                                <a class="variable" href="#Network.useBatches">useBatches</a>
                        </li>
                        <li>
                                <a class="variable" href="#Network.batchSize">batchSize</a>
                        </li>
                        <li>
                                <a class="function" href="#Network.addLayer">addLayer</a>
                        </li>
                        <li>
                                <a class="function" href="#Network.forwardPropagation">forwardPropagation</a>
                        </li>
                        <li>
                                <a class="function" href="#Network.optimise">optimise</a>
                        </li>
                        <li>
                                <a class="function" href="#Network.train">train</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#CNNModel">CNNModel</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#CNNModel.__init__">CNNModel</a>
                        </li>
                        <li>
                                <a class="variable" href="#CNNModel.layers">layers</a>
                        </li>
                        <li>
                                <a class="variable" href="#CNNModel.weights">weights</a>
                        </li>
                        <li>
                                <a class="variable" href="#CNNModel.biases">biases</a>
                        </li>
                        <li>
                                <a class="variable" href="#CNNModel.NeuralNetworkClass">NeuralNetworkClass</a>
                        </li>
                        <li>
                                <a class="variable" href="#CNNModel.optimisationFunction">optimisationFunction</a>
                        </li>
                        <li>
                                <a class="function" href="#CNNModel.addLayer">addLayer</a>
                        </li>
                        <li>
                                <a class="function" href="#CNNModel.forward">forward</a>
                        </li>
                        <li>
                                <a class="function" href="#CNNModel.train">train</a>
                        </li>
                        <li>
                                <a class="function" href="#CNNModel.createWeightsBiases">createWeightsBiases</a>
                        </li>
                        <li>
                                <a class="function" href="#CNNModel.saveModel">saveModel</a>
                        </li>
                        <li>
                                <a class="function" href="#CNNModel.loadModel">loadModel</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#Conv1DLayer">Conv1DLayer</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#Conv1DLayer.__init__">Conv1DLayer</a>
                        </li>
                        <li>
                                <a class="variable" href="#Conv1DLayer.kernalSize">kernalSize</a>
                        </li>
                        <li>
                                <a class="variable" href="#Conv1DLayer.numKernals">numKernals</a>
                        </li>
                        <li>
                                <a class="variable" href="#Conv1DLayer.kernalWeights">kernalWeights</a>
                        </li>
                        <li>
                                <a class="variable" href="#Conv1DLayer.kernalBiases">kernalBiases</a>
                        </li>
                        <li>
                                <a class="variable" href="#Conv1DLayer.depth">depth</a>
                        </li>
                        <li>
                                <a class="variable" href="#Conv1DLayer.stride">stride</a>
                        </li>
                        <li>
                                <a class="variable" href="#Conv1DLayer.padding">padding</a>
                        </li>
                        <li>
                                <a class="function" href="#Conv1DLayer.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#Conv2DLayer">Conv2DLayer</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#Conv2DLayer.__init__">Conv2DLayer</a>
                        </li>
                        <li>
                                <a class="variable" href="#Conv2DLayer.kernalSize">kernalSize</a>
                        </li>
                        <li>
                                <a class="variable" href="#Conv2DLayer.numKernals">numKernals</a>
                        </li>
                        <li>
                                <a class="variable" href="#Conv2DLayer.kernalWeights">kernalWeights</a>
                        </li>
                        <li>
                                <a class="variable" href="#Conv2DLayer.kernalBiases">kernalBiases</a>
                        </li>
                        <li>
                                <a class="variable" href="#Conv2DLayer.depth">depth</a>
                        </li>
                        <li>
                                <a class="variable" href="#Conv2DLayer.stride">stride</a>
                        </li>
                        <li>
                                <a class="variable" href="#Conv2DLayer.padding">padding</a>
                        </li>
                        <li>
                                <a class="function" href="#Conv2DLayer.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#ActivationLayer">ActivationLayer</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#ActivationLayer.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#DenseLayer">DenseLayer</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#DenseLayer.__init__">DenseLayer</a>
                        </li>
                        <li>
                                <a class="variable" href="#DenseLayer.NeuralNetworkClass">NeuralNetworkClass</a>
                        </li>
                        <li>
                                <a class="variable" href="#DenseLayer.orignalShape">orignalShape</a>
                        </li>
                        <li>
                                <a class="function" href="#DenseLayer.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#PoolingLayer">PoolingLayer</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#PoolingLayer.__init__">PoolingLayer</a>
                        </li>
                        <li>
                                <a class="variable" href="#PoolingLayer.gridSize">gridSize</a>
                        </li>
                        <li>
                                <a class="variable" href="#PoolingLayer.stride">stride</a>
                        </li>
                        <li>
                                <a class="variable" href="#PoolingLayer.mode">mode</a>
                        </li>
                        <li>
                                <a class="function" href="#PoolingLayer.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#GlobalAveragePooling">GlobalAveragePooling</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#GlobalAveragePooling.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#StackedRNN">StackedRNN</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#StackedRNN.__init__">StackedRNN</a>
                        </li>
                        <li>
                                <a class="variable" href="#StackedRNN.inputWeights">inputWeights</a>
                        </li>
                        <li>
                                <a class="variable" href="#StackedRNN.hiddenWeights">hiddenWeights</a>
                        </li>
                        <li>
                                <a class="variable" href="#StackedRNN.biases">biases</a>
                        </li>
                        <li>
                                <a class="variable" href="#StackedRNN.outputWeight">outputWeight</a>
                        </li>
                        <li>
                                <a class="variable" href="#StackedRNN.outputBias">outputBias</a>
                        </li>
                        <li>
                                <a class="variable" href="#StackedRNN.hiddenStates">hiddenStates</a>
                        </li>
                        <li>
                                <a class="variable" href="#StackedRNN.hiddenStateActivationFunction">hiddenStateActivationFunction</a>
                        </li>
                        <li>
                                <a class="variable" href="#StackedRNN.outputLayerActivationFunction">outputLayerActivationFunction</a>
                        </li>
                        <li>
                                <a class="variable" href="#StackedRNN.activationDerivative">activationDerivative</a>
                        </li>
                        <li>
                                <a class="variable" href="#StackedRNN.outputLayerDerivative">outputLayerDerivative</a>
                        </li>
                        <li>
                                <a class="variable" href="#StackedRNN.lossFunction">lossFunction</a>
                        </li>
                        <li>
                                <a class="variable" href="#StackedRNN.lossDerivative">lossDerivative</a>
                        </li>
                        <li>
                                <a class="variable" href="#StackedRNN.useBatches">useBatches</a>
                        </li>
                        <li>
                                <a class="variable" href="#StackedRNN.numberOfHiddenStates">numberOfHiddenStates</a>
                        </li>
                        <li>
                                <a class="variable" href="#StackedRNN.hiddenSizes">hiddenSizes</a>
                        </li>
                        <li>
                                <a class="variable" href="#StackedRNN.adam">adam</a>
                        </li>
                        <li>
                                <a class="function" href="#StackedRNN.forwardSequence">forwardSequence</a>
                        </li>
                        <li>
                                <a class="function" href="#StackedRNN.initialiseWeights">initialiseWeights</a>
                        </li>
                        <li>
                                <a class="function" href="#StackedRNN.backwardPropagation">backwardPropagation</a>
                        </li>
                        <li>
                                <a class="function" href="#StackedRNN.optimiser">optimiser</a>
                        </li>
                        <li>
                                <a class="function" href="#StackedRNN.train">train</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#SingularRNN">SingularRNN</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#SingularRNN.__init__">SingularRNN</a>
                        </li>
                        <li>
                                <a class="variable" href="#SingularRNN.inputWeight">inputWeight</a>
                        </li>
                        <li>
                                <a class="variable" href="#SingularRNN.hiddenWeight">hiddenWeight</a>
                        </li>
                        <li>
                                <a class="variable" href="#SingularRNN.bias">bias</a>
                        </li>
                        <li>
                                <a class="variable" href="#SingularRNN.outputWeight">outputWeight</a>
                        </li>
                        <li>
                                <a class="variable" href="#SingularRNN.outputBias">outputBias</a>
                        </li>
                        <li>
                                <a class="variable" href="#SingularRNN.hiddenState">hiddenState</a>
                        </li>
                        <li>
                                <a class="variable" href="#SingularRNN.hiddenStateActivationFunction">hiddenStateActivationFunction</a>
                        </li>
                        <li>
                                <a class="variable" href="#SingularRNN.outputLayerActivationFunction">outputLayerActivationFunction</a>
                        </li>
                        <li>
                                <a class="variable" href="#SingularRNN.activationDerivative">activationDerivative</a>
                        </li>
                        <li>
                                <a class="variable" href="#SingularRNN.outputLayerDerivative">outputLayerDerivative</a>
                        </li>
                        <li>
                                <a class="variable" href="#SingularRNN.lossFunction">lossFunction</a>
                        </li>
                        <li>
                                <a class="variable" href="#SingularRNN.lossDerivative">lossDerivative</a>
                        </li>
                        <li>
                                <a class="variable" href="#SingularRNN.useBatches">useBatches</a>
                        </li>
                        <li>
                                <a class="variable" href="#SingularRNN.batchSize">batchSize</a>
                        </li>
                        <li>
                                <a class="variable" href="#SingularRNN.adam">adam</a>
                        </li>
                        <li>
                                <a class="function" href="#SingularRNN.forwardSequence">forwardSequence</a>
                        </li>
                        <li>
                                <a class="function" href="#SingularRNN.initialiseWeights">initialiseWeights</a>
                        </li>
                        <li>
                                <a class="function" href="#SingularRNN.backwardPropagation">backwardPropagation</a>
                        </li>
                        <li>
                                <a class="function" href="#SingularRNN.optimiser">optimiser</a>
                        </li>
                        <li>
                                <a class="function" href="#SingularRNN.train">train</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#Transformer">Transformer</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#Transformer.__init__">Transformer</a>
                        </li>
                        <li>
                                <a class="variable" href="#Transformer.adam">adam</a>
                        </li>
                        <li>
                                <a class="variable" href="#Transformer.blocks">blocks</a>
                        </li>
                        <li>
                                <a class="variable" href="#Transformer.hasDecoderBlock">hasDecoderBlock</a>
                        </li>
                        <li>
                                <a class="function" href="#Transformer.addBlock">addBlock</a>
                        </li>
                        <li>
                                <a class="function" href="#Transformer.forwardPropagation">forwardPropagation</a>
                        </li>
                        <li>
                                <a class="function" href="#Transformer.backwardPropagation">backwardPropagation</a>
                        </li>
                        <li>
                                <a class="function" href="#Transformer.optimiser">optimiser</a>
                        </li>
                        <li>
                                <a class="function" href="#Transformer.saveWeights">saveWeights</a>
                        </li>
                        <li>
                                <a class="function" href="#Transformer.loadWeights">loadWeights</a>
                        </li>
                        <li>
                                <a class="function" href="#Transformer.train">train</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#TransformerBlock">TransformerBlock</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#TransformerBlock.__init__">TransformerBlock</a>
                        </li>
                        <li>
                                <a class="variable" href="#TransformerBlock.batchSize">batchSize</a>
                        </li>
                        <li>
                                <a class="variable" href="#TransformerBlock.sequenceLength">sequenceLength</a>
                        </li>
                        <li>
                                <a class="variable" href="#TransformerBlock.vocabSize">vocabSize</a>
                        </li>
                        <li>
                                <a class="variable" href="#TransformerBlock.embedDimension">embedDimension</a>
                        </li>
                        <li>
                                <a class="variable" href="#TransformerBlock.numberHeads">numberHeads</a>
                        </li>
                        <li>
                                <a class="variable" href="#TransformerBlock.hiddenDimensionFFN">hiddenDimensionFFN</a>
                        </li>
                        <li>
                                <a class="variable" href="#TransformerBlock.useResidual">useResidual</a>
                        </li>
                        <li>
                                <a class="variable" href="#TransformerBlock.useNorm">useNorm</a>
                        </li>
                        <li>
                                <a class="variable" href="#TransformerBlock.positionalEmbddingDimension">positionalEmbddingDimension</a>
                        </li>
                        <li>
                                <a class="variable" href="#TransformerBlock.isDecoder">isDecoder</a>
                        </li>
                        <li>
                                <a class="variable" href="#TransformerBlock.embedding">embedding</a>
                        </li>
                        <li>
                                <a class="variable" href="#TransformerBlock.positionalEncoding">positionalEncoding</a>
                        </li>
                        <li>
                                <a class="variable" href="#TransformerBlock.attention">attention</a>
                        </li>
                        <li>
                                <a class="variable" href="#TransformerBlock.FFN">FFN</a>
                        </li>
                        <li>
                                <a class="variable" href="#TransformerBlock.adam">adam</a>
                        </li>
                        <li>
                                <a class="function" href="#TransformerBlock.forwardPropagation">forwardPropagation</a>
                        </li>
                        <li>
                                <a class="function" href="#TransformerBlock.backwardPropagation">backwardPropagation</a>
                        </li>
                        <li>
                                <a class="function" href="#TransformerBlock.blockBackPropagation">blockBackPropagation</a>
                        </li>
                        <li>
                                <a class="function" href="#TransformerBlock.optimiser">optimiser</a>
                        </li>
                        <li>
                                <a class="function" href="#TransformerBlock.train">train</a>
                        </li>
                        <li>
                                <a class="function" href="#TransformerBlock.saveWeights">saveWeights</a>
                        </li>
                        <li>
                                <a class="function" href="#TransformerBlock.loadWeights">loadWeights</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#MultiAttentionHeadLayer">MultiAttentionHeadLayer</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#MultiAttentionHeadLayer.__init__">MultiAttentionHeadLayer</a>
                        </li>
                        <li>
                                <a class="variable" href="#MultiAttentionHeadLayer.embedDimension">embedDimension</a>
                        </li>
                        <li>
                                <a class="variable" href="#MultiAttentionHeadLayer.numberOfHeads">numberOfHeads</a>
                        </li>
                        <li>
                                <a class="variable" href="#MultiAttentionHeadLayer.QueryWeights">QueryWeights</a>
                        </li>
                        <li>
                                <a class="variable" href="#MultiAttentionHeadLayer.KeyWeights">KeyWeights</a>
                        </li>
                        <li>
                                <a class="variable" href="#MultiAttentionHeadLayer.ValueWeights">ValueWeights</a>
                        </li>
                        <li>
                                <a class="variable" href="#MultiAttentionHeadLayer.outputWeight">outputWeight</a>
                        </li>
                        <li>
                                <a class="variable" href="#MultiAttentionHeadLayer.outputBias">outputBias</a>
                        </li>
                        <li>
                                <a class="variable" href="#MultiAttentionHeadLayer.batchSize">batchSize</a>
                        </li>
                        <li>
                                <a class="variable" href="#MultiAttentionHeadLayer.sequenceLength">sequenceLength</a>
                        </li>
                        <li>
                                <a class="variable" href="#MultiAttentionHeadLayer.useCasualMasking">useCasualMasking</a>
                        </li>
                        <li>
                                <a class="variable" href="#MultiAttentionHeadLayer.usePaddingMask">usePaddingMask</a>
                        </li>
                        <li>
                                <a class="variable" href="#MultiAttentionHeadLayer.paddingMask">paddingMask</a>
                        </li>
                        <li>
                                <a class="variable" href="#MultiAttentionHeadLayer.batchIndex">batchIndex</a>
                        </li>
                        <li>
                                <a class="variable" href="#MultiAttentionHeadLayer.backProp">backProp</a>
                        </li>
                        <li>
                                <a class="function" href="#MultiAttentionHeadLayer.QKVLinearProjection">QKVLinearProjection</a>
                        </li>
                        <li>
                                <a class="function" href="#MultiAttentionHeadLayer.SplitIntoHeads">SplitIntoHeads</a>
                        </li>
                        <li>
                                <a class="function" href="#MultiAttentionHeadLayer.calculateAttention">calculateAttention</a>
                        </li>
                        <li>
                                <a class="function" href="#MultiAttentionHeadLayer.outputProjectionLayer">outputProjectionLayer</a>
                        </li>
                        <li>
                                <a class="function" href="#MultiAttentionHeadLayer.forwardPropagation">forwardPropagation</a>
                        </li>
                        <li>
                                <a class="function" href="#MultiAttentionHeadLayer.createWeights">createWeights</a>
                        </li>
                        <li>
                                <a class="function" href="#MultiAttentionHeadLayer.backwardPropagation">backwardPropagation</a>
                        </li>
                        <li>
                                <a class="function" href="#MultiAttentionHeadLayer.createCausalMask">createCausalMask</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#EmbeddingLayer">EmbeddingLayer</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#EmbeddingLayer.__init__">EmbeddingLayer</a>
                        </li>
                        <li>
                                <a class="variable" href="#EmbeddingLayer.vocabSize">vocabSize</a>
                        </li>
                        <li>
                                <a class="variable" href="#EmbeddingLayer.embedDimension">embedDimension</a>
                        </li>
                        <li>
                                <a class="variable" href="#EmbeddingLayer.weights">weights</a>
                        </li>
                        <li>
                                <a class="function" href="#EmbeddingLayer.forwardPropagation">forwardPropagation</a>
                        </li>
                        <li>
                                <a class="function" href="#EmbeddingLayer.backwardPropagation">backwardPropagation</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#FeedForwardNetwork">FeedForwardNetwork</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#FeedForwardNetwork.__init__">FeedForwardNetwork</a>
                        </li>
                        <li>
                                <a class="variable" href="#FeedForwardNetwork.inputDimension">inputDimension</a>
                        </li>
                        <li>
                                <a class="variable" href="#FeedForwardNetwork.hiddenDimension">hiddenDimension</a>
                        </li>
                        <li>
                                <a class="variable" href="#FeedForwardNetwork.W1">W1</a>
                        </li>
                        <li>
                                <a class="variable" href="#FeedForwardNetwork.b1">b1</a>
                        </li>
                        <li>
                                <a class="variable" href="#FeedForwardNetwork.W2">W2</a>
                        </li>
                        <li>
                                <a class="variable" href="#FeedForwardNetwork.b2">b2</a>
                        </li>
                        <li>
                                <a class="function" href="#FeedForwardNetwork.createWeights">createWeights</a>
                        </li>
                        <li>
                                <a class="function" href="#FeedForwardNetwork.forwardPropagation">forwardPropagation</a>
                        </li>
                        <li>
                                <a class="function" href="#FeedForwardNetwork.backwardPropagation">backwardPropagation</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#NormLayer">NormLayer</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#NormLayer.__init__">NormLayer</a>
                        </li>
                        <li>
                                <a class="variable" href="#NormLayer.gamma">gamma</a>
                        </li>
                        <li>
                                <a class="variable" href="#NormLayer.beta">beta</a>
                        </li>
                        <li>
                                <a class="variable" href="#NormLayer.epsilon">epsilon</a>
                        </li>
                        <li>
                                <a class="variable" href="#NormLayer.input">input</a>
                        </li>
                        <li>
                                <a class="function" href="#NormLayer.forwardPropagation">forwardPropagation</a>
                        </li>
                        <li>
                                <a class="function" href="#NormLayer.backwardPropagation">backwardPropagation</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#PositionalEncoding">PositionalEncoding</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#PositionalEncoding.__init__">PositionalEncoding</a>
                        </li>
                        <li>
                                <a class="variable" href="#PositionalEncoding.maxDimension">maxDimension</a>
                        </li>
                        <li>
                                <a class="variable" href="#PositionalEncoding.embeddingSize">embeddingSize</a>
                        </li>
                        <li>
                                <a class="variable" href="#PositionalEncoding.encoding">encoding</a>
                        </li>
                        <li>
                                <a class="function" href="#PositionalEncoding.createEmbed">createEmbed</a>
                        </li>
                        <li>
                                <a class="function" href="#PositionalEncoding.forwardPropagation">forwardPropagation</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#ResidualConnection">ResidualConnection</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#ResidualConnection.forwardPropagation">forwardPropagation</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#Dropout">Dropout</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#Dropout.__init__">Dropout</a>
                        </li>
                        <li>
                                <a class="variable" href="#Dropout.dropProbability">dropProbability</a>
                        </li>
                        <li>
                                <a class="variable" href="#Dropout.mask">mask</a>
                        </li>
                        <li>
                                <a class="function" href="#Dropout.forward">forward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#Adam">Adam</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#Adam.__init__">Adam</a>
                        </li>
                        <li>
                                <a class="variable" href="#Adam.firstMoment">firstMoment</a>
                        </li>
                        <li>
                                <a class="variable" href="#Adam.secondMoment">secondMoment</a>
                        </li>
                        <li>
                                <a class="variable" href="#Adam.t">t</a>
                        </li>
                        <li>
                                <a class="variable" href="#Adam.forwardPropagationFunction">forwardPropagationFunction</a>
                        </li>
                        <li>
                                <a class="variable" href="#Adam.backwardPropagationFunction">backwardPropagationFunction</a>
                        </li>
                        <li>
                                <a class="variable" href="#Adam.giveInputsToBackprop">giveInputsToBackprop</a>
                        </li>
                        <li>
                                <a class="function" href="#Adam.optimiser">optimiser</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#AdamW">AdamW</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#AdamW.__init__">AdamW</a>
                        </li>
                        <li>
                                <a class="variable" href="#AdamW.firstMoment">firstMoment</a>
                        </li>
                        <li>
                                <a class="variable" href="#AdamW.secondMoment">secondMoment</a>
                        </li>
                        <li>
                                <a class="variable" href="#AdamW.t">t</a>
                        </li>
                        <li>
                                <a class="variable" href="#AdamW.forwardPropagationFunction">forwardPropagationFunction</a>
                        </li>
                        <li>
                                <a class="variable" href="#AdamW.backwardPropagationFunction">backwardPropagationFunction</a>
                        </li>
                        <li>
                                <a class="variable" href="#AdamW.giveInputsToBackprop">giveInputsToBackprop</a>
                        </li>
                        <li>
                                <a class="variable" href="#AdamW.weightDecay">weightDecay</a>
                        </li>
                        <li>
                                <a class="function" href="#AdamW.optimiser">optimiser</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#SGD">SGD</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#SGD.__init__">SGD</a>
                        </li>
                        <li>
                                <a class="variable" href="#SGD.forwardPropagationFunction">forwardPropagationFunction</a>
                        </li>
                        <li>
                                <a class="variable" href="#SGD.backwardPropagationFunction">backwardPropagationFunction</a>
                        </li>
                        <li>
                                <a class="variable" href="#SGD.giveInputsToBackprop">giveInputsToBackprop</a>
                        </li>
                        <li>
                                <a class="function" href="#SGD.optimiser">optimiser</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#GD">GD</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#GD.__init__">GD</a>
                        </li>
                        <li>
                                <a class="variable" href="#GD.forwardPropagationFunction">forwardPropagationFunction</a>
                        </li>
                        <li>
                                <a class="variable" href="#GD.backwardPropagationFunction">backwardPropagationFunction</a>
                        </li>
                        <li>
                                <a class="variable" href="#GD.giveInputsToBackprop">giveInputsToBackprop</a>
                        </li>
                        <li>
                                <a class="function" href="#GD.optimiser">optimiser</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#RMSProp">RMSProp</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#RMSProp.__init__">RMSProp</a>
                        </li>
                        <li>
                                <a class="variable" href="#RMSProp.squaredAvg">squaredAvg</a>
                        </li>
                        <li>
                                <a class="variable" href="#RMSProp.forwardPropagationFunction">forwardPropagationFunction</a>
                        </li>
                        <li>
                                <a class="variable" href="#RMSProp.backwardPropagationFunction">backwardPropagationFunction</a>
                        </li>
                        <li>
                                <a class="variable" href="#RMSProp.giveInputsToBackprop">giveInputsToBackprop</a>
                        </li>
                        <li>
                                <a class="variable" href="#RMSProp.decay">decay</a>
                        </li>
                        <li>
                                <a class="variable" href="#RMSProp.epsilon">epsilon</a>
                        </li>
                        <li>
                                <a class="function" href="#RMSProp.optimiser">optimiser</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#Lion">Lion</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#Lion.__init__">Lion</a>
                        </li>
                        <li>
                                <a class="variable" href="#Lion.forward">forward</a>
                        </li>
                        <li>
                                <a class="variable" href="#Lion.backward">backward</a>
                        </li>
                        <li>
                                <a class="variable" href="#Lion.giveInputsToBackprop">giveInputsToBackprop</a>
                        </li>
                        <li>
                                <a class="variable" href="#Lion.beta1">beta1</a>
                        </li>
                        <li>
                                <a class="variable" href="#Lion.momentum">momentum</a>
                        </li>
                        <li>
                                <a class="function" href="#Lion.optimiser">optimiser</a>
                        </li>
                </ul>

            </li>
    </ul>



        <a class="attribution" title="pdoc: Python API documentation generator" href="https://pdoc.dev" target="_blank">
            built with <span class="visually-hidden">pdoc</span><img
                alt="pdoc logo"
                src="data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20role%3D%22img%22%20aria-label%3D%22pdoc%20logo%22%20width%3D%22300%22%20height%3D%22150%22%20viewBox%3D%22-1%200%2060%2030%22%3E%3Ctitle%3Epdoc%3C/title%3E%3Cpath%20d%3D%22M29.621%2021.293c-.011-.273-.214-.475-.511-.481a.5.5%200%200%200-.489.503l-.044%201.393c-.097.551-.695%201.215-1.566%201.704-.577.428-1.306.486-2.193.182-1.426-.617-2.467-1.654-3.304-2.487l-.173-.172a3.43%203.43%200%200%200-.365-.306.49.49%200%200%200-.286-.196c-1.718-1.06-4.931-1.47-7.353.191l-.219.15c-1.707%201.187-3.413%202.131-4.328%201.03-.02-.027-.49-.685-.141-1.763.233-.721.546-2.408.772-4.076.042-.09.067-.187.046-.288.166-1.347.277-2.625.241-3.351%201.378-1.008%202.271-2.586%202.271-4.362%200-.976-.272-1.935-.788-2.774-.057-.094-.122-.18-.184-.268.033-.167.052-.339.052-.516%200-1.477-1.202-2.679-2.679-2.679-.791%200-1.496.352-1.987.9a6.3%206.3%200%200%200-1.001.029c-.492-.564-1.207-.929-2.012-.929-1.477%200-2.679%201.202-2.679%202.679A2.65%202.65%200%200%200%20.97%206.554c-.383.747-.595%201.572-.595%202.41%200%202.311%201.507%204.29%203.635%205.107-.037.699-.147%202.27-.423%203.294l-.137.461c-.622%202.042-2.515%208.257%201.727%2010.643%201.614.908%203.06%201.248%204.317%201.248%202.665%200%204.492-1.524%205.322-2.401%201.476-1.559%202.886-1.854%206.491.82%201.877%201.393%203.514%201.753%204.861%201.068%202.223-1.713%202.811-3.867%203.399-6.374.077-.846.056-1.469.054-1.537zm-4.835%204.313c-.054.305-.156.586-.242.629-.034-.007-.131-.022-.307-.157-.145-.111-.314-.478-.456-.908.221.121.432.25.675.355.115.039.219.051.33.081zm-2.251-1.238c-.05.33-.158.648-.252.694-.022.001-.125-.018-.307-.157-.217-.166-.488-.906-.639-1.573.358.344.754.693%201.198%201.036zm-3.887-2.337c-.006-.116-.018-.231-.041-.342.635.145%201.189.368%201.599.625.097.231.166.481.174.642-.03.049-.055.101-.067.158-.046.013-.128.026-.298.004-.278-.037-.901-.57-1.367-1.087zm-1.127-.497c.116.306.176.625.12.71-.019.014-.117.045-.345.016-.206-.027-.604-.332-.986-.695.41-.051.816-.056%201.211-.031zm-4.535%201.535c.209.22.379.47.358.598-.006.041-.088.138-.351.234-.144.055-.539-.063-.979-.259a11.66%2011.66%200%200%200%20.972-.573zm.983-.664c.359-.237.738-.418%201.126-.554.25.237.479.548.457.694-.006.042-.087.138-.351.235-.174.064-.694-.105-1.232-.375zm-3.381%201.794c-.022.145-.061.29-.149.401-.133.166-.358.248-.69.251h-.002c-.133%200-.306-.26-.45-.621.417.091.854.07%201.291-.031zm-2.066-8.077a4.78%204.78%200%200%201-.775-.584c.172-.115.505-.254.88-.378l-.105.962zm-.331%202.302a10.32%2010.32%200%200%201-.828-.502c.202-.143.576-.328.984-.49l-.156.992zm-.45%202.157l-.701-.403c.214-.115.536-.249.891-.376a11.57%2011.57%200%200%201-.19.779zm-.181%201.716c.064.398.194.702.298.893-.194-.051-.435-.162-.736-.398.061-.119.224-.3.438-.495zM8.87%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zm-.735-.389a1.15%201.15%200%200%200-.314.783%201.16%201.16%200%200%200%201.162%201.162c.457%200%20.842-.27%201.032-.653.026.117.042.238.042.362a1.68%201.68%200%200%201-1.679%201.679%201.68%201.68%200%200%201-1.679-1.679c0-.843.626-1.535%201.436-1.654zM5.059%205.406A1.68%201.68%200%200%201%203.38%207.085a1.68%201.68%200%200%201-1.679-1.679c0-.037.009-.072.011-.109.21.3.541.508.935.508a1.16%201.16%200%200%200%201.162-1.162%201.14%201.14%200%200%200-.474-.912c.015%200%20.03-.005.045-.005.926.001%201.679.754%201.679%201.68zM3.198%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zM1.375%208.964c0-.52.103-1.035.288-1.52.466.394%201.06.64%201.717.64%201.144%200%202.116-.725%202.499-1.738.383%201.012%201.355%201.738%202.499%201.738.867%200%201.631-.421%202.121-1.062.307.605.478%201.267.478%201.942%200%202.486-2.153%204.51-4.801%204.51s-4.801-2.023-4.801-4.51zm24.342%2019.349c-.985.498-2.267.168-3.813-.979-3.073-2.281-5.453-3.199-7.813-.705-1.315%201.391-4.163%203.365-8.423.97-3.174-1.786-2.239-6.266-1.261-9.479l.146-.492c.276-1.02.395-2.457.444-3.268a6.11%206.11%200%200%200%201.18.115%206.01%206.01%200%200%200%202.536-.562l-.006.175c-.802.215-1.848.612-2.021%201.25-.079.295.021.601.274.837.219.203.415.364.598.501-.667.304-1.243.698-1.311%201.179-.02.144-.022.507.393.787.213.144.395.26.564.365-1.285.521-1.361.96-1.381%201.126-.018.142-.011.496.427.746l.854.489c-.473.389-.971.914-.999%201.429-.018.278.095.532.316.713.675.556%201.231.721%201.653.721.059%200%20.104-.014.158-.02.207.707.641%201.64%201.513%201.64h.013c.8-.008%201.236-.345%201.462-.626.173-.216.268-.457.325-.692.424.195.93.374%201.372.374.151%200%20.294-.021.423-.068.732-.27.944-.704.993-1.021.009-.061.003-.119.002-.179.266.086.538.147.789.147.15%200%20.294-.021.423-.069.542-.2.797-.489.914-.754.237.147.478.258.704.288.106.014.205.021.296.021.356%200%20.595-.101.767-.229.438.435%201.094.992%201.656%201.067.106.014.205.021.296.021a1.56%201.56%200%200%200%20.323-.035c.17.575.453%201.289.866%201.605.358.273.665.362.914.362a.99.99%200%200%200%20.421-.093%201.03%201.03%200%200%200%20.245-.164c.168.428.39.846.68%201.068.358.273.665.362.913.362a.99.99%200%200%200%20.421-.093c.317-.148.512-.448.639-.762.251.157.495.257.726.257.127%200%20.25-.024.37-.071.427-.17.706-.617.841-1.314.022-.015.047-.022.068-.038.067-.051.133-.104.196-.159-.443%201.486-1.107%202.761-2.086%203.257zM8.66%209.925a.5.5%200%201%200-1%200c0%20.653-.818%201.205-1.787%201.205s-1.787-.552-1.787-1.205a.5.5%200%201%200-1%200c0%201.216%201.25%202.205%202.787%202.205s2.787-.989%202.787-2.205zm4.4%2015.965l-.208.097c-2.661%201.258-4.708%201.436-6.086.527-1.542-1.017-1.88-3.19-1.844-4.198a.4.4%200%200%200-.385-.414c-.242-.029-.406.164-.414.385-.046%201.249.367%203.686%202.202%204.896.708.467%201.547.7%202.51.7%201.248%200%202.706-.392%204.362-1.174l.185-.086a.4.4%200%200%200%20.205-.527c-.089-.204-.326-.291-.527-.206zM9.547%202.292c.093.077.205.114.317.114a.5.5%200%200%200%20.318-.886L8.817.397a.5.5%200%200%200-.703.068.5.5%200%200%200%20.069.703l1.364%201.124zm-7.661-.065c.086%200%20.173-.022.253-.068l1.523-.893a.5.5%200%200%200-.506-.863l-1.523.892a.5.5%200%200%200-.179.685c.094.158.261.247.432.247z%22%20transform%3D%22matrix%28-1%200%200%201%2058%200%29%22%20fill%3D%22%233bb300%22/%3E%3Cpath%20d%3D%22M.3%2021.86V10.18q0-.46.02-.68.04-.22.18-.5.28-.54%201.34-.54%201.06%200%201.42.28.38.26.44.78.76-1.04%202.38-1.04%201.64%200%203.1%201.54%201.46%201.54%201.46%203.58%200%202.04-1.46%203.58-1.44%201.54-3.08%201.54-1.64%200-2.38-.92v4.04q0%20.46-.04.68-.02.22-.18.5-.14.3-.5.42-.36.12-.98.12-.62%200-1-.12-.36-.12-.52-.4-.14-.28-.18-.5-.02-.22-.02-.68zm3.96-9.42q-.46.54-.46%201.18%200%20.64.46%201.18.48.52%201.2.52.74%200%201.24-.52.52-.52.52-1.18%200-.66-.48-1.18-.48-.54-1.26-.54-.76%200-1.22.54zm14.741-8.36q.16-.3.54-.42.38-.12%201-.12.64%200%201.02.12.38.12.52.42.16.3.18.54.04.22.04.68v11.94q0%20.46-.04.7-.02.22-.18.5-.3.54-1.7.54-1.38%200-1.54-.98-.84.96-2.34.96-1.8%200-3.28-1.56-1.48-1.58-1.48-3.66%200-2.1%201.48-3.68%201.5-1.58%203.28-1.58%201.48%200%202.3%201v-4.2q0-.46.02-.68.04-.24.18-.52zm-3.24%2010.86q.52.54%201.26.54.74%200%201.22-.54.5-.54.5-1.18%200-.66-.48-1.22-.46-.56-1.26-.56-.8%200-1.28.56-.48.54-.48%201.2%200%20.66.52%201.2zm7.833-1.2q0-2.4%201.68-3.96%201.68-1.56%203.84-1.56%202.16%200%203.82%201.56%201.66%201.54%201.66%203.94%200%201.66-.86%202.96-.86%201.28-2.1%201.9-1.22.6-2.54.6-1.32%200-2.56-.64-1.24-.66-2.1-1.92-.84-1.28-.84-2.88zm4.18%201.44q.64.48%201.3.48.66%200%201.32-.5.66-.5.66-1.48%200-.98-.62-1.46-.62-.48-1.34-.48-.72%200-1.34.5-.62.5-.62%201.48%200%20.96.64%201.46zm11.412-1.44q0%20.84.56%201.32.56.46%201.18.46.64%200%201.18-.36.56-.38.9-.38.6%200%201.46%201.06.46.58.46%201.04%200%20.76-1.1%201.42-1.14.8-2.8.8-1.86%200-3.58-1.34-.82-.64-1.34-1.7-.52-1.08-.52-2.36%200-1.3.52-2.34.52-1.06%201.34-1.7%201.66-1.32%203.54-1.32.76%200%201.48.22.72.2%201.06.4l.32.2q.36.24.56.38.52.4.52.92%200%20.5-.42%201.14-.72%201.1-1.38%201.1-.38%200-1.08-.44-.36-.34-1.04-.34-.66%200-1.24.48-.58.48-.58%201.34z%22%20fill%3D%22green%22/%3E%3C/svg%3E"/>
        </a>
</div>
    </nav>
    <main class="pdoc">
            <section class="module-info">
                    <h1 class="modulename">
quacknet    </h1>

                        <div class="docstring"><h1 id="quacknet">QuackNet</h1>

<p><strong>QuackNet</strong> is a from scratch deep learning library built entirely in NumPy. 
It supports Neural Networks, CNNs, RNNs, and Transformers, providing full access to forward 
and backward propagation, manual gradient computation, and weight updates. All without relying 
on frameworks like TensorFlow or PyTorch.</p>

<p>QuackNet is designed for both educational purposes and small-scale research, letting you 
experiment with deep learning architectures while understanding the underlying math.</p>

<h2 id="key-features">Key Features</h2>

<p><strong>1. Core Functionality:</strong></p>

<ul>
<li>Forward and backward propagation for NN, CNN, RNN, and Transformer models</li>
<li>Fully customisable architectures</li>
<li>Manual gradient computation for each layer</li>
<li>Modular API design</li>
</ul>

<p><strong>2. Supported Layers:</strong></p>

<ul>
<li>Dense (fully connected) layers</li>
<li>Convolutional layers (1D and 2D)</li>
<li>Pooling (Max and Average, Global Average)</li>
<li>Multi head self attention, positional encoding, residiual connections</li>
</ul>

<p><strong>3. Activation and Loss Functions:</strong></p>

<ul>
<li>Activation functions: ReLU, Leaky ReLU, Sigmoid, TanH, Linear, Softmax with derivatives</li>
<li>Loss functions: MSE, MAE, Cross entropy, Normalised Cross Entropy with derivatives</li>
</ul>

<p><strong>4. Optimisers:</strong></p>

<ul>
<li>Gradient Descent (GD)</li>
<li>Stochastic Gradiend Descent (SGD)</li>
<li>Adam, AdamW, RMSProp, Lion</li>
<li>Supports batching</li>
</ul>

<p><strong>5. Utilities and Extras:</strong></p>

<ul>
<li>Save/Load weights and biases</li>
<li>Training visualisation (accuracy/loss graphs)</li>
<li>Data augmentation (flipping, normalisation, one hot labels)</li>
<li>Evaluation metrics (accuracy, loss)</li>
<li>Demo projects (MNIST, HAM10000 skin lesion detection)</li>
<li>150+ unit tests with 91% coverage</li>
<li>Benchmarking against PyTorch and TensorFlow</li>
</ul>

<h2 id="installation">Installation</h2>

<p>QuackNet is simple to install via PyPI.</p>

<p><strong>Install via PyPI</strong></p>

<pre><code>pip install QuackNet
</code></pre>

<h2 id="usage-example">Usage Example</h2>

<div class="pdoc-code codehilite">
<pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">quacknet.main</span><span class="w"> </span><span class="kn">import</span> <span class="n">Network</span>

<span class="c1"># Define a neural network architecture</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">Network</span><span class="p">(</span>
    <span class="n">lossFunc</span> <span class="o">=</span> <span class="s2">&quot;cross entropy&quot;</span><span class="p">,</span>
    <span class="n">learningRate</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
    <span class="n">optimisationFunc</span> <span class="o">=</span> <span class="s2">&quot;sgd&quot;</span><span class="p">,</span> <span class="c1">#stochastic gradient descent</span>
<span class="p">)</span>
<span class="n">n</span><span class="o">.</span><span class="n">addLayer</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;relu&quot;</span><span class="p">)</span> <span class="c1"># Input layer</span>
<span class="n">n</span><span class="o">.</span><span class="n">addLayer</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;relu&quot;</span><span class="p">)</span> <span class="c1"># Hidden layer</span>
<span class="n">n</span><span class="o">.</span><span class="n">addLayer</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;softmax&quot;</span><span class="p">)</span> <span class="c1"># Output layer</span>
<span class="n">n</span><span class="o">.</span><span class="n">createWeightsAndBiases</span><span class="p">()</span>

<span class="c1"># Example data</span>
<span class="n">inputData</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">]]</span>

<span class="c1"># Train the network</span>
<span class="n">accuracy</span><span class="p">,</span> <span class="n">averageLoss</span> <span class="o">=</span> <span class="n">n</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>

<span class="c1"># Evaluate</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Average loss: </span><span class="si">{</span><span class="n">averageLoss</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre>
</div>

<h2 id="examples">Examples</h2>

<ul>
<li><a href="/ExampleCode/NNExample.py">Simple Neural Network Example</a>: A basic neural network implementation demonstrating forward and backpropagation</li>
<li><a href="/ExampleCode/CNNExample.py">Convolutional Neural Network Example</a>: Shows how to use the convolutional layers in the library</li>
<li><a href="/ExampleCode/MNISTExample/mnistExample.py">MNIST Neural Network Example</a>: Shows how to use neural network to train on MNIST</li>
</ul>

<h2 id="highlights">Highlights</h2>

<ul>
<li>Fully from scratch implementation for educational insight</li>
<li>Full suport for modern deep learning architecures</li>
<li>Benchmarked against PyTorch and TensorFlow on MNIST and had better performance</li>
<li>Easy to use API and modular design for experimenting</li>
</ul>

<h2 id="code-structure">Code structure</h2>

<ul>
<li><strong>Neural Network Class:</strong>
<ul>
<li>Dense Layers, forward/backprop</li>
</ul></li>
<li><strong>Convolutional Neural Network Class:</strong>
<ul>
<li>Conv1D/2D, pooling, global average pooling, activation layers</li>
</ul></li>
<li><strong>Recurrent Neural Network Class:</strong>
<ul>
<li>Singular and Stacked RNN, with BPTT</li>
</ul></li>
<li><strong>Transformer Class:</strong>
<ul>
<li>Multi head attention with casual padding, position wise FFN, residual connections, input embedding layer, positional encoding</li>
</ul></li>
</ul>

<h2 id="related-projects">Related Projects</h2>

<h3 id="skin-lesion-detector">Skin Lesion Detector</h3>

<p>A convolutional neural network (CNN) skin lesion classification model built with QuackNet, trained using the HAM10000 dataset. This model achieved 72% accuracy on a balanced test set.</p>

<p>You can explore the full project here:
<a href="https://github.com/SirQuackPng/skinLesionDetector">Skin Lesion Detector Repository</a></p>

<h2 id="license">License</h2>

<p>This project is licensed under the MIT License - see the <a href="LICENSE">LICENSE</a> file for details.</p>
</div>

                        <input id="mod-quacknet-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">

                        <label class="view-source-button" for="mod-quacknet-view-source"><span>View Source</span></label>

                        <div class="pdoc-code codehilite"><pre><span></span><span id="L-1"><a href="#L-1"><span class="linenos">  1</span></a><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-2"><a href="#L-2"><span class="linenos">  2</span></a><span class="sd"># QuackNet</span>
</span><span id="L-3"><a href="#L-3"><span class="linenos">  3</span></a>
</span><span id="L-4"><a href="#L-4"><span class="linenos">  4</span></a><span class="sd">**QuackNet** is a from scratch deep learning library built entirely in NumPy. </span>
</span><span id="L-5"><a href="#L-5"><span class="linenos">  5</span></a><span class="sd">It supports Neural Networks, CNNs, RNNs, and Transformers, providing full access to forward </span>
</span><span id="L-6"><a href="#L-6"><span class="linenos">  6</span></a><span class="sd">and backward propagation, manual gradient computation, and weight updates. All without relying </span>
</span><span id="L-7"><a href="#L-7"><span class="linenos">  7</span></a><span class="sd">on frameworks like TensorFlow or PyTorch.</span>
</span><span id="L-8"><a href="#L-8"><span class="linenos">  8</span></a>
</span><span id="L-9"><a href="#L-9"><span class="linenos">  9</span></a><span class="sd">QuackNet is designed for both educational purposes and small-scale research, letting you </span>
</span><span id="L-10"><a href="#L-10"><span class="linenos"> 10</span></a><span class="sd">experiment with deep learning architectures while understanding the underlying math.</span>
</span><span id="L-11"><a href="#L-11"><span class="linenos"> 11</span></a>
</span><span id="L-12"><a href="#L-12"><span class="linenos"> 12</span></a><span class="sd">## Key Features</span>
</span><span id="L-13"><a href="#L-13"><span class="linenos"> 13</span></a>
</span><span id="L-14"><a href="#L-14"><span class="linenos"> 14</span></a><span class="sd">**1. Core Functionality:**</span>
</span><span id="L-15"><a href="#L-15"><span class="linenos"> 15</span></a><span class="sd">-   Forward and backward propagation for NN, CNN, RNN, and Transformer models</span>
</span><span id="L-16"><a href="#L-16"><span class="linenos"> 16</span></a><span class="sd">-   Fully customisable architectures</span>
</span><span id="L-17"><a href="#L-17"><span class="linenos"> 17</span></a><span class="sd">-   Manual gradient computation for each layer</span>
</span><span id="L-18"><a href="#L-18"><span class="linenos"> 18</span></a><span class="sd">-   Modular API design</span>
</span><span id="L-19"><a href="#L-19"><span class="linenos"> 19</span></a>
</span><span id="L-20"><a href="#L-20"><span class="linenos"> 20</span></a><span class="sd">**2. Supported Layers:**</span>
</span><span id="L-21"><a href="#L-21"><span class="linenos"> 21</span></a><span class="sd">-   Dense (fully connected) layers</span>
</span><span id="L-22"><a href="#L-22"><span class="linenos"> 22</span></a><span class="sd">-   Convolutional layers (1D and 2D)</span>
</span><span id="L-23"><a href="#L-23"><span class="linenos"> 23</span></a><span class="sd">-   Pooling (Max and Average, Global Average)</span>
</span><span id="L-24"><a href="#L-24"><span class="linenos"> 24</span></a><span class="sd">-   Multi head self attention, positional encoding, residiual connections</span>
</span><span id="L-25"><a href="#L-25"><span class="linenos"> 25</span></a>
</span><span id="L-26"><a href="#L-26"><span class="linenos"> 26</span></a><span class="sd">**3. Activation and Loss Functions:**</span>
</span><span id="L-27"><a href="#L-27"><span class="linenos"> 27</span></a><span class="sd">-   Activation functions: ReLU, Leaky ReLU, Sigmoid, TanH, Linear, Softmax with derivatives</span>
</span><span id="L-28"><a href="#L-28"><span class="linenos"> 28</span></a><span class="sd">-   Loss functions: MSE, MAE, Cross entropy, Normalised Cross Entropy with derivatives</span>
</span><span id="L-29"><a href="#L-29"><span class="linenos"> 29</span></a>
</span><span id="L-30"><a href="#L-30"><span class="linenos"> 30</span></a><span class="sd">**4. Optimisers:**</span>
</span><span id="L-31"><a href="#L-31"><span class="linenos"> 31</span></a><span class="sd">-   Gradient Descent (GD)</span>
</span><span id="L-32"><a href="#L-32"><span class="linenos"> 32</span></a><span class="sd">-   Stochastic Gradiend Descent (SGD)</span>
</span><span id="L-33"><a href="#L-33"><span class="linenos"> 33</span></a><span class="sd">-   Adam, AdamW, RMSProp, Lion</span>
</span><span id="L-34"><a href="#L-34"><span class="linenos"> 34</span></a><span class="sd">-   Supports batching</span>
</span><span id="L-35"><a href="#L-35"><span class="linenos"> 35</span></a>
</span><span id="L-36"><a href="#L-36"><span class="linenos"> 36</span></a><span class="sd">**5. Utilities and Extras:**</span>
</span><span id="L-37"><a href="#L-37"><span class="linenos"> 37</span></a><span class="sd">-   Save/Load weights and biases</span>
</span><span id="L-38"><a href="#L-38"><span class="linenos"> 38</span></a><span class="sd">-   Training visualisation (accuracy/loss graphs)</span>
</span><span id="L-39"><a href="#L-39"><span class="linenos"> 39</span></a><span class="sd">-   Data augmentation (flipping, normalisation, one hot labels)</span>
</span><span id="L-40"><a href="#L-40"><span class="linenos"> 40</span></a><span class="sd">-   Evaluation metrics (accuracy, loss)</span>
</span><span id="L-41"><a href="#L-41"><span class="linenos"> 41</span></a><span class="sd">-   Demo projects (MNIST, HAM10000 skin lesion detection)</span>
</span><span id="L-42"><a href="#L-42"><span class="linenos"> 42</span></a><span class="sd">-   150+ unit tests with 91% coverage</span>
</span><span id="L-43"><a href="#L-43"><span class="linenos"> 43</span></a><span class="sd">-   Benchmarking against PyTorch and TensorFlow</span>
</span><span id="L-44"><a href="#L-44"><span class="linenos"> 44</span></a>
</span><span id="L-45"><a href="#L-45"><span class="linenos"> 45</span></a><span class="sd">## Installation</span>
</span><span id="L-46"><a href="#L-46"><span class="linenos"> 46</span></a>
</span><span id="L-47"><a href="#L-47"><span class="linenos"> 47</span></a><span class="sd">QuackNet is simple to install via PyPI.</span>
</span><span id="L-48"><a href="#L-48"><span class="linenos"> 48</span></a>
</span><span id="L-49"><a href="#L-49"><span class="linenos"> 49</span></a><span class="sd">**Install via PyPI**</span>
</span><span id="L-50"><a href="#L-50"><span class="linenos"> 50</span></a>
</span><span id="L-51"><a href="#L-51"><span class="linenos"> 51</span></a><span class="sd">```</span>
</span><span id="L-52"><a href="#L-52"><span class="linenos"> 52</span></a><span class="sd">pip install QuackNet</span>
</span><span id="L-53"><a href="#L-53"><span class="linenos"> 53</span></a><span class="sd">```</span>
</span><span id="L-54"><a href="#L-54"><span class="linenos"> 54</span></a>
</span><span id="L-55"><a href="#L-55"><span class="linenos"> 55</span></a><span class="sd">## Usage Example</span>
</span><span id="L-56"><a href="#L-56"><span class="linenos"> 56</span></a>
</span><span id="L-57"><a href="#L-57"><span class="linenos"> 57</span></a><span class="sd">```Python</span>
</span><span id="L-58"><a href="#L-58"><span class="linenos"> 58</span></a><span class="sd">from quacknet.main import Network</span>
</span><span id="L-59"><a href="#L-59"><span class="linenos"> 59</span></a>
</span><span id="L-60"><a href="#L-60"><span class="linenos"> 60</span></a><span class="sd"># Define a neural network architecture</span>
</span><span id="L-61"><a href="#L-61"><span class="linenos"> 61</span></a><span class="sd">n = Network(</span>
</span><span id="L-62"><a href="#L-62"><span class="linenos"> 62</span></a><span class="sd">    lossFunc = &quot;cross entropy&quot;,</span>
</span><span id="L-63"><a href="#L-63"><span class="linenos"> 63</span></a><span class="sd">    learningRate = 0.01,</span>
</span><span id="L-64"><a href="#L-64"><span class="linenos"> 64</span></a><span class="sd">    optimisationFunc = &quot;sgd&quot;, #stochastic gradient descent</span>
</span><span id="L-65"><a href="#L-65"><span class="linenos"> 65</span></a><span class="sd">)</span>
</span><span id="L-66"><a href="#L-66"><span class="linenos"> 66</span></a><span class="sd">n.addLayer(3, &quot;relu&quot;) # Input layer</span>
</span><span id="L-67"><a href="#L-67"><span class="linenos"> 67</span></a><span class="sd">n.addLayer(2, &quot;relu&quot;) # Hidden layer</span>
</span><span id="L-68"><a href="#L-68"><span class="linenos"> 68</span></a><span class="sd">n.addLayer(1, &quot;softmax&quot;) # Output layer</span>
</span><span id="L-69"><a href="#L-69"><span class="linenos"> 69</span></a><span class="sd">n.createWeightsAndBiases()</span>
</span><span id="L-70"><a href="#L-70"><span class="linenos"> 70</span></a>
</span><span id="L-71"><a href="#L-71"><span class="linenos"> 71</span></a><span class="sd"># Example data</span>
</span><span id="L-72"><a href="#L-72"><span class="linenos"> 72</span></a><span class="sd">inputData = [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]</span>
</span><span id="L-73"><a href="#L-73"><span class="linenos"> 73</span></a><span class="sd">labels = [[1], [0]]</span>
</span><span id="L-74"><a href="#L-74"><span class="linenos"> 74</span></a>
</span><span id="L-75"><a href="#L-75"><span class="linenos"> 75</span></a><span class="sd"># Train the network</span>
</span><span id="L-76"><a href="#L-76"><span class="linenos"> 76</span></a><span class="sd">accuracy, averageLoss = n.train(inputData, labels, epochs = 10)</span>
</span><span id="L-77"><a href="#L-77"><span class="linenos"> 77</span></a>
</span><span id="L-78"><a href="#L-78"><span class="linenos"> 78</span></a><span class="sd"># Evaluate</span>
</span><span id="L-79"><a href="#L-79"><span class="linenos"> 79</span></a><span class="sd">print(f&quot;Accuracy: {accuracy}%&quot;)</span>
</span><span id="L-80"><a href="#L-80"><span class="linenos"> 80</span></a><span class="sd">print(f&quot;Average loss: {averageLoss}&quot;)</span>
</span><span id="L-81"><a href="#L-81"><span class="linenos"> 81</span></a><span class="sd">```</span>
</span><span id="L-82"><a href="#L-82"><span class="linenos"> 82</span></a>
</span><span id="L-83"><a href="#L-83"><span class="linenos"> 83</span></a><span class="sd">## Examples</span>
</span><span id="L-84"><a href="#L-84"><span class="linenos"> 84</span></a>
</span><span id="L-85"><a href="#L-85"><span class="linenos"> 85</span></a><span class="sd">-   [Simple Neural Network Example](/ExampleCode/NNExample.py): A basic neural network implementation demonstrating forward and backpropagation</span>
</span><span id="L-86"><a href="#L-86"><span class="linenos"> 86</span></a><span class="sd">-   [Convolutional Neural Network Example](/ExampleCode/CNNExample.py): Shows how to use the convolutional layers in the library</span>
</span><span id="L-87"><a href="#L-87"><span class="linenos"> 87</span></a><span class="sd">-   [MNIST Neural Network Example](/ExampleCode/MNISTExample/mnistExample.py): Shows how to use neural network to train on MNIST</span>
</span><span id="L-88"><a href="#L-88"><span class="linenos"> 88</span></a>
</span><span id="L-89"><a href="#L-89"><span class="linenos"> 89</span></a><span class="sd">## Highlights</span>
</span><span id="L-90"><a href="#L-90"><span class="linenos"> 90</span></a>
</span><span id="L-91"><a href="#L-91"><span class="linenos"> 91</span></a><span class="sd">-   Fully from scratch implementation for educational insight</span>
</span><span id="L-92"><a href="#L-92"><span class="linenos"> 92</span></a><span class="sd">-   Full suport for modern deep learning architecures</span>
</span><span id="L-93"><a href="#L-93"><span class="linenos"> 93</span></a><span class="sd">-   Benchmarked against PyTorch and TensorFlow on MNIST and had better performance</span>
</span><span id="L-94"><a href="#L-94"><span class="linenos"> 94</span></a><span class="sd">-   Easy to use API and modular design for experimenting</span>
</span><span id="L-95"><a href="#L-95"><span class="linenos"> 95</span></a>
</span><span id="L-96"><a href="#L-96"><span class="linenos"> 96</span></a><span class="sd">## Code structure</span>
</span><span id="L-97"><a href="#L-97"><span class="linenos"> 97</span></a>
</span><span id="L-98"><a href="#L-98"><span class="linenos"> 98</span></a><span class="sd">-   **Neural Network Class:**</span>
</span><span id="L-99"><a href="#L-99"><span class="linenos"> 99</span></a><span class="sd">    -   Dense Layers, forward/backprop</span>
</span><span id="L-100"><a href="#L-100"><span class="linenos">100</span></a><span class="sd">-   **Convolutional Neural Network Class:**</span>
</span><span id="L-101"><a href="#L-101"><span class="linenos">101</span></a><span class="sd">    -   Conv1D/2D, pooling, global average pooling, activation layers</span>
</span><span id="L-102"><a href="#L-102"><span class="linenos">102</span></a><span class="sd">-   **Recurrent Neural Network Class:**</span>
</span><span id="L-103"><a href="#L-103"><span class="linenos">103</span></a><span class="sd">    -   Singular and Stacked RNN, with BPTT</span>
</span><span id="L-104"><a href="#L-104"><span class="linenos">104</span></a><span class="sd">-   **Transformer Class:**</span>
</span><span id="L-105"><a href="#L-105"><span class="linenos">105</span></a><span class="sd">    -   Multi head attention with casual padding, position wise FFN, residual connections, input embedding layer, positional encoding</span>
</span><span id="L-106"><a href="#L-106"><span class="linenos">106</span></a><span class="sd">    </span>
</span><span id="L-107"><a href="#L-107"><span class="linenos">107</span></a><span class="sd">## Related Projects</span>
</span><span id="L-108"><a href="#L-108"><span class="linenos">108</span></a>
</span><span id="L-109"><a href="#L-109"><span class="linenos">109</span></a><span class="sd">### Skin Lesion Detector</span>
</span><span id="L-110"><a href="#L-110"><span class="linenos">110</span></a>
</span><span id="L-111"><a href="#L-111"><span class="linenos">111</span></a><span class="sd">A convolutional neural network (CNN) skin lesion classification model built with QuackNet, trained using the HAM10000 dataset. This model achieved 72% accuracy on a balanced test set.</span>
</span><span id="L-112"><a href="#L-112"><span class="linenos">112</span></a>
</span><span id="L-113"><a href="#L-113"><span class="linenos">113</span></a><span class="sd">You can explore the full project here:</span>
</span><span id="L-114"><a href="#L-114"><span class="linenos">114</span></a><span class="sd">[Skin Lesion Detector Repository](https://github.com/SirQuackPng/skinLesionDetector)</span>
</span><span id="L-115"><a href="#L-115"><span class="linenos">115</span></a>
</span><span id="L-116"><a href="#L-116"><span class="linenos">116</span></a><span class="sd">## License</span>
</span><span id="L-117"><a href="#L-117"><span class="linenos">117</span></a>
</span><span id="L-118"><a href="#L-118"><span class="linenos">118</span></a><span class="sd">This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.</span>
</span><span id="L-119"><a href="#L-119"><span class="linenos">119</span></a><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-120"><a href="#L-120"><span class="linenos">120</span></a>
</span><span id="L-121"><a href="#L-121"><span class="linenos">121</span></a>
</span><span id="L-122"><a href="#L-122"><span class="linenos">122</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">.NN</span><span class="w"> </span><span class="kn">import</span> <span class="n">Network</span>
</span><span id="L-123"><a href="#L-123"><span class="linenos">123</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">.CNN</span><span class="w"> </span><span class="kn">import</span> <span class="n">CNNModel</span>
</span><span id="L-124"><a href="#L-124"><span class="linenos">124</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">.CNN</span><span class="w"> </span><span class="kn">import</span> <span class="n">Conv1DLayer</span><span class="p">,</span> <span class="n">Conv2DLayer</span><span class="p">,</span> <span class="n">ActivationLayer</span><span class="p">,</span> <span class="n">DenseLayer</span><span class="p">,</span> <span class="n">PoolingLayer</span><span class="p">,</span> <span class="n">GlobalAveragePooling</span>
</span><span id="L-125"><a href="#L-125"><span class="linenos">125</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">.RNN</span><span class="w"> </span><span class="kn">import</span> <span class="n">StackedRNN</span><span class="p">,</span> <span class="n">SingularRNN</span>
</span><span id="L-126"><a href="#L-126"><span class="linenos">126</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">.Transformer</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
</span><span id="L-127"><a href="#L-127"><span class="linenos">127</span></a>    <span class="n">Transformer</span><span class="p">,</span> <span class="n">TransformerBlock</span><span class="p">,</span> <span class="n">MultiAttentionHeadLayer</span><span class="p">,</span>
</span><span id="L-128"><a href="#L-128"><span class="linenos">128</span></a>    <span class="n">EmbeddingLayer</span><span class="p">,</span> <span class="n">FeedForwardNetwork</span><span class="p">,</span> <span class="n">NormLayer</span><span class="p">,</span> <span class="n">PositionalEncoding</span><span class="p">,</span> <span class="n">ResidualConnection</span>
</span><span id="L-129"><a href="#L-129"><span class="linenos">129</span></a><span class="p">)</span>
</span><span id="L-130"><a href="#L-130"><span class="linenos">130</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">.universalLayers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dropout</span>
</span><span id="L-131"><a href="#L-131"><span class="linenos">131</span></a>
</span><span id="L-132"><a href="#L-132"><span class="linenos">132</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">.core.losses.lossFunctions</span><span class="w"> </span><span class="kn">import</span> <span class="o">*</span>
</span><span id="L-133"><a href="#L-133"><span class="linenos">133</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">.core.losses.lossDerivativeFunctions</span><span class="w"> </span><span class="kn">import</span> <span class="o">*</span>
</span><span id="L-134"><a href="#L-134"><span class="linenos">134</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">.core.activations.activationFunctions</span><span class="w"> </span><span class="kn">import</span> <span class="o">*</span>
</span><span id="L-135"><a href="#L-135"><span class="linenos">135</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">.core.activations.activationDerivativeFunctions</span><span class="w"> </span><span class="kn">import</span> <span class="o">*</span>
</span><span id="L-136"><a href="#L-136"><span class="linenos">136</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">.core.utilities.dataAugmentation</span><span class="w"> </span><span class="kn">import</span> <span class="o">*</span>
</span><span id="L-137"><a href="#L-137"><span class="linenos">137</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">.core.utilities.drawGraphs</span><span class="w"> </span><span class="kn">import</span> <span class="o">*</span>
</span><span id="L-138"><a href="#L-138"><span class="linenos">138</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">.core.optimisers.adam</span><span class="w"> </span><span class="kn">import</span> <span class="n">Adam</span>
</span><span id="L-139"><a href="#L-139"><span class="linenos">139</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">.core.optimisers.adamW</span><span class="w"> </span><span class="kn">import</span> <span class="n">AdamW</span>
</span><span id="L-140"><a href="#L-140"><span class="linenos">140</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">.core.optimisers.stochasticGD</span><span class="w"> </span><span class="kn">import</span> <span class="n">SGD</span>
</span><span id="L-141"><a href="#L-141"><span class="linenos">141</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">.core.optimisers.gradientDescent</span><span class="w"> </span><span class="kn">import</span> <span class="n">GD</span>
</span><span id="L-142"><a href="#L-142"><span class="linenos">142</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">.core.optimisers.rmsProp</span><span class="w"> </span><span class="kn">import</span> <span class="n">RMSProp</span>
</span><span id="L-143"><a href="#L-143"><span class="linenos">143</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">.core.optimisers.lion</span><span class="w"> </span><span class="kn">import</span> <span class="n">Lion</span>
</span><span id="L-144"><a href="#L-144"><span class="linenos">144</span></a>
</span><span id="L-145"><a href="#L-145"><span class="linenos">145</span></a><span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
</span><span id="L-146"><a href="#L-146"><span class="linenos">146</span></a>    <span class="s2">&quot;Network&quot;</span><span class="p">,</span>
</span><span id="L-147"><a href="#L-147"><span class="linenos">147</span></a>    <span class="s2">&quot;CNNModel&quot;</span><span class="p">,</span>
</span><span id="L-148"><a href="#L-148"><span class="linenos">148</span></a>    <span class="s2">&quot;Conv1DLayer&quot;</span><span class="p">,</span>
</span><span id="L-149"><a href="#L-149"><span class="linenos">149</span></a>    <span class="s2">&quot;Conv2DLayer&quot;</span><span class="p">,</span>
</span><span id="L-150"><a href="#L-150"><span class="linenos">150</span></a>    <span class="s2">&quot;ActivationLayer&quot;</span><span class="p">,</span>
</span><span id="L-151"><a href="#L-151"><span class="linenos">151</span></a>    <span class="s2">&quot;DenseLayer&quot;</span><span class="p">,</span>
</span><span id="L-152"><a href="#L-152"><span class="linenos">152</span></a>    <span class="s2">&quot;PoolingLayer&quot;</span><span class="p">,</span>
</span><span id="L-153"><a href="#L-153"><span class="linenos">153</span></a>    <span class="s2">&quot;GlobalAveragePooling&quot;</span><span class="p">,</span>
</span><span id="L-154"><a href="#L-154"><span class="linenos">154</span></a>    <span class="s2">&quot;StackedRNN&quot;</span><span class="p">,</span>
</span><span id="L-155"><a href="#L-155"><span class="linenos">155</span></a>    <span class="s2">&quot;SingularRNN&quot;</span><span class="p">,</span>
</span><span id="L-156"><a href="#L-156"><span class="linenos">156</span></a>    <span class="s2">&quot;Transformer&quot;</span><span class="p">,</span>
</span><span id="L-157"><a href="#L-157"><span class="linenos">157</span></a>    <span class="s2">&quot;TransformerBlock&quot;</span><span class="p">,</span>
</span><span id="L-158"><a href="#L-158"><span class="linenos">158</span></a>    <span class="s2">&quot;MultiAttentionHeadLayer&quot;</span><span class="p">,</span>
</span><span id="L-159"><a href="#L-159"><span class="linenos">159</span></a>    <span class="s2">&quot;EmbeddingLayer&quot;</span><span class="p">,</span>
</span><span id="L-160"><a href="#L-160"><span class="linenos">160</span></a>    <span class="s2">&quot;FeedForwardNetwork&quot;</span><span class="p">,</span>
</span><span id="L-161"><a href="#L-161"><span class="linenos">161</span></a>    <span class="s2">&quot;NormLayer&quot;</span><span class="p">,</span>
</span><span id="L-162"><a href="#L-162"><span class="linenos">162</span></a>    <span class="s2">&quot;PositionalEncoding&quot;</span><span class="p">,</span>
</span><span id="L-163"><a href="#L-163"><span class="linenos">163</span></a>    <span class="s2">&quot;ResidualConnection&quot;</span><span class="p">,</span>
</span><span id="L-164"><a href="#L-164"><span class="linenos">164</span></a>    <span class="s2">&quot;Dropout&quot;</span><span class="p">,</span>
</span><span id="L-165"><a href="#L-165"><span class="linenos">165</span></a>    <span class="s2">&quot;Adam&quot;</span><span class="p">,</span>
</span><span id="L-166"><a href="#L-166"><span class="linenos">166</span></a>    <span class="s2">&quot;AdamW&quot;</span><span class="p">,</span>
</span><span id="L-167"><a href="#L-167"><span class="linenos">167</span></a>    <span class="s2">&quot;SGD&quot;</span><span class="p">,</span>
</span><span id="L-168"><a href="#L-168"><span class="linenos">168</span></a>    <span class="s2">&quot;GD&quot;</span><span class="p">,</span>
</span><span id="L-169"><a href="#L-169"><span class="linenos">169</span></a>    <span class="s2">&quot;RMSProp&quot;</span><span class="p">,</span>
</span><span id="L-170"><a href="#L-170"><span class="linenos">170</span></a>    <span class="s2">&quot;Lion&quot;</span><span class="p">,</span>
</span><span id="L-171"><a href="#L-171"><span class="linenos">171</span></a><span class="p">]</span>
</span></pre></div>


            </section>
                <section id="Network">
                            <input id="Network-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">Network</span><wbr>(<span class="base">quacknet.NN.initialisers.Initialisers</span>, <span class="base">quacknet.NN.writeAndReadWeightBias.writeAndRead</span>, <span class="base">quacknet.core.utilities.dataAugmentation.Augementation</span>):

                <label class="view-source-button" for="Network-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Network"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Network-11"><a href="#Network-11"><span class="linenos"> 11</span></a><span class="k">class</span><span class="w"> </span><span class="nc">Network</span><span class="p">(</span><span class="n">Initialisers</span><span class="p">,</span> <span class="n">writeAndRead</span><span class="p">,</span> <span class="n">Augementation</span><span class="p">):</span>
</span><span id="Network-12"><a href="#Network-12"><span class="linenos"> 12</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lossFunc</span> <span class="o">=</span> <span class="s2">&quot;MSE&quot;</span><span class="p">,</span> <span class="n">learningRate</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">useBatches</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">batchSize</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span> <span class="n">optimisationFunction</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">):</span>
</span><span id="Network-13"><a href="#Network-13"><span class="linenos"> 13</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="Network-14"><a href="#Network-14"><span class="linenos"> 14</span></a><span class="sd">        Args:</span>
</span><span id="Network-15"><a href="#Network-15"><span class="linenos"> 15</span></a><span class="sd">            lossFunc (str): Loss function name (&#39;mse&#39;, &#39;mae&#39;, &#39;cross&#39;). Default is &quot;MSE&quot;.</span>
</span><span id="Network-16"><a href="#Network-16"><span class="linenos"> 16</span></a><span class="sd">            learningRate (float, optional): Learning rate for training. Default is 0.01.</span>
</span><span id="Network-17"><a href="#Network-17"><span class="linenos"> 17</span></a><span class="sd">            optimisationFunc (str, optional): Optimisaztion method (&#39;gd&#39;, &#39;sgd&#39;, &#39;batching&#39;). Default is &quot;gd&quot;.</span>
</span><span id="Network-18"><a href="#Network-18"><span class="linenos"> 18</span></a><span class="sd">            useBatches (bool, optional): Wether to use mini batches. Default is False.</span>
</span><span id="Network-19"><a href="#Network-19"><span class="linenos"> 19</span></a><span class="sd">            batchSize (int, optional): size of mini batches. Default is 32.</span>
</span><span id="Network-20"><a href="#Network-20"><span class="linenos"> 20</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="Network-21"><a href="#Network-21"><span class="linenos"> 21</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="Network-22"><a href="#Network-22"><span class="linenos"> 22</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="Network-23"><a href="#Network-23"><span class="linenos"> 23</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="Network-24"><a href="#Network-24"><span class="linenos"> 24</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">learningRate</span> <span class="o">=</span> <span class="n">learningRate</span>
</span><span id="Network-25"><a href="#Network-25"><span class="linenos"> 25</span></a>
</span><span id="Network-26"><a href="#Network-26"><span class="linenos"> 26</span></a>        <span class="n">lossFunctionDict</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="Network-27"><a href="#Network-27"><span class="linenos"> 27</span></a>            <span class="s2">&quot;mse&quot;</span><span class="p">:</span> <span class="n">MSELossFunction</span><span class="p">,</span>
</span><span id="Network-28"><a href="#Network-28"><span class="linenos"> 28</span></a>            <span class="s2">&quot;mae&quot;</span><span class="p">:</span> <span class="n">MAELossFunction</span><span class="p">,</span>
</span><span id="Network-29"><a href="#Network-29"><span class="linenos"> 29</span></a>            <span class="s2">&quot;cross entropy&quot;</span><span class="p">:</span> <span class="n">CrossEntropyLossFunction</span><span class="p">,</span><span class="s2">&quot;cross&quot;</span><span class="p">:</span> <span class="n">CrossEntropyLossFunction</span><span class="p">,</span>
</span><span id="Network-30"><a href="#Network-30"><span class="linenos"> 30</span></a>            <span class="s2">&quot;normalised cross entropy&quot;</span><span class="p">:</span> <span class="n">NormalisedCrossEntropyLossFunction</span><span class="p">,</span><span class="s2">&quot;normalised cross&quot;</span><span class="p">:</span> <span class="n">NormalisedCrossEntropyLossFunction</span><span class="p">,</span>
</span><span id="Network-31"><a href="#Network-31"><span class="linenos"> 31</span></a>        <span class="p">}</span>
</span><span id="Network-32"><a href="#Network-32"><span class="linenos"> 32</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">lossFunction</span> <span class="o">=</span> <span class="n">lossFunctionDict</span><span class="p">[</span><span class="n">lossFunc</span><span class="o">.</span><span class="n">lower</span><span class="p">()]</span>
</span><span id="Network-33"><a href="#Network-33"><span class="linenos"> 33</span></a>
</span><span id="Network-34"><a href="#Network-34"><span class="linenos"> 34</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">optimisationFunction</span> <span class="o">=</span> <span class="n">optimisationFunction</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">forwardPropagation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backPropgation</span><span class="p">)</span>
</span><span id="Network-35"><a href="#Network-35"><span class="linenos"> 35</span></a>
</span><span id="Network-36"><a href="#Network-36"><span class="linenos"> 36</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">useBatches</span> <span class="o">=</span> <span class="n">useBatches</span>
</span><span id="Network-37"><a href="#Network-37"><span class="linenos"> 37</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span> <span class="o">=</span> <span class="n">batchSize</span>
</span><span id="Network-38"><a href="#Network-38"><span class="linenos"> 38</span></a>
</span><span id="Network-39"><a href="#Network-39"><span class="linenos"> 39</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">addLayer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">activationFunction</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">):</span>
</span><span id="Network-40"><a href="#Network-40"><span class="linenos"> 40</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="Network-41"><a href="#Network-41"><span class="linenos"> 41</span></a><span class="sd">        Add a layer to the network with the specified number of nodes and activation function.</span>
</span><span id="Network-42"><a href="#Network-42"><span class="linenos"> 42</span></a>
</span><span id="Network-43"><a href="#Network-43"><span class="linenos"> 43</span></a><span class="sd">        Args:</span>
</span><span id="Network-44"><a href="#Network-44"><span class="linenos"> 44</span></a><span class="sd">            size (int): Number of nodes in the new layer.</span>
</span><span id="Network-45"><a href="#Network-45"><span class="linenos"> 45</span></a><span class="sd">            activationFunction (str, optional): Activation function name (&#39;relu&#39;, &#39;sigmoid&#39;, &#39;linear&#39;, &#39;tanh&#39;, &#39;softmax&#39;). Default is &quot;relu&quot;.</span>
</span><span id="Network-46"><a href="#Network-46"><span class="linenos"> 46</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="Network-47"><a href="#Network-47"><span class="linenos"> 47</span></a>        <span class="n">funcs</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="Network-48"><a href="#Network-48"><span class="linenos"> 48</span></a>            <span class="s2">&quot;relu&quot;</span><span class="p">:</span> <span class="n">relu</span><span class="p">,</span>
</span><span id="Network-49"><a href="#Network-49"><span class="linenos"> 49</span></a>            <span class="s2">&quot;sigmoid&quot;</span><span class="p">:</span> <span class="n">sigmoid</span><span class="p">,</span>
</span><span id="Network-50"><a href="#Network-50"><span class="linenos"> 50</span></a>            <span class="s2">&quot;linear&quot;</span><span class="p">:</span> <span class="n">linear</span><span class="p">,</span>
</span><span id="Network-51"><a href="#Network-51"><span class="linenos"> 51</span></a>            <span class="s2">&quot;tanh&quot;</span><span class="p">:</span> <span class="n">tanH</span><span class="p">,</span>
</span><span id="Network-52"><a href="#Network-52"><span class="linenos"> 52</span></a>            <span class="s2">&quot;softmax&quot;</span><span class="p">:</span> <span class="n">softMax</span><span class="p">,</span>
</span><span id="Network-53"><a href="#Network-53"><span class="linenos"> 53</span></a>        <span class="p">}</span>
</span><span id="Network-54"><a href="#Network-54"><span class="linenos"> 54</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">activationFunction</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">funcs</span><span class="p">):</span>
</span><span id="Network-55"><a href="#Network-55"><span class="linenos"> 55</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Activation function not made: </span><span class="si">{</span><span class="n">activationFunction</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="Network-56"><a href="#Network-56"><span class="linenos"> 56</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">size</span><span class="p">,</span> <span class="n">funcs</span><span class="p">[</span><span class="n">activationFunction</span><span class="o">.</span><span class="n">lower</span><span class="p">()]])</span>
</span><span id="Network-57"><a href="#Network-57"><span class="linenos"> 57</span></a>
</span><span id="Network-58"><a href="#Network-58"><span class="linenos"> 58</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_calculateLayerNodes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lastLayerNodes</span><span class="p">,</span> <span class="n">lastLayerWeights</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="n">currentLayer</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="Network-59"><a href="#Network-59"><span class="linenos"> 59</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="Network-60"><a href="#Network-60"><span class="linenos"> 60</span></a><span class="sd">        Calculate the output of a layer given inputs, weights and biases.</span>
</span><span id="Network-61"><a href="#Network-61"><span class="linenos"> 61</span></a>
</span><span id="Network-62"><a href="#Network-62"><span class="linenos"> 62</span></a><span class="sd">        Args:</span>
</span><span id="Network-63"><a href="#Network-63"><span class="linenos"> 63</span></a><span class="sd">            lastLayerNodes (ndarray): Output from the previous layer.</span>
</span><span id="Network-64"><a href="#Network-64"><span class="linenos"> 64</span></a><span class="sd">            lastLayerWeights (ndarray): Weights connecting the previous layer.</span>
</span><span id="Network-65"><a href="#Network-65"><span class="linenos"> 65</span></a><span class="sd">            biases (ndarray): Biases of the current layer.</span>
</span><span id="Network-66"><a href="#Network-66"><span class="linenos"> 66</span></a><span class="sd">            currentLayer (list): List containing layer size and activation function.</span>
</span><span id="Network-67"><a href="#Network-67"><span class="linenos"> 67</span></a><span class="sd">        </span>
</span><span id="Network-68"><a href="#Network-68"><span class="linenos"> 68</span></a><span class="sd">        Returns:</span>
</span><span id="Network-69"><a href="#Network-69"><span class="linenos"> 69</span></a><span class="sd">            ndarray: Output of the current layer.</span>
</span><span id="Network-70"><a href="#Network-70"><span class="linenos"> 70</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="Network-71"><a href="#Network-71"><span class="linenos"> 71</span></a>        <span class="n">summ</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">lastLayerNodes</span><span class="p">,</span> <span class="n">lastLayerWeights</span><span class="p">)</span> <span class="o">+</span> <span class="n">biases</span>
</span><span id="Network-72"><a href="#Network-72"><span class="linenos"> 72</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">currentLayer</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">softMax</span><span class="p">):</span>
</span><span id="Network-73"><a href="#Network-73"><span class="linenos"> 73</span></a>            <span class="k">return</span> <span class="n">currentLayer</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">summ</span><span class="p">)</span>
</span><span id="Network-74"><a href="#Network-74"><span class="linenos"> 74</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="Network-75"><a href="#Network-75"><span class="linenos"> 75</span></a>            <span class="k">return</span> <span class="n">softMax</span><span class="p">(</span><span class="n">summ</span><span class="p">)</span>
</span><span id="Network-76"><a href="#Network-76"><span class="linenos"> 76</span></a>        
</span><span id="Network-77"><a href="#Network-77"><span class="linenos"> 77</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forwardPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">):</span>
</span><span id="Network-78"><a href="#Network-78"><span class="linenos"> 78</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="Network-79"><a href="#Network-79"><span class="linenos"> 79</span></a><span class="sd">        Perform forward propagation through the network for the given input data.</span>
</span><span id="Network-80"><a href="#Network-80"><span class="linenos"> 80</span></a>
</span><span id="Network-81"><a href="#Network-81"><span class="linenos"> 81</span></a><span class="sd">        Args:</span>
</span><span id="Network-82"><a href="#Network-82"><span class="linenos"> 82</span></a><span class="sd">            inputData (list): Input data for the network.</span>
</span><span id="Network-83"><a href="#Network-83"><span class="linenos"> 83</span></a>
</span><span id="Network-84"><a href="#Network-84"><span class="linenos"> 84</span></a><span class="sd">        Returns:</span>
</span><span id="Network-85"><a href="#Network-85"><span class="linenos"> 85</span></a><span class="sd">            list of ndarray: List containing outputs of each layer including input layer.</span>
</span><span id="Network-86"><a href="#Network-86"><span class="linenos"> 86</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="Network-87"><a href="#Network-87"><span class="linenos"> 87</span></a>        <span class="n">layerNodes</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputData</span><span class="p">)]</span>
</span><span id="Network-88"><a href="#Network-88"><span class="linenos"> 88</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)):</span>
</span><span id="Network-89"><a href="#Network-89"><span class="linenos"> 89</span></a>            <span class="n">layerNodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_calculateLayerNodes</span><span class="p">(</span><span class="n">layerNodes</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">])))</span>
</span><span id="Network-90"><a href="#Network-90"><span class="linenos"> 90</span></a>        <span class="k">return</span> <span class="n">layerNodes</span>
</span><span id="Network-91"><a href="#Network-91"><span class="linenos"> 91</span></a>    
</span><span id="Network-92"><a href="#Network-92"><span class="linenos"> 92</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_backPropgation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layerNodes</span><span class="p">,</span> <span class="n">trueValues</span><span class="p">,</span> <span class="n">returnErrorTermForCNN</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="Network-93"><a href="#Network-93"><span class="linenos"> 93</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="Network-94"><a href="#Network-94"><span class="linenos"> 94</span></a><span class="sd">        Perform backpropagation over the network layers to compute gradients for weights and biases.</span>
</span><span id="Network-95"><a href="#Network-95"><span class="linenos"> 95</span></a>
</span><span id="Network-96"><a href="#Network-96"><span class="linenos"> 96</span></a><span class="sd">        Args:</span>
</span><span id="Network-97"><a href="#Network-97"><span class="linenos"> 97</span></a><span class="sd">            layerNodes (list of ndarray): List of output values for each layer.</span>
</span><span id="Network-98"><a href="#Network-98"><span class="linenos"> 98</span></a><span class="sd">            weights (list of ndarray): List of weights for each layer.</span>
</span><span id="Network-99"><a href="#Network-99"><span class="linenos"> 99</span></a><span class="sd">            biases (list of ndarray): List of biases for each layer.</span>
</span><span id="Network-100"><a href="#Network-100"><span class="linenos">100</span></a><span class="sd">            trueValues (ndarray): True target values for the output layer.</span>
</span><span id="Network-101"><a href="#Network-101"><span class="linenos">101</span></a><span class="sd">            returnErrorTermForCNN (bool, optional): Whether to return error terms for CNN backpropagation. Defaults to False.</span>
</span><span id="Network-102"><a href="#Network-102"><span class="linenos">102</span></a>
</span><span id="Network-103"><a href="#Network-103"><span class="linenos">103</span></a><span class="sd">        Returns:</span>
</span><span id="Network-104"><a href="#Network-104"><span class="linenos">104</span></a><span class="sd">            weightGradients (list of ndarray): Gradients of weights for each layer.</span>
</span><span id="Network-105"><a href="#Network-105"><span class="linenos">105</span></a><span class="sd">            biasGradients (list of ndarray): Gradients of biases for each layer.</span>
</span><span id="Network-106"><a href="#Network-106"><span class="linenos">106</span></a><span class="sd">            If returnErrorTermForCNN is True:</span>
</span><span id="Network-107"><a href="#Network-107"><span class="linenos">107</span></a><span class="sd">                hiddenWeightErrorTermsForCNNBackpropgation (ndarray): Error terms from the output layer weights.   </span>
</span><span id="Network-108"><a href="#Network-108"><span class="linenos">108</span></a><span class="sd">        &quot;&quot;&quot;</span>  
</span><span id="Network-109"><a href="#Network-109"><span class="linenos">109</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">returnErrorTermForCNN</span><span class="p">):</span>
</span><span id="Network-110"><a href="#Network-110"><span class="linenos">110</span></a>            <span class="k">return</span> <span class="n">backPropgation</span><span class="o">.</span><span class="n">_backPropgation</span><span class="p">(</span><span class="n">layerNodes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">,</span> <span class="n">trueValues</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lossFunction</span><span class="p">,</span> <span class="n">returnErrorTermForCNN</span><span class="p">)</span>
</span><span id="Network-111"><a href="#Network-111"><span class="linenos">111</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="Network-112"><a href="#Network-112"><span class="linenos">112</span></a>            <span class="n">weightGradients</span><span class="p">,</span> <span class="n">biasGradients</span> <span class="o">=</span> <span class="n">backPropgation</span><span class="o">.</span><span class="n">_backPropgation</span><span class="p">(</span><span class="n">layerNodes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">,</span> <span class="n">trueValues</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lossFunction</span><span class="p">,</span> <span class="n">returnErrorTermForCNN</span><span class="p">)</span>
</span><span id="Network-113"><a href="#Network-113"><span class="linenos">113</span></a>        
</span><span id="Network-114"><a href="#Network-114"><span class="linenos">114</span></a>        <span class="n">Parameters</span> <span class="o">=</span>  <span class="p">{</span>
</span><span id="Network-115"><a href="#Network-115"><span class="linenos">115</span></a>            <span class="s2">&quot;weight&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span>
</span><span id="Network-116"><a href="#Network-116"><span class="linenos">116</span></a>            <span class="s2">&quot;biases&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">,</span>
</span><span id="Network-117"><a href="#Network-117"><span class="linenos">117</span></a>        <span class="p">}</span>
</span><span id="Network-118"><a href="#Network-118"><span class="linenos">118</span></a>  
</span><span id="Network-119"><a href="#Network-119"><span class="linenos">119</span></a>        <span class="n">Gradients</span> <span class="o">=</span>  <span class="p">{</span>
</span><span id="Network-120"><a href="#Network-120"><span class="linenos">120</span></a>            <span class="s2">&quot;weight&quot;</span><span class="p">:</span> <span class="n">weightGradients</span><span class="p">,</span>
</span><span id="Network-121"><a href="#Network-121"><span class="linenos">121</span></a>            <span class="s2">&quot;biases&quot;</span><span class="p">:</span> <span class="n">biasGradients</span><span class="p">,</span>
</span><span id="Network-122"><a href="#Network-122"><span class="linenos">122</span></a>        <span class="p">}</span>
</span><span id="Network-123"><a href="#Network-123"><span class="linenos">123</span></a>        <span class="k">return</span> <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span> 
</span><span id="Network-124"><a href="#Network-124"><span class="linenos">124</span></a>
</span><span id="Network-125"><a href="#Network-125"><span class="linenos">125</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">optimise</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">learningRate</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">):</span>
</span><span id="Network-126"><a href="#Network-126"><span class="linenos">126</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimisationFunction</span><span class="o">.</span><span class="n">optimiser</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">useBatches</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">learningRate</span><span class="p">)</span> 
</span><span id="Network-127"><a href="#Network-127"><span class="linenos">127</span></a>
</span><span id="Network-128"><a href="#Network-128"><span class="linenos">128</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">epochs</span><span class="p">):</span>
</span><span id="Network-129"><a href="#Network-129"><span class="linenos">129</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="Network-130"><a href="#Network-130"><span class="linenos">130</span></a><span class="sd">        Train the neural network using the specified optimisation function.</span>
</span><span id="Network-131"><a href="#Network-131"><span class="linenos">131</span></a>
</span><span id="Network-132"><a href="#Network-132"><span class="linenos">132</span></a><span class="sd">        Args:</span>
</span><span id="Network-133"><a href="#Network-133"><span class="linenos">133</span></a><span class="sd">            inputData (list of lists): All of the training input data</span>
</span><span id="Network-134"><a href="#Network-134"><span class="linenos">134</span></a><span class="sd">            labels (list of ndarray): All of the labels for all the input data.</span>
</span><span id="Network-135"><a href="#Network-135"><span class="linenos">135</span></a><span class="sd">            epochs (int): Number of training epochs.</span>
</span><span id="Network-136"><a href="#Network-136"><span class="linenos">136</span></a><span class="sd">        </span>
</span><span id="Network-137"><a href="#Network-137"><span class="linenos">137</span></a><span class="sd">        Returns:</span>
</span><span id="Network-138"><a href="#Network-138"><span class="linenos">138</span></a><span class="sd">            float: Average accauracy over all epochs.</span>
</span><span id="Network-139"><a href="#Network-139"><span class="linenos">139</span></a><span class="sd">            float: Average loss over all epochs.</span>
</span><span id="Network-140"><a href="#Network-140"><span class="linenos">140</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="Network-141"><a href="#Network-141"><span class="linenos">141</span></a>        <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputData</span><span class="p">)</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Dimension wrong size, got </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputData</span><span class="p">)</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">, expected 2&quot;</span>
</span><span id="Network-142"><a href="#Network-142"><span class="linenos">142</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_checkIfNetworkCorrect</span><span class="p">()</span>
</span><span id="Network-143"><a href="#Network-143"><span class="linenos">143</span></a>        <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="Network-144"><a href="#Network-144"><span class="linenos">144</span></a>        <span class="n">totalLoss</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="Network-145"><a href="#Network-145"><span class="linenos">145</span></a>        <span class="n">nodes</span><span class="p">,</span> <span class="n">Parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimise</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">learningRate</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">)</span>        
</span><span id="Network-146"><a href="#Network-146"><span class="linenos">146</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;weight&quot;</span><span class="p">]</span>
</span><span id="Network-147"><a href="#Network-147"><span class="linenos">147</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;biases&quot;</span><span class="p">]</span>
</span><span id="Network-148"><a href="#Network-148"><span class="linenos">148</span></a>
</span><span id="Network-149"><a href="#Network-149"><span class="linenos">149</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="Network-150"><a href="#Network-150"><span class="linenos">150</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="Network-151"><a href="#Network-151"><span class="linenos">151</span></a>            
</span><span id="Network-152"><a href="#Network-152"><span class="linenos">152</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">nodes</span><span class="p">)):</span> <span class="c1"># nodes[i][-1] shape: (batchSize, outputSize)</span>
</span><span id="Network-153"><a href="#Network-153"><span class="linenos">153</span></a>            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
</span><span id="Network-154"><a href="#Network-154"><span class="linenos">154</span></a>                <span class="n">totalLoss</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lossFunction</span><span class="p">(</span><span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="n">j</span><span class="p">],</span> <span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="o">+</span><span class="n">j</span><span class="p">])</span>
</span><span id="Network-155"><a href="#Network-155"><span class="linenos">155</span></a>
</span><span id="Network-156"><a href="#Network-156"><span class="linenos">156</span></a>                <span class="n">nodeIndex</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="n">j</span><span class="p">])</span>
</span><span id="Network-157"><a href="#Network-157"><span class="linenos">157</span></a>                <span class="n">labelIndex</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="o">+</span><span class="n">j</span><span class="p">])</span>
</span><span id="Network-158"><a href="#Network-158"><span class="linenos">158</span></a>                <span class="k">if</span><span class="p">(</span><span class="n">nodeIndex</span> <span class="o">==</span> <span class="n">labelIndex</span><span class="p">):</span>
</span><span id="Network-159"><a href="#Network-159"><span class="linenos">159</span></a>                    <span class="n">correct</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span id="Network-160"><a href="#Network-160"><span class="linenos">160</span></a>        <span class="k">return</span> <span class="n">correct</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span> <span class="n">totalLoss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
</span><span id="Network-161"><a href="#Network-161"><span class="linenos">161</span></a>    
</span><span id="Network-162"><a href="#Network-162"><span class="linenos">162</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_checkIfNetworkCorrect</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="c1">#this is to check if activation functions/loss functions adhere to certain rule</span>
</span><span id="Network-163"><a href="#Network-163"><span class="linenos">163</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span> <span class="c1">#checks if softmax is used for any activation func that isnt output layer</span>
</span><span id="Network-164"><a href="#Network-164"><span class="linenos">164</span></a>            <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">softMax</span><span class="p">):</span> <span class="c1">#if so it stops the user</span>
</span><span id="Network-165"><a href="#Network-165"><span class="linenos">165</span></a>                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Softmax shouldnt be used in non ouput layers. Error at Layer </span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="Network-166"><a href="#Network-166"><span class="linenos">166</span></a>        <span class="n">usingSoftMax</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">softMax</span>
</span><span id="Network-167"><a href="#Network-167"><span class="linenos">167</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">usingSoftMax</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="Network-168"><a href="#Network-168"><span class="linenos">168</span></a>            <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lossFunction</span> <span class="o">!=</span> <span class="n">CrossEntropyLossFunction</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">lossFunction</span> <span class="o">!=</span> <span class="n">NormalisedCrossEntropyLossFunction</span><span class="p">):</span> <span class="c1">#checks if softmax is used without cross entropy loss function</span>
</span><span id="Network-169"><a href="#Network-169"><span class="linenos">169</span></a>                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Softmax output layer requires Cross Entropy loss function&quot;</span><span class="p">)</span> <span class="c1">#if so stops the user</span>
</span><span id="Network-170"><a href="#Network-170"><span class="linenos">170</span></a>        <span class="k">elif</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lossFunction</span> <span class="o">==</span> <span class="n">CrossEntropyLossFunction</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">lossFunction</span> <span class="o">==</span> <span class="n">NormalisedCrossEntropyLossFunction</span><span class="p">):</span>
</span><span id="Network-171"><a href="#Network-171"><span class="linenos">171</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cross Entropy loss function requires Softmax output layer&quot;</span><span class="p">)</span> <span class="c1">#if so stops the user</span>
</span></pre></div>


    

                            <div id="Network.__init__" class="classattr">
                                        <input id="Network.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">Network</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">lossFunc</span><span class="o">=</span><span class="s1">&#39;MSE&#39;</span>,</span><span class="param">	<span class="n">learningRate</span><span class="o">=</span><span class="mf">0.01</span>,</span><span class="param">	<span class="n">useBatches</span><span class="o">=</span><span class="kc">False</span>,</span><span class="param">	<span class="n">batchSize</span><span class="o">=</span><span class="mi">32</span>,</span><span class="param">	optimisationFunction=&lt;class &#x27;<a href="#Adam">Adam</a>&#x27;&gt;</span>)</span>

                <label class="view-source-button" for="Network.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Network.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Network.__init__-12"><a href="#Network.__init__-12"><span class="linenos">12</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lossFunc</span> <span class="o">=</span> <span class="s2">&quot;MSE&quot;</span><span class="p">,</span> <span class="n">learningRate</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">useBatches</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">batchSize</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span> <span class="n">optimisationFunction</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">):</span>
</span><span id="Network.__init__-13"><a href="#Network.__init__-13"><span class="linenos">13</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="Network.__init__-14"><a href="#Network.__init__-14"><span class="linenos">14</span></a><span class="sd">        Args:</span>
</span><span id="Network.__init__-15"><a href="#Network.__init__-15"><span class="linenos">15</span></a><span class="sd">            lossFunc (str): Loss function name (&#39;mse&#39;, &#39;mae&#39;, &#39;cross&#39;). Default is &quot;MSE&quot;.</span>
</span><span id="Network.__init__-16"><a href="#Network.__init__-16"><span class="linenos">16</span></a><span class="sd">            learningRate (float, optional): Learning rate for training. Default is 0.01.</span>
</span><span id="Network.__init__-17"><a href="#Network.__init__-17"><span class="linenos">17</span></a><span class="sd">            optimisationFunc (str, optional): Optimisaztion method (&#39;gd&#39;, &#39;sgd&#39;, &#39;batching&#39;). Default is &quot;gd&quot;.</span>
</span><span id="Network.__init__-18"><a href="#Network.__init__-18"><span class="linenos">18</span></a><span class="sd">            useBatches (bool, optional): Wether to use mini batches. Default is False.</span>
</span><span id="Network.__init__-19"><a href="#Network.__init__-19"><span class="linenos">19</span></a><span class="sd">            batchSize (int, optional): size of mini batches. Default is 32.</span>
</span><span id="Network.__init__-20"><a href="#Network.__init__-20"><span class="linenos">20</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="Network.__init__-21"><a href="#Network.__init__-21"><span class="linenos">21</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="Network.__init__-22"><a href="#Network.__init__-22"><span class="linenos">22</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="Network.__init__-23"><a href="#Network.__init__-23"><span class="linenos">23</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="Network.__init__-24"><a href="#Network.__init__-24"><span class="linenos">24</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">learningRate</span> <span class="o">=</span> <span class="n">learningRate</span>
</span><span id="Network.__init__-25"><a href="#Network.__init__-25"><span class="linenos">25</span></a>
</span><span id="Network.__init__-26"><a href="#Network.__init__-26"><span class="linenos">26</span></a>        <span class="n">lossFunctionDict</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="Network.__init__-27"><a href="#Network.__init__-27"><span class="linenos">27</span></a>            <span class="s2">&quot;mse&quot;</span><span class="p">:</span> <span class="n">MSELossFunction</span><span class="p">,</span>
</span><span id="Network.__init__-28"><a href="#Network.__init__-28"><span class="linenos">28</span></a>            <span class="s2">&quot;mae&quot;</span><span class="p">:</span> <span class="n">MAELossFunction</span><span class="p">,</span>
</span><span id="Network.__init__-29"><a href="#Network.__init__-29"><span class="linenos">29</span></a>            <span class="s2">&quot;cross entropy&quot;</span><span class="p">:</span> <span class="n">CrossEntropyLossFunction</span><span class="p">,</span><span class="s2">&quot;cross&quot;</span><span class="p">:</span> <span class="n">CrossEntropyLossFunction</span><span class="p">,</span>
</span><span id="Network.__init__-30"><a href="#Network.__init__-30"><span class="linenos">30</span></a>            <span class="s2">&quot;normalised cross entropy&quot;</span><span class="p">:</span> <span class="n">NormalisedCrossEntropyLossFunction</span><span class="p">,</span><span class="s2">&quot;normalised cross&quot;</span><span class="p">:</span> <span class="n">NormalisedCrossEntropyLossFunction</span><span class="p">,</span>
</span><span id="Network.__init__-31"><a href="#Network.__init__-31"><span class="linenos">31</span></a>        <span class="p">}</span>
</span><span id="Network.__init__-32"><a href="#Network.__init__-32"><span class="linenos">32</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">lossFunction</span> <span class="o">=</span> <span class="n">lossFunctionDict</span><span class="p">[</span><span class="n">lossFunc</span><span class="o">.</span><span class="n">lower</span><span class="p">()]</span>
</span><span id="Network.__init__-33"><a href="#Network.__init__-33"><span class="linenos">33</span></a>
</span><span id="Network.__init__-34"><a href="#Network.__init__-34"><span class="linenos">34</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">optimisationFunction</span> <span class="o">=</span> <span class="n">optimisationFunction</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">forwardPropagation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backPropgation</span><span class="p">)</span>
</span><span id="Network.__init__-35"><a href="#Network.__init__-35"><span class="linenos">35</span></a>
</span><span id="Network.__init__-36"><a href="#Network.__init__-36"><span class="linenos">36</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">useBatches</span> <span class="o">=</span> <span class="n">useBatches</span>
</span><span id="Network.__init__-37"><a href="#Network.__init__-37"><span class="linenos">37</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span> <span class="o">=</span> <span class="n">batchSize</span>
</span></pre></div>


            <div class="docstring"><p>Args:
    lossFunc (str): Loss function name ('mse', 'mae', 'cross'). Default is "MSE".
    learningRate (float, optional): Learning rate for training. Default is 0.01.
    optimisationFunc (str, optional): Optimisaztion method ('gd', 'sgd', 'batching'). Default is "gd".
    useBatches (bool, optional): Wether to use mini batches. Default is False.
    batchSize (int, optional): size of mini batches. Default is 32.</p>
</div>


                            </div>
                            <div id="Network.layers" class="classattr">
                                <div class="attr variable">
            <span class="name">layers</span>

        
    </div>
    <a class="headerlink" href="#Network.layers"></a>
    
    

                            </div>
                            <div id="Network.weights" class="classattr">
                                <div class="attr variable">
            <span class="name">weights</span>

        
    </div>
    <a class="headerlink" href="#Network.weights"></a>
    
    

                            </div>
                            <div id="Network.biases" class="classattr">
                                <div class="attr variable">
            <span class="name">biases</span>

        
    </div>
    <a class="headerlink" href="#Network.biases"></a>
    
    

                            </div>
                            <div id="Network.learningRate" class="classattr">
                                <div class="attr variable">
            <span class="name">learningRate</span>

        
    </div>
    <a class="headerlink" href="#Network.learningRate"></a>
    
    

                            </div>
                            <div id="Network.lossFunction" class="classattr">
                                <div class="attr variable">
            <span class="name">lossFunction</span>

        
    </div>
    <a class="headerlink" href="#Network.lossFunction"></a>
    
    

                            </div>
                            <div id="Network.optimisationFunction" class="classattr">
                                <div class="attr variable">
            <span class="name">optimisationFunction</span>

        
    </div>
    <a class="headerlink" href="#Network.optimisationFunction"></a>
    
    

                            </div>
                            <div id="Network.useBatches" class="classattr">
                                <div class="attr variable">
            <span class="name">useBatches</span>

        
    </div>
    <a class="headerlink" href="#Network.useBatches"></a>
    
    

                            </div>
                            <div id="Network.batchSize" class="classattr">
                                <div class="attr variable">
            <span class="name">batchSize</span>

        
    </div>
    <a class="headerlink" href="#Network.batchSize"></a>
    
    

                            </div>
                            <div id="Network.addLayer" class="classattr">
                                        <input id="Network.addLayer-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">addLayer</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">size</span>, </span><span class="param"><span class="n">activationFunction</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="Network.addLayer-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Network.addLayer"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Network.addLayer-39"><a href="#Network.addLayer-39"><span class="linenos">39</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">addLayer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">activationFunction</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">):</span>
</span><span id="Network.addLayer-40"><a href="#Network.addLayer-40"><span class="linenos">40</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="Network.addLayer-41"><a href="#Network.addLayer-41"><span class="linenos">41</span></a><span class="sd">        Add a layer to the network with the specified number of nodes and activation function.</span>
</span><span id="Network.addLayer-42"><a href="#Network.addLayer-42"><span class="linenos">42</span></a>
</span><span id="Network.addLayer-43"><a href="#Network.addLayer-43"><span class="linenos">43</span></a><span class="sd">        Args:</span>
</span><span id="Network.addLayer-44"><a href="#Network.addLayer-44"><span class="linenos">44</span></a><span class="sd">            size (int): Number of nodes in the new layer.</span>
</span><span id="Network.addLayer-45"><a href="#Network.addLayer-45"><span class="linenos">45</span></a><span class="sd">            activationFunction (str, optional): Activation function name (&#39;relu&#39;, &#39;sigmoid&#39;, &#39;linear&#39;, &#39;tanh&#39;, &#39;softmax&#39;). Default is &quot;relu&quot;.</span>
</span><span id="Network.addLayer-46"><a href="#Network.addLayer-46"><span class="linenos">46</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="Network.addLayer-47"><a href="#Network.addLayer-47"><span class="linenos">47</span></a>        <span class="n">funcs</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="Network.addLayer-48"><a href="#Network.addLayer-48"><span class="linenos">48</span></a>            <span class="s2">&quot;relu&quot;</span><span class="p">:</span> <span class="n">relu</span><span class="p">,</span>
</span><span id="Network.addLayer-49"><a href="#Network.addLayer-49"><span class="linenos">49</span></a>            <span class="s2">&quot;sigmoid&quot;</span><span class="p">:</span> <span class="n">sigmoid</span><span class="p">,</span>
</span><span id="Network.addLayer-50"><a href="#Network.addLayer-50"><span class="linenos">50</span></a>            <span class="s2">&quot;linear&quot;</span><span class="p">:</span> <span class="n">linear</span><span class="p">,</span>
</span><span id="Network.addLayer-51"><a href="#Network.addLayer-51"><span class="linenos">51</span></a>            <span class="s2">&quot;tanh&quot;</span><span class="p">:</span> <span class="n">tanH</span><span class="p">,</span>
</span><span id="Network.addLayer-52"><a href="#Network.addLayer-52"><span class="linenos">52</span></a>            <span class="s2">&quot;softmax&quot;</span><span class="p">:</span> <span class="n">softMax</span><span class="p">,</span>
</span><span id="Network.addLayer-53"><a href="#Network.addLayer-53"><span class="linenos">53</span></a>        <span class="p">}</span>
</span><span id="Network.addLayer-54"><a href="#Network.addLayer-54"><span class="linenos">54</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">activationFunction</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">funcs</span><span class="p">):</span>
</span><span id="Network.addLayer-55"><a href="#Network.addLayer-55"><span class="linenos">55</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Activation function not made: </span><span class="si">{</span><span class="n">activationFunction</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="Network.addLayer-56"><a href="#Network.addLayer-56"><span class="linenos">56</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">size</span><span class="p">,</span> <span class="n">funcs</span><span class="p">[</span><span class="n">activationFunction</span><span class="o">.</span><span class="n">lower</span><span class="p">()]])</span>
</span></pre></div>


            <div class="docstring"><p>Add a layer to the network with the specified number of nodes and activation function.</p>

<p>Args:
    size (int): Number of nodes in the new layer.
    activationFunction (str, optional): Activation function name ('relu', 'sigmoid', 'linear', 'tanh', 'softmax'). Default is "relu".</p>
</div>


                            </div>
                            <div id="Network.forwardPropagation" class="classattr">
                                        <input id="Network.forwardPropagation-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forwardPropagation</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">inputData</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="Network.forwardPropagation-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Network.forwardPropagation"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Network.forwardPropagation-77"><a href="#Network.forwardPropagation-77"><span class="linenos">77</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forwardPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">):</span>
</span><span id="Network.forwardPropagation-78"><a href="#Network.forwardPropagation-78"><span class="linenos">78</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="Network.forwardPropagation-79"><a href="#Network.forwardPropagation-79"><span class="linenos">79</span></a><span class="sd">        Perform forward propagation through the network for the given input data.</span>
</span><span id="Network.forwardPropagation-80"><a href="#Network.forwardPropagation-80"><span class="linenos">80</span></a>
</span><span id="Network.forwardPropagation-81"><a href="#Network.forwardPropagation-81"><span class="linenos">81</span></a><span class="sd">        Args:</span>
</span><span id="Network.forwardPropagation-82"><a href="#Network.forwardPropagation-82"><span class="linenos">82</span></a><span class="sd">            inputData (list): Input data for the network.</span>
</span><span id="Network.forwardPropagation-83"><a href="#Network.forwardPropagation-83"><span class="linenos">83</span></a>
</span><span id="Network.forwardPropagation-84"><a href="#Network.forwardPropagation-84"><span class="linenos">84</span></a><span class="sd">        Returns:</span>
</span><span id="Network.forwardPropagation-85"><a href="#Network.forwardPropagation-85"><span class="linenos">85</span></a><span class="sd">            list of ndarray: List containing outputs of each layer including input layer.</span>
</span><span id="Network.forwardPropagation-86"><a href="#Network.forwardPropagation-86"><span class="linenos">86</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="Network.forwardPropagation-87"><a href="#Network.forwardPropagation-87"><span class="linenos">87</span></a>        <span class="n">layerNodes</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputData</span><span class="p">)]</span>
</span><span id="Network.forwardPropagation-88"><a href="#Network.forwardPropagation-88"><span class="linenos">88</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)):</span>
</span><span id="Network.forwardPropagation-89"><a href="#Network.forwardPropagation-89"><span class="linenos">89</span></a>            <span class="n">layerNodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_calculateLayerNodes</span><span class="p">(</span><span class="n">layerNodes</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">])))</span>
</span><span id="Network.forwardPropagation-90"><a href="#Network.forwardPropagation-90"><span class="linenos">90</span></a>        <span class="k">return</span> <span class="n">layerNodes</span>
</span></pre></div>


            <div class="docstring"><p>Perform forward propagation through the network for the given input data.</p>

<p>Args:
    inputData (list): Input data for the network.</p>

<p>Returns:
    list of ndarray: List containing outputs of each layer including input layer.</p>
</div>


                            </div>
                            <div id="Network.optimise" class="classattr">
                                        <input id="Network.optimise-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">optimise</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">inputData</span>, </span><span class="param"><span class="n">labels</span>, </span><span class="param"><span class="n">learningRate</span>, </span><span class="param"><span class="n">batchSize</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="Network.optimise-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Network.optimise"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Network.optimise-125"><a href="#Network.optimise-125"><span class="linenos">125</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">optimise</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">learningRate</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">):</span>
</span><span id="Network.optimise-126"><a href="#Network.optimise-126"><span class="linenos">126</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimisationFunction</span><span class="o">.</span><span class="n">optimiser</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">useBatches</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">learningRate</span><span class="p">)</span> 
</span></pre></div>


    

                            </div>
                            <div id="Network.train" class="classattr">
                                        <input id="Network.train-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">train</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">inputData</span>, </span><span class="param"><span class="n">labels</span>, </span><span class="param"><span class="n">epochs</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="Network.train-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Network.train"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Network.train-128"><a href="#Network.train-128"><span class="linenos">128</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">epochs</span><span class="p">):</span>
</span><span id="Network.train-129"><a href="#Network.train-129"><span class="linenos">129</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="Network.train-130"><a href="#Network.train-130"><span class="linenos">130</span></a><span class="sd">        Train the neural network using the specified optimisation function.</span>
</span><span id="Network.train-131"><a href="#Network.train-131"><span class="linenos">131</span></a>
</span><span id="Network.train-132"><a href="#Network.train-132"><span class="linenos">132</span></a><span class="sd">        Args:</span>
</span><span id="Network.train-133"><a href="#Network.train-133"><span class="linenos">133</span></a><span class="sd">            inputData (list of lists): All of the training input data</span>
</span><span id="Network.train-134"><a href="#Network.train-134"><span class="linenos">134</span></a><span class="sd">            labels (list of ndarray): All of the labels for all the input data.</span>
</span><span id="Network.train-135"><a href="#Network.train-135"><span class="linenos">135</span></a><span class="sd">            epochs (int): Number of training epochs.</span>
</span><span id="Network.train-136"><a href="#Network.train-136"><span class="linenos">136</span></a><span class="sd">        </span>
</span><span id="Network.train-137"><a href="#Network.train-137"><span class="linenos">137</span></a><span class="sd">        Returns:</span>
</span><span id="Network.train-138"><a href="#Network.train-138"><span class="linenos">138</span></a><span class="sd">            float: Average accauracy over all epochs.</span>
</span><span id="Network.train-139"><a href="#Network.train-139"><span class="linenos">139</span></a><span class="sd">            float: Average loss over all epochs.</span>
</span><span id="Network.train-140"><a href="#Network.train-140"><span class="linenos">140</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="Network.train-141"><a href="#Network.train-141"><span class="linenos">141</span></a>        <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputData</span><span class="p">)</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Dimension wrong size, got </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputData</span><span class="p">)</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">, expected 2&quot;</span>
</span><span id="Network.train-142"><a href="#Network.train-142"><span class="linenos">142</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_checkIfNetworkCorrect</span><span class="p">()</span>
</span><span id="Network.train-143"><a href="#Network.train-143"><span class="linenos">143</span></a>        <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="Network.train-144"><a href="#Network.train-144"><span class="linenos">144</span></a>        <span class="n">totalLoss</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="Network.train-145"><a href="#Network.train-145"><span class="linenos">145</span></a>        <span class="n">nodes</span><span class="p">,</span> <span class="n">Parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimise</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">learningRate</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">)</span>        
</span><span id="Network.train-146"><a href="#Network.train-146"><span class="linenos">146</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;weight&quot;</span><span class="p">]</span>
</span><span id="Network.train-147"><a href="#Network.train-147"><span class="linenos">147</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;biases&quot;</span><span class="p">]</span>
</span><span id="Network.train-148"><a href="#Network.train-148"><span class="linenos">148</span></a>
</span><span id="Network.train-149"><a href="#Network.train-149"><span class="linenos">149</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="Network.train-150"><a href="#Network.train-150"><span class="linenos">150</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="Network.train-151"><a href="#Network.train-151"><span class="linenos">151</span></a>            
</span><span id="Network.train-152"><a href="#Network.train-152"><span class="linenos">152</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">nodes</span><span class="p">)):</span> <span class="c1"># nodes[i][-1] shape: (batchSize, outputSize)</span>
</span><span id="Network.train-153"><a href="#Network.train-153"><span class="linenos">153</span></a>            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
</span><span id="Network.train-154"><a href="#Network.train-154"><span class="linenos">154</span></a>                <span class="n">totalLoss</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lossFunction</span><span class="p">(</span><span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="n">j</span><span class="p">],</span> <span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="o">+</span><span class="n">j</span><span class="p">])</span>
</span><span id="Network.train-155"><a href="#Network.train-155"><span class="linenos">155</span></a>
</span><span id="Network.train-156"><a href="#Network.train-156"><span class="linenos">156</span></a>                <span class="n">nodeIndex</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="n">j</span><span class="p">])</span>
</span><span id="Network.train-157"><a href="#Network.train-157"><span class="linenos">157</span></a>                <span class="n">labelIndex</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="o">+</span><span class="n">j</span><span class="p">])</span>
</span><span id="Network.train-158"><a href="#Network.train-158"><span class="linenos">158</span></a>                <span class="k">if</span><span class="p">(</span><span class="n">nodeIndex</span> <span class="o">==</span> <span class="n">labelIndex</span><span class="p">):</span>
</span><span id="Network.train-159"><a href="#Network.train-159"><span class="linenos">159</span></a>                    <span class="n">correct</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span id="Network.train-160"><a href="#Network.train-160"><span class="linenos">160</span></a>        <span class="k">return</span> <span class="n">correct</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span> <span class="n">totalLoss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Train the neural network using the specified optimisation function.</p>

<p>Args:
    inputData (list of lists): All of the training input data
    labels (list of ndarray): All of the labels for all the input data.
    epochs (int): Number of training epochs.</p>

<p>Returns:
    float: Average accauracy over all epochs.
    float: Average loss over all epochs.</p>
</div>


                            </div>
                </section>
                <section id="CNNModel">
                            <input id="CNNModel-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">CNNModel</span>:

                <label class="view-source-button" for="CNNModel-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#CNNModel"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="CNNModel-12"><a href="#CNNModel-12"><span class="linenos"> 12</span></a><span class="k">class</span><span class="w"> </span><span class="nc">CNNModel</span><span class="p">:</span>
</span><span id="CNNModel-13"><a href="#CNNModel-13"><span class="linenos"> 13</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">NeuralNetworkClass</span><span class="p">,</span> <span class="n">optimisationFunction</span><span class="o">=</span><span class="n">Adam</span><span class="p">):</span>
</span><span id="CNNModel-14"><a href="#CNNModel-14"><span class="linenos"> 14</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="CNNModel-15"><a href="#CNNModel-15"><span class="linenos"> 15</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="CNNModel-16"><a href="#CNNModel-16"><span class="linenos"> 16</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="CNNModel-17"><a href="#CNNModel-17"><span class="linenos"> 17</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">NeuralNetworkClass</span> <span class="o">=</span> <span class="n">NeuralNetworkClass</span>
</span><span id="CNNModel-18"><a href="#CNNModel-18"><span class="linenos"> 18</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">optimisationFunction</span> <span class="o">=</span> <span class="n">optimisationFunction</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backpropagation</span><span class="p">)</span>
</span><span id="CNNModel-19"><a href="#CNNModel-19"><span class="linenos"> 19</span></a>    
</span><span id="CNNModel-20"><a href="#CNNModel-20"><span class="linenos"> 20</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">addLayer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer</span><span class="p">):</span>
</span><span id="CNNModel-21"><a href="#CNNModel-21"><span class="linenos"> 21</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="CNNModel-22"><a href="#CNNModel-22"><span class="linenos"> 22</span></a><span class="sd">        Adds a layer to the CNN model.</span>
</span><span id="CNNModel-23"><a href="#CNNModel-23"><span class="linenos"> 23</span></a>
</span><span id="CNNModel-24"><a href="#CNNModel-24"><span class="linenos"> 24</span></a><span class="sd">        Args:</span>
</span><span id="CNNModel-25"><a href="#CNNModel-25"><span class="linenos"> 25</span></a><span class="sd">            layer (class): ConvLayer, PoolingLayer, GlobalAveragePoolingLayer, ActivationLayer, and DenseLayer</span>
</span><span id="CNNModel-26"><a href="#CNNModel-26"><span class="linenos"> 26</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="CNNModel-27"><a href="#CNNModel-27"><span class="linenos"> 27</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
</span><span id="CNNModel-28"><a href="#CNNModel-28"><span class="linenos"> 28</span></a>    
</span><span id="CNNModel-29"><a href="#CNNModel-29"><span class="linenos"> 29</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputTensor</span><span class="p">):</span>
</span><span id="CNNModel-30"><a href="#CNNModel-30"><span class="linenos"> 30</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="CNNModel-31"><a href="#CNNModel-31"><span class="linenos"> 31</span></a><span class="sd">        Performs a forward pass through all layers.</span>
</span><span id="CNNModel-32"><a href="#CNNModel-32"><span class="linenos"> 32</span></a>
</span><span id="CNNModel-33"><a href="#CNNModel-33"><span class="linenos"> 33</span></a><span class="sd">        Args:</span>
</span><span id="CNNModel-34"><a href="#CNNModel-34"><span class="linenos"> 34</span></a><span class="sd">            inputTensor (ndarray): Input data tensor to the CNN.</span>
</span><span id="CNNModel-35"><a href="#CNNModel-35"><span class="linenos"> 35</span></a><span class="sd">        </span>
</span><span id="CNNModel-36"><a href="#CNNModel-36"><span class="linenos"> 36</span></a><span class="sd">        Returns:</span>
</span><span id="CNNModel-37"><a href="#CNNModel-37"><span class="linenos"> 37</span></a><span class="sd">            list: List of tensors output by each layer including the input.</span>
</span><span id="CNNModel-38"><a href="#CNNModel-38"><span class="linenos"> 38</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="CNNModel-39"><a href="#CNNModel-39"><span class="linenos"> 39</span></a>        <span class="n">allTensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">inputTensor</span><span class="p">]</span>
</span><span id="CNNModel-40"><a href="#CNNModel-40"><span class="linenos"> 40</span></a>        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
</span><span id="CNNModel-41"><a href="#CNNModel-41"><span class="linenos"> 41</span></a>            <span class="n">inputTensor</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">inputTensor</span><span class="p">)</span>
</span><span id="CNNModel-42"><a href="#CNNModel-42"><span class="linenos"> 42</span></a>            <span class="n">allTensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inputTensor</span><span class="p">)</span>
</span><span id="CNNModel-43"><a href="#CNNModel-43"><span class="linenos"> 43</span></a>        <span class="k">return</span> <span class="n">allTensors</span>
</span><span id="CNNModel-44"><a href="#CNNModel-44"><span class="linenos"> 44</span></a>
</span><span id="CNNModel-45"><a href="#CNNModel-45"><span class="linenos"> 45</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_backpropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">allTensors</span><span class="p">,</span> <span class="n">trueValues</span><span class="p">):</span>
</span><span id="CNNModel-46"><a href="#CNNModel-46"><span class="linenos"> 46</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="CNNModel-47"><a href="#CNNModel-47"><span class="linenos"> 47</span></a><span class="sd">        Performs backpropagation through all layers to compute gradients.</span>
</span><span id="CNNModel-48"><a href="#CNNModel-48"><span class="linenos"> 48</span></a>
</span><span id="CNNModel-49"><a href="#CNNModel-49"><span class="linenos"> 49</span></a><span class="sd">        Args:</span>
</span><span id="CNNModel-50"><a href="#CNNModel-50"><span class="linenos"> 50</span></a><span class="sd">            allTensors (list): List of all layer outputs from forward propagation.</span>
</span><span id="CNNModel-51"><a href="#CNNModel-51"><span class="linenos"> 51</span></a><span class="sd">        </span>
</span><span id="CNNModel-52"><a href="#CNNModel-52"><span class="linenos"> 52</span></a><span class="sd">        Returns:</span>
</span><span id="CNNModel-53"><a href="#CNNModel-53"><span class="linenos"> 53</span></a><span class="sd">            allWeightGradients (list): List of all the weight gradients calculated during backpropgation.</span>
</span><span id="CNNModel-54"><a href="#CNNModel-54"><span class="linenos"> 54</span></a><span class="sd">            allBiasGradients (list): List of all the bias gradients calculated during backpropgation.</span>
</span><span id="CNNModel-55"><a href="#CNNModel-55"><span class="linenos"> 55</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="CNNModel-56"><a href="#CNNModel-56"><span class="linenos"> 56</span></a>        <span class="n">weightGradients</span><span class="p">,</span> <span class="n">biasGradients</span><span class="p">,</span> <span class="n">errorTerms</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">_backpropagation</span><span class="p">(</span><span class="n">trueValues</span><span class="p">)</span> <span class="c1"># &lt;-- this is a neural network </span>
</span><span id="CNNModel-57"><a href="#CNNModel-57"><span class="linenos"> 57</span></a>        <span class="n">allWeightGradients</span> <span class="o">=</span> <span class="p">[</span><span class="n">weightGradients</span><span class="p">]</span>
</span><span id="CNNModel-58"><a href="#CNNModel-58"><span class="linenos"> 58</span></a>        <span class="n">allBiasGradients</span> <span class="o">=</span> <span class="p">[</span><span class="n">biasGradients</span><span class="p">]</span>
</span><span id="CNNModel-59"><a href="#CNNModel-59"><span class="linenos"> 59</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
</span><span id="CNNModel-60"><a href="#CNNModel-60"><span class="linenos"> 60</span></a>            <span class="k">if</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">==</span> <span class="n">GlobalAveragePooling</span><span class="p">):</span>
</span><span id="CNNModel-61"><a href="#CNNModel-61"><span class="linenos"> 61</span></a>                <span class="n">errorTerms</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">_backpropagation</span><span class="p">(</span><span class="n">errorTerms</span><span class="p">)</span>
</span><span id="CNNModel-62"><a href="#CNNModel-62"><span class="linenos"> 62</span></a>            <span class="k">if</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">==</span> <span class="n">PoolingLayer</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">==</span> <span class="n">ActivationLayer</span><span class="p">):</span>
</span><span id="CNNModel-63"><a href="#CNNModel-63"><span class="linenos"> 63</span></a>                <span class="n">errorTerms</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">_backpropagation</span><span class="p">(</span><span class="n">errorTerms</span><span class="p">,</span> <span class="n">allTensors</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span><span id="CNNModel-64"><a href="#CNNModel-64"><span class="linenos"> 64</span></a>            <span class="k">elif</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">==</span> <span class="n">Conv2DLayer</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">==</span> <span class="n">Conv1DLayer</span><span class="p">):</span>
</span><span id="CNNModel-65"><a href="#CNNModel-65"><span class="linenos"> 65</span></a>                <span class="n">weightGradients</span><span class="p">,</span> <span class="n">biasGradients</span><span class="p">,</span> <span class="n">errorTerms</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">_backpropagation</span><span class="p">(</span><span class="n">errorTerms</span><span class="p">,</span> <span class="n">allTensors</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span><span id="CNNModel-66"><a href="#CNNModel-66"><span class="linenos"> 66</span></a>                <span class="n">allWeightGradients</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">weightGradients</span><span class="p">)</span>
</span><span id="CNNModel-67"><a href="#CNNModel-67"><span class="linenos"> 67</span></a>                <span class="n">allBiasGradients</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">biasGradients</span><span class="p">)</span>
</span><span id="CNNModel-68"><a href="#CNNModel-68"><span class="linenos"> 68</span></a>            <span class="k">elif</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">==</span> <span class="n">Dropout</span><span class="p">):</span>
</span><span id="CNNModel-69"><a href="#CNNModel-69"><span class="linenos"> 69</span></a>                <span class="n">errorTerms</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">_backpropagation</span><span class="p">(</span><span class="n">errorTerms</span><span class="p">)</span>
</span><span id="CNNModel-70"><a href="#CNNModel-70"><span class="linenos"> 70</span></a>        
</span><span id="CNNModel-71"><a href="#CNNModel-71"><span class="linenos"> 71</span></a>        <span class="n">Parameters</span> <span class="o">=</span>  <span class="p">{</span>
</span><span id="CNNModel-72"><a href="#CNNModel-72"><span class="linenos"> 72</span></a>            <span class="s2">&quot;weight&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span>
</span><span id="CNNModel-73"><a href="#CNNModel-73"><span class="linenos"> 73</span></a>            <span class="s2">&quot;biases&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">,</span>
</span><span id="CNNModel-74"><a href="#CNNModel-74"><span class="linenos"> 74</span></a>        <span class="p">}</span>
</span><span id="CNNModel-75"><a href="#CNNModel-75"><span class="linenos"> 75</span></a>  
</span><span id="CNNModel-76"><a href="#CNNModel-76"><span class="linenos"> 76</span></a>        <span class="n">Gradients</span> <span class="o">=</span>  <span class="p">{</span>
</span><span id="CNNModel-77"><a href="#CNNModel-77"><span class="linenos"> 77</span></a>            <span class="s2">&quot;weight&quot;</span><span class="p">:</span> <span class="n">allWeightGradients</span><span class="p">,</span>
</span><span id="CNNModel-78"><a href="#CNNModel-78"><span class="linenos"> 78</span></a>            <span class="s2">&quot;biases&quot;</span><span class="p">:</span> <span class="n">allBiasGradients</span><span class="p">,</span>
</span><span id="CNNModel-79"><a href="#CNNModel-79"><span class="linenos"> 79</span></a>        <span class="p">}</span>
</span><span id="CNNModel-80"><a href="#CNNModel-80"><span class="linenos"> 80</span></a>        <span class="k">return</span> <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span> 
</span><span id="CNNModel-81"><a href="#CNNModel-81"><span class="linenos"> 81</span></a>    
</span><span id="CNNModel-82"><a href="#CNNModel-82"><span class="linenos"> 82</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_optimser</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">useBatches</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">learningRate</span><span class="p">):</span>
</span><span id="CNNModel-83"><a href="#CNNModel-83"><span class="linenos"> 83</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="CNNModel-84"><a href="#CNNModel-84"><span class="linenos"> 84</span></a><span class="sd">        Runs the Adam optimiser either with or without batches.</span>
</span><span id="CNNModel-85"><a href="#CNNModel-85"><span class="linenos"> 85</span></a>
</span><span id="CNNModel-86"><a href="#CNNModel-86"><span class="linenos"> 86</span></a><span class="sd">        Args:</span>
</span><span id="CNNModel-87"><a href="#CNNModel-87"><span class="linenos"> 87</span></a><span class="sd">            inputData (ndarray): All the training data.</span>
</span><span id="CNNModel-88"><a href="#CNNModel-88"><span class="linenos"> 88</span></a><span class="sd">            labels (ndarray): All the true labels for the training data.</span>
</span><span id="CNNModel-89"><a href="#CNNModel-89"><span class="linenos"> 89</span></a><span class="sd">            useBatches (bool): Whether to use batching.</span>
</span><span id="CNNModel-90"><a href="#CNNModel-90"><span class="linenos"> 90</span></a><span class="sd">            weights (list): Current weights.</span>
</span><span id="CNNModel-91"><a href="#CNNModel-91"><span class="linenos"> 91</span></a><span class="sd">            biases (list): Current biases.</span>
</span><span id="CNNModel-92"><a href="#CNNModel-92"><span class="linenos"> 92</span></a><span class="sd">            batchSize (int): Size of batches.</span>
</span><span id="CNNModel-93"><a href="#CNNModel-93"><span class="linenos"> 93</span></a><span class="sd">            alpha (float): Learning rate.</span>
</span><span id="CNNModel-94"><a href="#CNNModel-94"><span class="linenos"> 94</span></a><span class="sd">            beta1 (float): Adam&#39;s beta1 parameter.</span>
</span><span id="CNNModel-95"><a href="#CNNModel-95"><span class="linenos"> 95</span></a><span class="sd">            beta2 (float): Adam&#39;s beta2 parameter.</span>
</span><span id="CNNModel-96"><a href="#CNNModel-96"><span class="linenos"> 96</span></a><span class="sd">            epsilon (float): Adam&#39;s epsilon parameter.</span>
</span><span id="CNNModel-97"><a href="#CNNModel-97"><span class="linenos"> 97</span></a><span class="sd">        </span>
</span><span id="CNNModel-98"><a href="#CNNModel-98"><span class="linenos"> 98</span></a><span class="sd">        Returns:</span>
</span><span id="CNNModel-99"><a href="#CNNModel-99"><span class="linenos"> 99</span></a><span class="sd">            list: The nodes (returned to calculate accuracy and loss).</span>
</span><span id="CNNModel-100"><a href="#CNNModel-100"><span class="linenos">100</span></a><span class="sd">            list: Updated weights after optimisation</span>
</span><span id="CNNModel-101"><a href="#CNNModel-101"><span class="linenos">101</span></a><span class="sd">            list: Updated biases after optimisation</span>
</span><span id="CNNModel-102"><a href="#CNNModel-102"><span class="linenos">102</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="CNNModel-103"><a href="#CNNModel-103"><span class="linenos">103</span></a>        <span class="n">allOutputs</span><span class="p">,</span> <span class="n">Parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimisationFunction</span><span class="o">.</span><span class="n">optimiser</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">useBatches</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">learningRate</span><span class="p">)</span>
</span><span id="CNNModel-104"><a href="#CNNModel-104"><span class="linenos">104</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;weight&quot;</span><span class="p">]</span>
</span><span id="CNNModel-105"><a href="#CNNModel-105"><span class="linenos">105</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;biases&quot;</span><span class="p">]</span>
</span><span id="CNNModel-106"><a href="#CNNModel-106"><span class="linenos">106</span></a>        <span class="k">return</span> <span class="n">allOutputs</span><span class="p">,</span> <span class="n">Parameters</span>
</span><span id="CNNModel-107"><a href="#CNNModel-107"><span class="linenos">107</span></a>    
</span><span id="CNNModel-108"><a href="#CNNModel-108"><span class="linenos">108</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">useBatches</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">learningRate</span><span class="p">):</span>
</span><span id="CNNModel-109"><a href="#CNNModel-109"><span class="linenos">109</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="CNNModel-110"><a href="#CNNModel-110"><span class="linenos">110</span></a><span class="sd">        Trains the CNN for one epoch and calculates accuracy and average loss.</span>
</span><span id="CNNModel-111"><a href="#CNNModel-111"><span class="linenos">111</span></a>
</span><span id="CNNModel-112"><a href="#CNNModel-112"><span class="linenos">112</span></a><span class="sd">        Args:</span>
</span><span id="CNNModel-113"><a href="#CNNModel-113"><span class="linenos">113</span></a><span class="sd">            inputData (ndarray): All the training data.</span>
</span><span id="CNNModel-114"><a href="#CNNModel-114"><span class="linenos">114</span></a><span class="sd">            labels (ndarray): All the true labels for the training data.</span>
</span><span id="CNNModel-115"><a href="#CNNModel-115"><span class="linenos">115</span></a><span class="sd">            useBatches (bool): Whether to use batching.</span>
</span><span id="CNNModel-116"><a href="#CNNModel-116"><span class="linenos">116</span></a><span class="sd">            batchSize (int): Size of batches.</span>
</span><span id="CNNModel-117"><a href="#CNNModel-117"><span class="linenos">117</span></a><span class="sd">            alpha (float, optional): Learning rate. Default is 0.001.</span>
</span><span id="CNNModel-118"><a href="#CNNModel-118"><span class="linenos">118</span></a><span class="sd">            beta1 (float, optional): Adam&#39;s beta1 parameter. Default is 0.9.</span>
</span><span id="CNNModel-119"><a href="#CNNModel-119"><span class="linenos">119</span></a><span class="sd">            beta2 (float, optional): Adam&#39;s beta2 parameter. Default is 0.999.</span>
</span><span id="CNNModel-120"><a href="#CNNModel-120"><span class="linenos">120</span></a><span class="sd">            epsilon (float, optional): Adam&#39;s epsilon parameter. Default is 1e-8.</span>
</span><span id="CNNModel-121"><a href="#CNNModel-121"><span class="linenos">121</span></a>
</span><span id="CNNModel-122"><a href="#CNNModel-122"><span class="linenos">122</span></a><span class="sd">        Returns:</span>
</span><span id="CNNModel-123"><a href="#CNNModel-123"><span class="linenos">123</span></a><span class="sd">            float: accuracy percentage.</span>
</span><span id="CNNModel-124"><a href="#CNNModel-124"><span class="linenos">124</span></a><span class="sd">            float: average loss.</span>
</span><span id="CNNModel-125"><a href="#CNNModel-125"><span class="linenos">125</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="CNNModel-126"><a href="#CNNModel-126"><span class="linenos">126</span></a>        <span class="c1"># InputData: (numImages, channels, height, width)   or (numImages, channels, width)  &lt;--- if using Conv1D layer</span>
</span><span id="CNNModel-127"><a href="#CNNModel-127"><span class="linenos">127</span></a>        <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputData</span><span class="p">)</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputData</span><span class="p">)</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Dimension wrong size, got </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputData</span><span class="p">)</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">, expected 4 or 3&quot;</span>
</span><span id="CNNModel-128"><a href="#CNNModel-128"><span class="linenos">128</span></a>        <span class="n">correct</span><span class="p">,</span> <span class="n">totalLoss</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
</span><span id="CNNModel-129"><a href="#CNNModel-129"><span class="linenos">129</span></a>        
</span><span id="CNNModel-130"><a href="#CNNModel-130"><span class="linenos">130</span></a>        <span class="n">nodes</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimser</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">useBatches</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">learningRate</span><span class="p">)</span>        
</span><span id="CNNModel-131"><a href="#CNNModel-131"><span class="linenos">131</span></a><span class="w">        </span>
</span><span id="CNNModel-132"><a href="#CNNModel-132"><span class="linenos">132</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="CNNModel-133"><a href="#CNNModel-133"><span class="linenos">133</span></a><span class="sd">        #nodes: (numbatches, numLayers, batchSize, outputSize)</span>
</span><span id="CNNModel-134"><a href="#CNNModel-134"><span class="linenos">134</span></a>
</span><span id="CNNModel-135"><a href="#CNNModel-135"><span class="linenos">135</span></a><span class="sd">        #lastLayer = len(nodes[0]) - 1</span>
</span><span id="CNNModel-136"><a href="#CNNModel-136"><span class="linenos">136</span></a><span class="sd">        #for i in range(len(nodes)): </span>
</span><span id="CNNModel-137"><a href="#CNNModel-137"><span class="linenos">137</span></a><span class="sd">        #    totalLoss += self.NeuralNetworkClass.lossFunction(nodes[i][lastLayer], labels[i])</span>
</span><span id="CNNModel-138"><a href="#CNNModel-138"><span class="linenos">138</span></a><span class="sd">        #    nodeIndex = np.argmax(nodes[i][lastLayer])</span>
</span><span id="CNNModel-139"><a href="#CNNModel-139"><span class="linenos">139</span></a><span class="sd">        #    labelIndex = np.argmax(labels[i])</span>
</span><span id="CNNModel-140"><a href="#CNNModel-140"><span class="linenos">140</span></a><span class="sd">        #    </span>
</span><span id="CNNModel-141"><a href="#CNNModel-141"><span class="linenos">141</span></a><span class="sd">        #    if(nodeIndex == labelIndex):</span>
</span><span id="CNNModel-142"><a href="#CNNModel-142"><span class="linenos">142</span></a><span class="sd">        #        correct += 1</span>
</span><span id="CNNModel-143"><a href="#CNNModel-143"><span class="linenos">143</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="CNNModel-144"><a href="#CNNModel-144"><span class="linenos">144</span></a>
</span><span id="CNNModel-145"><a href="#CNNModel-145"><span class="linenos">145</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">useBatches</span> <span class="o">==</span> <span class="kc">False</span><span class="p">):</span> 
</span><span id="CNNModel-146"><a href="#CNNModel-146"><span class="linenos">146</span></a>            <span class="n">batchSize</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="CNNModel-147"><a href="#CNNModel-147"><span class="linenos">147</span></a>
</span><span id="CNNModel-148"><a href="#CNNModel-148"><span class="linenos">148</span></a>        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">nodes</span><span class="p">)):</span> 
</span><span id="CNNModel-149"><a href="#CNNModel-149"><span class="linenos">149</span></a>            <span class="n">lastLayer</span> <span class="o">=</span> <span class="n">nodes</span><span class="p">[</span><span class="n">batch</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># shape: (batchSize, outputSize) &lt;-- last layer so is the FNN</span>
</span><span id="CNNModel-150"><a href="#CNNModel-150"><span class="linenos">150</span></a>            <span class="k">for</span> <span class="n">miniBatch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">lastLayer</span><span class="p">)):</span> <span class="c1">#shape: (outputSize)</span>
</span><span id="CNNModel-151"><a href="#CNNModel-151"><span class="linenos">151</span></a>                <span class="n">totalLoss</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">NeuralNetworkClass</span><span class="o">.</span><span class="n">lossFunction</span><span class="p">(</span><span class="n">lastLayer</span><span class="p">[</span><span class="n">miniBatch</span><span class="p">],</span> <span class="n">labels</span><span class="p">[</span><span class="n">batch</span> <span class="o">*</span> <span class="n">batchSize</span> <span class="o">+</span> <span class="n">miniBatch</span><span class="p">])</span>
</span><span id="CNNModel-152"><a href="#CNNModel-152"><span class="linenos">152</span></a>                <span class="n">nodeIndex</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">lastLayer</span><span class="p">[</span><span class="n">miniBatch</span><span class="p">])</span>
</span><span id="CNNModel-153"><a href="#CNNModel-153"><span class="linenos">153</span></a>                <span class="n">labelIndex</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">batch</span> <span class="o">*</span> <span class="n">batchSize</span> <span class="o">+</span> <span class="n">miniBatch</span><span class="p">])</span>
</span><span id="CNNModel-154"><a href="#CNNModel-154"><span class="linenos">154</span></a>                
</span><span id="CNNModel-155"><a href="#CNNModel-155"><span class="linenos">155</span></a>                <span class="k">if</span><span class="p">(</span><span class="n">nodeIndex</span> <span class="o">==</span> <span class="n">labelIndex</span><span class="p">):</span>
</span><span id="CNNModel-156"><a href="#CNNModel-156"><span class="linenos">156</span></a>                    <span class="n">correct</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span id="CNNModel-157"><a href="#CNNModel-157"><span class="linenos">157</span></a>
</span><span id="CNNModel-158"><a href="#CNNModel-158"><span class="linenos">158</span></a>
</span><span id="CNNModel-159"><a href="#CNNModel-159"><span class="linenos">159</span></a>        <span class="k">return</span> <span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="n">correct</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)),</span> <span class="n">totalLoss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
</span><span id="CNNModel-160"><a href="#CNNModel-160"><span class="linenos">160</span></a>    
</span><span id="CNNModel-161"><a href="#CNNModel-161"><span class="linenos">161</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">createWeightsBiases</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="CNNModel-162"><a href="#CNNModel-162"><span class="linenos">162</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="CNNModel-163"><a href="#CNNModel-163"><span class="linenos">163</span></a><span class="sd">        Initialises weights and biases for convolutional and dense layers.</span>
</span><span id="CNNModel-164"><a href="#CNNModel-164"><span class="linenos">164</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="CNNModel-165"><a href="#CNNModel-165"><span class="linenos">165</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)):</span>
</span><span id="CNNModel-166"><a href="#CNNModel-166"><span class="linenos">166</span></a>            <span class="k">if</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">==</span> <span class="n">Conv2DLayer</span><span class="p">):</span>
</span><span id="CNNModel-167"><a href="#CNNModel-167"><span class="linenos">167</span></a>                <span class="n">kernalSize</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">kernalSize</span>
</span><span id="CNNModel-168"><a href="#CNNModel-168"><span class="linenos">168</span></a>                <span class="n">numKernals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">numKernals</span>
</span><span id="CNNModel-169"><a href="#CNNModel-169"><span class="linenos">169</span></a>                <span class="n">depth</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">depth</span>
</span><span id="CNNModel-170"><a href="#CNNModel-170"><span class="linenos">170</span></a>
</span><span id="CNNModel-171"><a href="#CNNModel-171"><span class="linenos">171</span></a>                <span class="n">bounds</span> <span class="o">=</span>  <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">kernalSize</span><span class="p">)</span> <span class="c1"># He initialisation</span>
</span><span id="CNNModel-172"><a href="#CNNModel-172"><span class="linenos">172</span></a>
</span><span id="CNNModel-173"><a href="#CNNModel-173"><span class="linenos">173</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">bounds</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">numKernals</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">kernalSize</span><span class="p">,</span> <span class="n">kernalSize</span><span class="p">)))</span>
</span><span id="CNNModel-174"><a href="#CNNModel-174"><span class="linenos">174</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">numKernals</span><span class="p">)))</span>
</span><span id="CNNModel-175"><a href="#CNNModel-175"><span class="linenos">175</span></a>
</span><span id="CNNModel-176"><a href="#CNNModel-176"><span class="linenos">176</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">kernalWeights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span><span id="CNNModel-177"><a href="#CNNModel-177"><span class="linenos">177</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">kernalBiases</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span><span id="CNNModel-178"><a href="#CNNModel-178"><span class="linenos">178</span></a>            <span class="k">elif</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">==</span> <span class="n">Conv1DLayer</span><span class="p">):</span>
</span><span id="CNNModel-179"><a href="#CNNModel-179"><span class="linenos">179</span></a>                <span class="n">kernalSize</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">kernalSize</span>
</span><span id="CNNModel-180"><a href="#CNNModel-180"><span class="linenos">180</span></a>                <span class="n">numKernals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">numKernals</span>
</span><span id="CNNModel-181"><a href="#CNNModel-181"><span class="linenos">181</span></a>
</span><span id="CNNModel-182"><a href="#CNNModel-182"><span class="linenos">182</span></a>                <span class="n">bounds</span> <span class="o">=</span>  <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">kernalSize</span><span class="p">)</span> <span class="c1"># He initialisation</span>
</span><span id="CNNModel-183"><a href="#CNNModel-183"><span class="linenos">183</span></a>
</span><span id="CNNModel-184"><a href="#CNNModel-184"><span class="linenos">184</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">bounds</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">numKernals</span><span class="p">,</span> <span class="n">kernalSize</span><span class="p">,</span> <span class="n">kernalSize</span><span class="p">)))</span>
</span><span id="CNNModel-185"><a href="#CNNModel-185"><span class="linenos">185</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">numKernals</span><span class="p">)))</span>
</span><span id="CNNModel-186"><a href="#CNNModel-186"><span class="linenos">186</span></a>
</span><span id="CNNModel-187"><a href="#CNNModel-187"><span class="linenos">187</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">kernalWeights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span><span id="CNNModel-188"><a href="#CNNModel-188"><span class="linenos">188</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">kernalBiases</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span><span id="CNNModel-189"><a href="#CNNModel-189"><span class="linenos">189</span></a>
</span><span id="CNNModel-190"><a href="#CNNModel-190"><span class="linenos">190</span></a>            <span class="k">elif</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">==</span> <span class="n">DenseLayer</span><span class="p">):</span>
</span><span id="CNNModel-191"><a href="#CNNModel-191"><span class="linenos">191</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">NeuralNetworkClass</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span>
</span><span id="CNNModel-192"><a href="#CNNModel-192"><span class="linenos">192</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">NeuralNetworkClass</span><span class="o">.</span><span class="n">biases</span><span class="p">)</span>
</span><span id="CNNModel-193"><a href="#CNNModel-193"><span class="linenos">193</span></a>
</span><span id="CNNModel-194"><a href="#CNNModel-194"><span class="linenos">194</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">saveModel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">NNweights</span><span class="p">,</span> <span class="n">NNbiases</span><span class="p">,</span> <span class="n">CNNweights</span><span class="p">,</span> <span class="n">CNNbiases</span><span class="p">,</span> <span class="n">filename</span> <span class="o">=</span> <span class="s2">&quot;modelWeights.npz&quot;</span><span class="p">):</span>
</span><span id="CNNModel-195"><a href="#CNNModel-195"><span class="linenos">195</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="CNNModel-196"><a href="#CNNModel-196"><span class="linenos">196</span></a><span class="sd">        Saves model weights and biases to a compressed npz file.</span>
</span><span id="CNNModel-197"><a href="#CNNModel-197"><span class="linenos">197</span></a>
</span><span id="CNNModel-198"><a href="#CNNModel-198"><span class="linenos">198</span></a><span class="sd">        Args:</span>
</span><span id="CNNModel-199"><a href="#CNNModel-199"><span class="linenos">199</span></a><span class="sd">            NNweights (list): Weights of the dense neural network layers.</span>
</span><span id="CNNModel-200"><a href="#CNNModel-200"><span class="linenos">200</span></a><span class="sd">            NNbiases (list): Biases of the dense neural network layers.</span>
</span><span id="CNNModel-201"><a href="#CNNModel-201"><span class="linenos">201</span></a><span class="sd">            CNNweights (list): Weights of the convolutional layers.</span>
</span><span id="CNNModel-202"><a href="#CNNModel-202"><span class="linenos">202</span></a><span class="sd">            CNNbiases (list): Biases of the convolutional layers.</span>
</span><span id="CNNModel-203"><a href="#CNNModel-203"><span class="linenos">203</span></a><span class="sd">            filename (str, optional): Filename to save the weights. Default is &quot;modelWeights.npz&quot;.</span>
</span><span id="CNNModel-204"><a href="#CNNModel-204"><span class="linenos">204</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="CNNModel-205"><a href="#CNNModel-205"><span class="linenos">205</span></a>        <span class="n">CNNweights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">CNNweights</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
</span><span id="CNNModel-206"><a href="#CNNModel-206"><span class="linenos">206</span></a>        <span class="n">CNNbiases</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">CNNbiases</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
</span><span id="CNNModel-207"><a href="#CNNModel-207"><span class="linenos">207</span></a>        <span class="n">NNweights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">NNweights</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
</span><span id="CNNModel-208"><a href="#CNNModel-208"><span class="linenos">208</span></a>        <span class="n">NNbiases</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">NNbiases</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
</span><span id="CNNModel-209"><a href="#CNNModel-209"><span class="linenos">209</span></a>        <span class="n">np</span><span class="o">.</span><span class="n">savez_compressed</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">CNNweights</span> <span class="o">=</span> <span class="n">CNNweights</span><span class="p">,</span> <span class="n">CNNbiases</span> <span class="o">=</span> <span class="n">CNNbiases</span><span class="p">,</span> <span class="n">NNweights</span> <span class="o">=</span> <span class="n">NNweights</span><span class="p">,</span> <span class="n">NNbiases</span> <span class="o">=</span> <span class="n">NNbiases</span><span class="p">,</span> <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</span><span id="CNNModel-210"><a href="#CNNModel-210"><span class="linenos">210</span></a>
</span><span id="CNNModel-211"><a href="#CNNModel-211"><span class="linenos">211</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">loadModel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">neuralNetwork</span><span class="p">,</span> <span class="n">filename</span> <span class="o">=</span> <span class="s2">&quot;modelWeights.npz&quot;</span><span class="p">):</span>
</span><span id="CNNModel-212"><a href="#CNNModel-212"><span class="linenos">212</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="CNNModel-213"><a href="#CNNModel-213"><span class="linenos">213</span></a><span class="sd">        Loads model weights and biases from a compressed npz file and assigns them to layers.</span>
</span><span id="CNNModel-214"><a href="#CNNModel-214"><span class="linenos">214</span></a><span class="sd">        </span>
</span><span id="CNNModel-215"><a href="#CNNModel-215"><span class="linenos">215</span></a><span class="sd">        Args:</span>
</span><span id="CNNModel-216"><a href="#CNNModel-216"><span class="linenos">216</span></a><span class="sd">            neuralNetwork (class): The dense neural network to load weights into.</span>
</span><span id="CNNModel-217"><a href="#CNNModel-217"><span class="linenos">217</span></a><span class="sd">            filename (str, optional): Filename to save the weights. Default is &quot;modelWeights.npz&quot;.</span>
</span><span id="CNNModel-218"><a href="#CNNModel-218"><span class="linenos">218</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="CNNModel-219"><a href="#CNNModel-219"><span class="linenos">219</span></a>        <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</span><span id="CNNModel-220"><a href="#CNNModel-220"><span class="linenos">220</span></a>        <span class="n">CNNweights</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;CNNweights&quot;</span><span class="p">]</span>
</span><span id="CNNModel-221"><a href="#CNNModel-221"><span class="linenos">221</span></a>        <span class="n">CNNbiases</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;CNNbiases&quot;</span><span class="p">]</span>
</span><span id="CNNModel-222"><a href="#CNNModel-222"><span class="linenos">222</span></a>        <span class="n">NNweights</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;NNweights&quot;</span><span class="p">]</span>
</span><span id="CNNModel-223"><a href="#CNNModel-223"><span class="linenos">223</span></a>        <span class="n">NNbiases</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;NNbiases&quot;</span><span class="p">]</span>
</span><span id="CNNModel-224"><a href="#CNNModel-224"><span class="linenos">224</span></a>
</span><span id="CNNModel-225"><a href="#CNNModel-225"><span class="linenos">225</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">NeuralNetworkClass</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">NNweights</span>
</span><span id="CNNModel-226"><a href="#CNNModel-226"><span class="linenos">226</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">NeuralNetworkClass</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="n">NNbiases</span>
</span><span id="CNNModel-227"><a href="#CNNModel-227"><span class="linenos">227</span></a>        <span class="n">neuralNetwork</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">NNweights</span>
</span><span id="CNNModel-228"><a href="#CNNModel-228"><span class="linenos">228</span></a>        <span class="n">neuralNetwork</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="n">NNbiases</span>
</span><span id="CNNModel-229"><a href="#CNNModel-229"><span class="linenos">229</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">CNNweights</span>
</span><span id="CNNModel-230"><a href="#CNNModel-230"><span class="linenos">230</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="n">CNNbiases</span>
</span><span id="CNNModel-231"><a href="#CNNModel-231"><span class="linenos">231</span></a>
</span><span id="CNNModel-232"><a href="#CNNModel-232"><span class="linenos">232</span></a>        <span class="n">currWeightIndex</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="CNNModel-233"><a href="#CNNModel-233"><span class="linenos">233</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)):</span>
</span><span id="CNNModel-234"><a href="#CNNModel-234"><span class="linenos">234</span></a>            <span class="k">if</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">==</span> <span class="n">Conv2DLayer</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">==</span> <span class="n">Conv1DLayer</span><span class="p">):</span>
</span><span id="CNNModel-235"><a href="#CNNModel-235"><span class="linenos">235</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">kernalWeights</span> <span class="o">=</span> <span class="n">CNNweights</span><span class="p">[</span><span class="n">currWeightIndex</span><span class="p">]</span>
</span><span id="CNNModel-236"><a href="#CNNModel-236"><span class="linenos">236</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">kernalBiases</span> <span class="o">=</span> <span class="n">CNNbiases</span><span class="p">[</span><span class="n">currWeightIndex</span><span class="p">]</span>
</span><span id="CNNModel-237"><a href="#CNNModel-237"><span class="linenos">237</span></a>                <span class="n">currWeightIndex</span> <span class="o">+=</span> <span class="mi">1</span>
</span></pre></div>


    

                            <div id="CNNModel.__init__" class="classattr">
                                        <input id="CNNModel.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">CNNModel</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">NeuralNetworkClass</span>,</span><span class="param">	optimisationFunction=&lt;class &#x27;<a href="#Adam">Adam</a>&#x27;&gt;</span>)</span>

                <label class="view-source-button" for="CNNModel.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#CNNModel.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="CNNModel.__init__-13"><a href="#CNNModel.__init__-13"><span class="linenos">13</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">NeuralNetworkClass</span><span class="p">,</span> <span class="n">optimisationFunction</span><span class="o">=</span><span class="n">Adam</span><span class="p">):</span>
</span><span id="CNNModel.__init__-14"><a href="#CNNModel.__init__-14"><span class="linenos">14</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="CNNModel.__init__-15"><a href="#CNNModel.__init__-15"><span class="linenos">15</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="CNNModel.__init__-16"><a href="#CNNModel.__init__-16"><span class="linenos">16</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="CNNModel.__init__-17"><a href="#CNNModel.__init__-17"><span class="linenos">17</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">NeuralNetworkClass</span> <span class="o">=</span> <span class="n">NeuralNetworkClass</span>
</span><span id="CNNModel.__init__-18"><a href="#CNNModel.__init__-18"><span class="linenos">18</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">optimisationFunction</span> <span class="o">=</span> <span class="n">optimisationFunction</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backpropagation</span><span class="p">)</span>
</span></pre></div>


    

                            </div>
                            <div id="CNNModel.layers" class="classattr">
                                <div class="attr variable">
            <span class="name">layers</span>

        
    </div>
    <a class="headerlink" href="#CNNModel.layers"></a>
    
    

                            </div>
                            <div id="CNNModel.weights" class="classattr">
                                <div class="attr variable">
            <span class="name">weights</span>

        
    </div>
    <a class="headerlink" href="#CNNModel.weights"></a>
    
    

                            </div>
                            <div id="CNNModel.biases" class="classattr">
                                <div class="attr variable">
            <span class="name">biases</span>

        
    </div>
    <a class="headerlink" href="#CNNModel.biases"></a>
    
    

                            </div>
                            <div id="CNNModel.NeuralNetworkClass" class="classattr">
                                <div class="attr variable">
            <span class="name">NeuralNetworkClass</span>

        
    </div>
    <a class="headerlink" href="#CNNModel.NeuralNetworkClass"></a>
    
    

                            </div>
                            <div id="CNNModel.optimisationFunction" class="classattr">
                                <div class="attr variable">
            <span class="name">optimisationFunction</span>

        
    </div>
    <a class="headerlink" href="#CNNModel.optimisationFunction"></a>
    
    

                            </div>
                            <div id="CNNModel.addLayer" class="classattr">
                                        <input id="CNNModel.addLayer-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">addLayer</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">layer</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="CNNModel.addLayer-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#CNNModel.addLayer"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="CNNModel.addLayer-20"><a href="#CNNModel.addLayer-20"><span class="linenos">20</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">addLayer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer</span><span class="p">):</span>
</span><span id="CNNModel.addLayer-21"><a href="#CNNModel.addLayer-21"><span class="linenos">21</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="CNNModel.addLayer-22"><a href="#CNNModel.addLayer-22"><span class="linenos">22</span></a><span class="sd">        Adds a layer to the CNN model.</span>
</span><span id="CNNModel.addLayer-23"><a href="#CNNModel.addLayer-23"><span class="linenos">23</span></a>
</span><span id="CNNModel.addLayer-24"><a href="#CNNModel.addLayer-24"><span class="linenos">24</span></a><span class="sd">        Args:</span>
</span><span id="CNNModel.addLayer-25"><a href="#CNNModel.addLayer-25"><span class="linenos">25</span></a><span class="sd">            layer (class): ConvLayer, PoolingLayer, GlobalAveragePoolingLayer, ActivationLayer, and DenseLayer</span>
</span><span id="CNNModel.addLayer-26"><a href="#CNNModel.addLayer-26"><span class="linenos">26</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="CNNModel.addLayer-27"><a href="#CNNModel.addLayer-27"><span class="linenos">27</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Adds a layer to the CNN model.</p>

<p>Args:
    layer (class): ConvLayer, PoolingLayer, GlobalAveragePoolingLayer, ActivationLayer, and DenseLayer</p>
</div>


                            </div>
                            <div id="CNNModel.forward" class="classattr">
                                        <input id="CNNModel.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">inputTensor</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="CNNModel.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#CNNModel.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="CNNModel.forward-29"><a href="#CNNModel.forward-29"><span class="linenos">29</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputTensor</span><span class="p">):</span>
</span><span id="CNNModel.forward-30"><a href="#CNNModel.forward-30"><span class="linenos">30</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="CNNModel.forward-31"><a href="#CNNModel.forward-31"><span class="linenos">31</span></a><span class="sd">        Performs a forward pass through all layers.</span>
</span><span id="CNNModel.forward-32"><a href="#CNNModel.forward-32"><span class="linenos">32</span></a>
</span><span id="CNNModel.forward-33"><a href="#CNNModel.forward-33"><span class="linenos">33</span></a><span class="sd">        Args:</span>
</span><span id="CNNModel.forward-34"><a href="#CNNModel.forward-34"><span class="linenos">34</span></a><span class="sd">            inputTensor (ndarray): Input data tensor to the CNN.</span>
</span><span id="CNNModel.forward-35"><a href="#CNNModel.forward-35"><span class="linenos">35</span></a><span class="sd">        </span>
</span><span id="CNNModel.forward-36"><a href="#CNNModel.forward-36"><span class="linenos">36</span></a><span class="sd">        Returns:</span>
</span><span id="CNNModel.forward-37"><a href="#CNNModel.forward-37"><span class="linenos">37</span></a><span class="sd">            list: List of tensors output by each layer including the input.</span>
</span><span id="CNNModel.forward-38"><a href="#CNNModel.forward-38"><span class="linenos">38</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="CNNModel.forward-39"><a href="#CNNModel.forward-39"><span class="linenos">39</span></a>        <span class="n">allTensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">inputTensor</span><span class="p">]</span>
</span><span id="CNNModel.forward-40"><a href="#CNNModel.forward-40"><span class="linenos">40</span></a>        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
</span><span id="CNNModel.forward-41"><a href="#CNNModel.forward-41"><span class="linenos">41</span></a>            <span class="n">inputTensor</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">inputTensor</span><span class="p">)</span>
</span><span id="CNNModel.forward-42"><a href="#CNNModel.forward-42"><span class="linenos">42</span></a>            <span class="n">allTensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inputTensor</span><span class="p">)</span>
</span><span id="CNNModel.forward-43"><a href="#CNNModel.forward-43"><span class="linenos">43</span></a>        <span class="k">return</span> <span class="n">allTensors</span>
</span></pre></div>


            <div class="docstring"><p>Performs a forward pass through all layers.</p>

<p>Args:
    inputTensor (ndarray): Input data tensor to the CNN.</p>

<p>Returns:
    list: List of tensors output by each layer including the input.</p>
</div>


                            </div>
                            <div id="CNNModel.train" class="classattr">
                                        <input id="CNNModel.train-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">train</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">inputData</span>, </span><span class="param"><span class="n">labels</span>, </span><span class="param"><span class="n">useBatches</span>, </span><span class="param"><span class="n">batchSize</span>, </span><span class="param"><span class="n">learningRate</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="CNNModel.train-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#CNNModel.train"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="CNNModel.train-108"><a href="#CNNModel.train-108"><span class="linenos">108</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">useBatches</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">learningRate</span><span class="p">):</span>
</span><span id="CNNModel.train-109"><a href="#CNNModel.train-109"><span class="linenos">109</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="CNNModel.train-110"><a href="#CNNModel.train-110"><span class="linenos">110</span></a><span class="sd">        Trains the CNN for one epoch and calculates accuracy and average loss.</span>
</span><span id="CNNModel.train-111"><a href="#CNNModel.train-111"><span class="linenos">111</span></a>
</span><span id="CNNModel.train-112"><a href="#CNNModel.train-112"><span class="linenos">112</span></a><span class="sd">        Args:</span>
</span><span id="CNNModel.train-113"><a href="#CNNModel.train-113"><span class="linenos">113</span></a><span class="sd">            inputData (ndarray): All the training data.</span>
</span><span id="CNNModel.train-114"><a href="#CNNModel.train-114"><span class="linenos">114</span></a><span class="sd">            labels (ndarray): All the true labels for the training data.</span>
</span><span id="CNNModel.train-115"><a href="#CNNModel.train-115"><span class="linenos">115</span></a><span class="sd">            useBatches (bool): Whether to use batching.</span>
</span><span id="CNNModel.train-116"><a href="#CNNModel.train-116"><span class="linenos">116</span></a><span class="sd">            batchSize (int): Size of batches.</span>
</span><span id="CNNModel.train-117"><a href="#CNNModel.train-117"><span class="linenos">117</span></a><span class="sd">            alpha (float, optional): Learning rate. Default is 0.001.</span>
</span><span id="CNNModel.train-118"><a href="#CNNModel.train-118"><span class="linenos">118</span></a><span class="sd">            beta1 (float, optional): Adam&#39;s beta1 parameter. Default is 0.9.</span>
</span><span id="CNNModel.train-119"><a href="#CNNModel.train-119"><span class="linenos">119</span></a><span class="sd">            beta2 (float, optional): Adam&#39;s beta2 parameter. Default is 0.999.</span>
</span><span id="CNNModel.train-120"><a href="#CNNModel.train-120"><span class="linenos">120</span></a><span class="sd">            epsilon (float, optional): Adam&#39;s epsilon parameter. Default is 1e-8.</span>
</span><span id="CNNModel.train-121"><a href="#CNNModel.train-121"><span class="linenos">121</span></a>
</span><span id="CNNModel.train-122"><a href="#CNNModel.train-122"><span class="linenos">122</span></a><span class="sd">        Returns:</span>
</span><span id="CNNModel.train-123"><a href="#CNNModel.train-123"><span class="linenos">123</span></a><span class="sd">            float: accuracy percentage.</span>
</span><span id="CNNModel.train-124"><a href="#CNNModel.train-124"><span class="linenos">124</span></a><span class="sd">            float: average loss.</span>
</span><span id="CNNModel.train-125"><a href="#CNNModel.train-125"><span class="linenos">125</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="CNNModel.train-126"><a href="#CNNModel.train-126"><span class="linenos">126</span></a>        <span class="c1"># InputData: (numImages, channels, height, width)   or (numImages, channels, width)  &lt;--- if using Conv1D layer</span>
</span><span id="CNNModel.train-127"><a href="#CNNModel.train-127"><span class="linenos">127</span></a>        <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputData</span><span class="p">)</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputData</span><span class="p">)</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Dimension wrong size, got </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputData</span><span class="p">)</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">, expected 4 or 3&quot;</span>
</span><span id="CNNModel.train-128"><a href="#CNNModel.train-128"><span class="linenos">128</span></a>        <span class="n">correct</span><span class="p">,</span> <span class="n">totalLoss</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
</span><span id="CNNModel.train-129"><a href="#CNNModel.train-129"><span class="linenos">129</span></a>        
</span><span id="CNNModel.train-130"><a href="#CNNModel.train-130"><span class="linenos">130</span></a>        <span class="n">nodes</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimser</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">useBatches</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">learningRate</span><span class="p">)</span>        
</span><span id="CNNModel.train-131"><a href="#CNNModel.train-131"><span class="linenos">131</span></a><span class="w">        </span>
</span><span id="CNNModel.train-132"><a href="#CNNModel.train-132"><span class="linenos">132</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="CNNModel.train-133"><a href="#CNNModel.train-133"><span class="linenos">133</span></a><span class="sd">        #nodes: (numbatches, numLayers, batchSize, outputSize)</span>
</span><span id="CNNModel.train-134"><a href="#CNNModel.train-134"><span class="linenos">134</span></a>
</span><span id="CNNModel.train-135"><a href="#CNNModel.train-135"><span class="linenos">135</span></a><span class="sd">        #lastLayer = len(nodes[0]) - 1</span>
</span><span id="CNNModel.train-136"><a href="#CNNModel.train-136"><span class="linenos">136</span></a><span class="sd">        #for i in range(len(nodes)): </span>
</span><span id="CNNModel.train-137"><a href="#CNNModel.train-137"><span class="linenos">137</span></a><span class="sd">        #    totalLoss += self.NeuralNetworkClass.lossFunction(nodes[i][lastLayer], labels[i])</span>
</span><span id="CNNModel.train-138"><a href="#CNNModel.train-138"><span class="linenos">138</span></a><span class="sd">        #    nodeIndex = np.argmax(nodes[i][lastLayer])</span>
</span><span id="CNNModel.train-139"><a href="#CNNModel.train-139"><span class="linenos">139</span></a><span class="sd">        #    labelIndex = np.argmax(labels[i])</span>
</span><span id="CNNModel.train-140"><a href="#CNNModel.train-140"><span class="linenos">140</span></a><span class="sd">        #    </span>
</span><span id="CNNModel.train-141"><a href="#CNNModel.train-141"><span class="linenos">141</span></a><span class="sd">        #    if(nodeIndex == labelIndex):</span>
</span><span id="CNNModel.train-142"><a href="#CNNModel.train-142"><span class="linenos">142</span></a><span class="sd">        #        correct += 1</span>
</span><span id="CNNModel.train-143"><a href="#CNNModel.train-143"><span class="linenos">143</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="CNNModel.train-144"><a href="#CNNModel.train-144"><span class="linenos">144</span></a>
</span><span id="CNNModel.train-145"><a href="#CNNModel.train-145"><span class="linenos">145</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">useBatches</span> <span class="o">==</span> <span class="kc">False</span><span class="p">):</span> 
</span><span id="CNNModel.train-146"><a href="#CNNModel.train-146"><span class="linenos">146</span></a>            <span class="n">batchSize</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="CNNModel.train-147"><a href="#CNNModel.train-147"><span class="linenos">147</span></a>
</span><span id="CNNModel.train-148"><a href="#CNNModel.train-148"><span class="linenos">148</span></a>        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">nodes</span><span class="p">)):</span> 
</span><span id="CNNModel.train-149"><a href="#CNNModel.train-149"><span class="linenos">149</span></a>            <span class="n">lastLayer</span> <span class="o">=</span> <span class="n">nodes</span><span class="p">[</span><span class="n">batch</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># shape: (batchSize, outputSize) &lt;-- last layer so is the FNN</span>
</span><span id="CNNModel.train-150"><a href="#CNNModel.train-150"><span class="linenos">150</span></a>            <span class="k">for</span> <span class="n">miniBatch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">lastLayer</span><span class="p">)):</span> <span class="c1">#shape: (outputSize)</span>
</span><span id="CNNModel.train-151"><a href="#CNNModel.train-151"><span class="linenos">151</span></a>                <span class="n">totalLoss</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">NeuralNetworkClass</span><span class="o">.</span><span class="n">lossFunction</span><span class="p">(</span><span class="n">lastLayer</span><span class="p">[</span><span class="n">miniBatch</span><span class="p">],</span> <span class="n">labels</span><span class="p">[</span><span class="n">batch</span> <span class="o">*</span> <span class="n">batchSize</span> <span class="o">+</span> <span class="n">miniBatch</span><span class="p">])</span>
</span><span id="CNNModel.train-152"><a href="#CNNModel.train-152"><span class="linenos">152</span></a>                <span class="n">nodeIndex</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">lastLayer</span><span class="p">[</span><span class="n">miniBatch</span><span class="p">])</span>
</span><span id="CNNModel.train-153"><a href="#CNNModel.train-153"><span class="linenos">153</span></a>                <span class="n">labelIndex</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">batch</span> <span class="o">*</span> <span class="n">batchSize</span> <span class="o">+</span> <span class="n">miniBatch</span><span class="p">])</span>
</span><span id="CNNModel.train-154"><a href="#CNNModel.train-154"><span class="linenos">154</span></a>                
</span><span id="CNNModel.train-155"><a href="#CNNModel.train-155"><span class="linenos">155</span></a>                <span class="k">if</span><span class="p">(</span><span class="n">nodeIndex</span> <span class="o">==</span> <span class="n">labelIndex</span><span class="p">):</span>
</span><span id="CNNModel.train-156"><a href="#CNNModel.train-156"><span class="linenos">156</span></a>                    <span class="n">correct</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span id="CNNModel.train-157"><a href="#CNNModel.train-157"><span class="linenos">157</span></a>
</span><span id="CNNModel.train-158"><a href="#CNNModel.train-158"><span class="linenos">158</span></a>
</span><span id="CNNModel.train-159"><a href="#CNNModel.train-159"><span class="linenos">159</span></a>        <span class="k">return</span> <span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="n">correct</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)),</span> <span class="n">totalLoss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Trains the CNN for one epoch and calculates accuracy and average loss.</p>

<p>Args:
    inputData (ndarray): All the training data.
    labels (ndarray): All the true labels for the training data.
    useBatches (bool): Whether to use batching.
    batchSize (int): Size of batches.
    alpha (float, optional): Learning rate. Default is 0.001.
    beta1 (float, optional): Adam's beta1 parameter. Default is 0.9.
    beta2 (float, optional): Adam's beta2 parameter. Default is 0.999.
    epsilon (float, optional): Adam's epsilon parameter. Default is 1e-8.</p>

<p>Returns:
    float: accuracy percentage.
    float: average loss.</p>
</div>


                            </div>
                            <div id="CNNModel.createWeightsBiases" class="classattr">
                                        <input id="CNNModel.createWeightsBiases-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">createWeightsBiases</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="CNNModel.createWeightsBiases-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#CNNModel.createWeightsBiases"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="CNNModel.createWeightsBiases-161"><a href="#CNNModel.createWeightsBiases-161"><span class="linenos">161</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">createWeightsBiases</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="CNNModel.createWeightsBiases-162"><a href="#CNNModel.createWeightsBiases-162"><span class="linenos">162</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="CNNModel.createWeightsBiases-163"><a href="#CNNModel.createWeightsBiases-163"><span class="linenos">163</span></a><span class="sd">        Initialises weights and biases for convolutional and dense layers.</span>
</span><span id="CNNModel.createWeightsBiases-164"><a href="#CNNModel.createWeightsBiases-164"><span class="linenos">164</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="CNNModel.createWeightsBiases-165"><a href="#CNNModel.createWeightsBiases-165"><span class="linenos">165</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)):</span>
</span><span id="CNNModel.createWeightsBiases-166"><a href="#CNNModel.createWeightsBiases-166"><span class="linenos">166</span></a>            <span class="k">if</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">==</span> <span class="n">Conv2DLayer</span><span class="p">):</span>
</span><span id="CNNModel.createWeightsBiases-167"><a href="#CNNModel.createWeightsBiases-167"><span class="linenos">167</span></a>                <span class="n">kernalSize</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">kernalSize</span>
</span><span id="CNNModel.createWeightsBiases-168"><a href="#CNNModel.createWeightsBiases-168"><span class="linenos">168</span></a>                <span class="n">numKernals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">numKernals</span>
</span><span id="CNNModel.createWeightsBiases-169"><a href="#CNNModel.createWeightsBiases-169"><span class="linenos">169</span></a>                <span class="n">depth</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">depth</span>
</span><span id="CNNModel.createWeightsBiases-170"><a href="#CNNModel.createWeightsBiases-170"><span class="linenos">170</span></a>
</span><span id="CNNModel.createWeightsBiases-171"><a href="#CNNModel.createWeightsBiases-171"><span class="linenos">171</span></a>                <span class="n">bounds</span> <span class="o">=</span>  <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">kernalSize</span><span class="p">)</span> <span class="c1"># He initialisation</span>
</span><span id="CNNModel.createWeightsBiases-172"><a href="#CNNModel.createWeightsBiases-172"><span class="linenos">172</span></a>
</span><span id="CNNModel.createWeightsBiases-173"><a href="#CNNModel.createWeightsBiases-173"><span class="linenos">173</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">bounds</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">numKernals</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">kernalSize</span><span class="p">,</span> <span class="n">kernalSize</span><span class="p">)))</span>
</span><span id="CNNModel.createWeightsBiases-174"><a href="#CNNModel.createWeightsBiases-174"><span class="linenos">174</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">numKernals</span><span class="p">)))</span>
</span><span id="CNNModel.createWeightsBiases-175"><a href="#CNNModel.createWeightsBiases-175"><span class="linenos">175</span></a>
</span><span id="CNNModel.createWeightsBiases-176"><a href="#CNNModel.createWeightsBiases-176"><span class="linenos">176</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">kernalWeights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span><span id="CNNModel.createWeightsBiases-177"><a href="#CNNModel.createWeightsBiases-177"><span class="linenos">177</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">kernalBiases</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span><span id="CNNModel.createWeightsBiases-178"><a href="#CNNModel.createWeightsBiases-178"><span class="linenos">178</span></a>            <span class="k">elif</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">==</span> <span class="n">Conv1DLayer</span><span class="p">):</span>
</span><span id="CNNModel.createWeightsBiases-179"><a href="#CNNModel.createWeightsBiases-179"><span class="linenos">179</span></a>                <span class="n">kernalSize</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">kernalSize</span>
</span><span id="CNNModel.createWeightsBiases-180"><a href="#CNNModel.createWeightsBiases-180"><span class="linenos">180</span></a>                <span class="n">numKernals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">numKernals</span>
</span><span id="CNNModel.createWeightsBiases-181"><a href="#CNNModel.createWeightsBiases-181"><span class="linenos">181</span></a>
</span><span id="CNNModel.createWeightsBiases-182"><a href="#CNNModel.createWeightsBiases-182"><span class="linenos">182</span></a>                <span class="n">bounds</span> <span class="o">=</span>  <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">kernalSize</span><span class="p">)</span> <span class="c1"># He initialisation</span>
</span><span id="CNNModel.createWeightsBiases-183"><a href="#CNNModel.createWeightsBiases-183"><span class="linenos">183</span></a>
</span><span id="CNNModel.createWeightsBiases-184"><a href="#CNNModel.createWeightsBiases-184"><span class="linenos">184</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">bounds</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">numKernals</span><span class="p">,</span> <span class="n">kernalSize</span><span class="p">,</span> <span class="n">kernalSize</span><span class="p">)))</span>
</span><span id="CNNModel.createWeightsBiases-185"><a href="#CNNModel.createWeightsBiases-185"><span class="linenos">185</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">numKernals</span><span class="p">)))</span>
</span><span id="CNNModel.createWeightsBiases-186"><a href="#CNNModel.createWeightsBiases-186"><span class="linenos">186</span></a>
</span><span id="CNNModel.createWeightsBiases-187"><a href="#CNNModel.createWeightsBiases-187"><span class="linenos">187</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">kernalWeights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span><span id="CNNModel.createWeightsBiases-188"><a href="#CNNModel.createWeightsBiases-188"><span class="linenos">188</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">kernalBiases</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span><span id="CNNModel.createWeightsBiases-189"><a href="#CNNModel.createWeightsBiases-189"><span class="linenos">189</span></a>
</span><span id="CNNModel.createWeightsBiases-190"><a href="#CNNModel.createWeightsBiases-190"><span class="linenos">190</span></a>            <span class="k">elif</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">==</span> <span class="n">DenseLayer</span><span class="p">):</span>
</span><span id="CNNModel.createWeightsBiases-191"><a href="#CNNModel.createWeightsBiases-191"><span class="linenos">191</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">NeuralNetworkClass</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span>
</span><span id="CNNModel.createWeightsBiases-192"><a href="#CNNModel.createWeightsBiases-192"><span class="linenos">192</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">NeuralNetworkClass</span><span class="o">.</span><span class="n">biases</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Initialises weights and biases for convolutional and dense layers.</p>
</div>


                            </div>
                            <div id="CNNModel.saveModel" class="classattr">
                                        <input id="CNNModel.saveModel-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">saveModel</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">NNweights</span>,</span><span class="param">	<span class="n">NNbiases</span>,</span><span class="param">	<span class="n">CNNweights</span>,</span><span class="param">	<span class="n">CNNbiases</span>,</span><span class="param">	<span class="n">filename</span><span class="o">=</span><span class="s1">&#39;modelWeights.npz&#39;</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="CNNModel.saveModel-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#CNNModel.saveModel"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="CNNModel.saveModel-194"><a href="#CNNModel.saveModel-194"><span class="linenos">194</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">saveModel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">NNweights</span><span class="p">,</span> <span class="n">NNbiases</span><span class="p">,</span> <span class="n">CNNweights</span><span class="p">,</span> <span class="n">CNNbiases</span><span class="p">,</span> <span class="n">filename</span> <span class="o">=</span> <span class="s2">&quot;modelWeights.npz&quot;</span><span class="p">):</span>
</span><span id="CNNModel.saveModel-195"><a href="#CNNModel.saveModel-195"><span class="linenos">195</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="CNNModel.saveModel-196"><a href="#CNNModel.saveModel-196"><span class="linenos">196</span></a><span class="sd">        Saves model weights and biases to a compressed npz file.</span>
</span><span id="CNNModel.saveModel-197"><a href="#CNNModel.saveModel-197"><span class="linenos">197</span></a>
</span><span id="CNNModel.saveModel-198"><a href="#CNNModel.saveModel-198"><span class="linenos">198</span></a><span class="sd">        Args:</span>
</span><span id="CNNModel.saveModel-199"><a href="#CNNModel.saveModel-199"><span class="linenos">199</span></a><span class="sd">            NNweights (list): Weights of the dense neural network layers.</span>
</span><span id="CNNModel.saveModel-200"><a href="#CNNModel.saveModel-200"><span class="linenos">200</span></a><span class="sd">            NNbiases (list): Biases of the dense neural network layers.</span>
</span><span id="CNNModel.saveModel-201"><a href="#CNNModel.saveModel-201"><span class="linenos">201</span></a><span class="sd">            CNNweights (list): Weights of the convolutional layers.</span>
</span><span id="CNNModel.saveModel-202"><a href="#CNNModel.saveModel-202"><span class="linenos">202</span></a><span class="sd">            CNNbiases (list): Biases of the convolutional layers.</span>
</span><span id="CNNModel.saveModel-203"><a href="#CNNModel.saveModel-203"><span class="linenos">203</span></a><span class="sd">            filename (str, optional): Filename to save the weights. Default is &quot;modelWeights.npz&quot;.</span>
</span><span id="CNNModel.saveModel-204"><a href="#CNNModel.saveModel-204"><span class="linenos">204</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="CNNModel.saveModel-205"><a href="#CNNModel.saveModel-205"><span class="linenos">205</span></a>        <span class="n">CNNweights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">CNNweights</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
</span><span id="CNNModel.saveModel-206"><a href="#CNNModel.saveModel-206"><span class="linenos">206</span></a>        <span class="n">CNNbiases</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">CNNbiases</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
</span><span id="CNNModel.saveModel-207"><a href="#CNNModel.saveModel-207"><span class="linenos">207</span></a>        <span class="n">NNweights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">NNweights</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
</span><span id="CNNModel.saveModel-208"><a href="#CNNModel.saveModel-208"><span class="linenos">208</span></a>        <span class="n">NNbiases</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">NNbiases</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
</span><span id="CNNModel.saveModel-209"><a href="#CNNModel.saveModel-209"><span class="linenos">209</span></a>        <span class="n">np</span><span class="o">.</span><span class="n">savez_compressed</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">CNNweights</span> <span class="o">=</span> <span class="n">CNNweights</span><span class="p">,</span> <span class="n">CNNbiases</span> <span class="o">=</span> <span class="n">CNNbiases</span><span class="p">,</span> <span class="n">NNweights</span> <span class="o">=</span> <span class="n">NNweights</span><span class="p">,</span> <span class="n">NNbiases</span> <span class="o">=</span> <span class="n">NNbiases</span><span class="p">,</span> <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Saves model weights and biases to a compressed npz file.</p>

<p>Args:
    NNweights (list): Weights of the dense neural network layers.
    NNbiases (list): Biases of the dense neural network layers.
    CNNweights (list): Weights of the convolutional layers.
    CNNbiases (list): Biases of the convolutional layers.
    filename (str, optional): Filename to save the weights. Default is "modelWeights.npz".</p>
</div>


                            </div>
                            <div id="CNNModel.loadModel" class="classattr">
                                        <input id="CNNModel.loadModel-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">loadModel</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">neuralNetwork</span>, </span><span class="param"><span class="n">filename</span><span class="o">=</span><span class="s1">&#39;modelWeights.npz&#39;</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="CNNModel.loadModel-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#CNNModel.loadModel"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="CNNModel.loadModel-211"><a href="#CNNModel.loadModel-211"><span class="linenos">211</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">loadModel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">neuralNetwork</span><span class="p">,</span> <span class="n">filename</span> <span class="o">=</span> <span class="s2">&quot;modelWeights.npz&quot;</span><span class="p">):</span>
</span><span id="CNNModel.loadModel-212"><a href="#CNNModel.loadModel-212"><span class="linenos">212</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="CNNModel.loadModel-213"><a href="#CNNModel.loadModel-213"><span class="linenos">213</span></a><span class="sd">        Loads model weights and biases from a compressed npz file and assigns them to layers.</span>
</span><span id="CNNModel.loadModel-214"><a href="#CNNModel.loadModel-214"><span class="linenos">214</span></a><span class="sd">        </span>
</span><span id="CNNModel.loadModel-215"><a href="#CNNModel.loadModel-215"><span class="linenos">215</span></a><span class="sd">        Args:</span>
</span><span id="CNNModel.loadModel-216"><a href="#CNNModel.loadModel-216"><span class="linenos">216</span></a><span class="sd">            neuralNetwork (class): The dense neural network to load weights into.</span>
</span><span id="CNNModel.loadModel-217"><a href="#CNNModel.loadModel-217"><span class="linenos">217</span></a><span class="sd">            filename (str, optional): Filename to save the weights. Default is &quot;modelWeights.npz&quot;.</span>
</span><span id="CNNModel.loadModel-218"><a href="#CNNModel.loadModel-218"><span class="linenos">218</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="CNNModel.loadModel-219"><a href="#CNNModel.loadModel-219"><span class="linenos">219</span></a>        <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">allow_pickle</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</span><span id="CNNModel.loadModel-220"><a href="#CNNModel.loadModel-220"><span class="linenos">220</span></a>        <span class="n">CNNweights</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;CNNweights&quot;</span><span class="p">]</span>
</span><span id="CNNModel.loadModel-221"><a href="#CNNModel.loadModel-221"><span class="linenos">221</span></a>        <span class="n">CNNbiases</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;CNNbiases&quot;</span><span class="p">]</span>
</span><span id="CNNModel.loadModel-222"><a href="#CNNModel.loadModel-222"><span class="linenos">222</span></a>        <span class="n">NNweights</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;NNweights&quot;</span><span class="p">]</span>
</span><span id="CNNModel.loadModel-223"><a href="#CNNModel.loadModel-223"><span class="linenos">223</span></a>        <span class="n">NNbiases</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;NNbiases&quot;</span><span class="p">]</span>
</span><span id="CNNModel.loadModel-224"><a href="#CNNModel.loadModel-224"><span class="linenos">224</span></a>
</span><span id="CNNModel.loadModel-225"><a href="#CNNModel.loadModel-225"><span class="linenos">225</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">NeuralNetworkClass</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">NNweights</span>
</span><span id="CNNModel.loadModel-226"><a href="#CNNModel.loadModel-226"><span class="linenos">226</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">NeuralNetworkClass</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="n">NNbiases</span>
</span><span id="CNNModel.loadModel-227"><a href="#CNNModel.loadModel-227"><span class="linenos">227</span></a>        <span class="n">neuralNetwork</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">NNweights</span>
</span><span id="CNNModel.loadModel-228"><a href="#CNNModel.loadModel-228"><span class="linenos">228</span></a>        <span class="n">neuralNetwork</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="n">NNbiases</span>
</span><span id="CNNModel.loadModel-229"><a href="#CNNModel.loadModel-229"><span class="linenos">229</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">CNNweights</span>
</span><span id="CNNModel.loadModel-230"><a href="#CNNModel.loadModel-230"><span class="linenos">230</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="n">CNNbiases</span>
</span><span id="CNNModel.loadModel-231"><a href="#CNNModel.loadModel-231"><span class="linenos">231</span></a>
</span><span id="CNNModel.loadModel-232"><a href="#CNNModel.loadModel-232"><span class="linenos">232</span></a>        <span class="n">currWeightIndex</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="CNNModel.loadModel-233"><a href="#CNNModel.loadModel-233"><span class="linenos">233</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)):</span>
</span><span id="CNNModel.loadModel-234"><a href="#CNNModel.loadModel-234"><span class="linenos">234</span></a>            <span class="k">if</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">==</span> <span class="n">Conv2DLayer</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">==</span> <span class="n">Conv1DLayer</span><span class="p">):</span>
</span><span id="CNNModel.loadModel-235"><a href="#CNNModel.loadModel-235"><span class="linenos">235</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">kernalWeights</span> <span class="o">=</span> <span class="n">CNNweights</span><span class="p">[</span><span class="n">currWeightIndex</span><span class="p">]</span>
</span><span id="CNNModel.loadModel-236"><a href="#CNNModel.loadModel-236"><span class="linenos">236</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">kernalBiases</span> <span class="o">=</span> <span class="n">CNNbiases</span><span class="p">[</span><span class="n">currWeightIndex</span><span class="p">]</span>
</span><span id="CNNModel.loadModel-237"><a href="#CNNModel.loadModel-237"><span class="linenos">237</span></a>                <span class="n">currWeightIndex</span> <span class="o">+=</span> <span class="mi">1</span>
</span></pre></div>


            <div class="docstring"><p>Loads model weights and biases from a compressed npz file and assigns them to layers.</p>

<p>Args:
    neuralNetwork (class): The dense neural network to load weights into.
    filename (str, optional): Filename to save the weights. Default is "modelWeights.npz".</p>
</div>


                            </div>
                </section>
                <section id="Conv1DLayer">
                            <input id="Conv1DLayer-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">Conv1DLayer</span>:

                <label class="view-source-button" for="Conv1DLayer-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Conv1DLayer"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Conv1DLayer-5"><a href="#Conv1DLayer-5"><span class="linenos"> 5</span></a><span class="k">class</span><span class="w"> </span><span class="nc">Conv1DLayer</span><span class="p">():</span>
</span><span id="Conv1DLayer-6"><a href="#Conv1DLayer-6"><span class="linenos"> 6</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernalSize</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">numKernals</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span> <span class="o">=</span> <span class="s2">&quot;no&quot;</span><span class="p">):</span>
</span><span id="Conv1DLayer-7"><a href="#Conv1DLayer-7"><span class="linenos"> 7</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">kernalSize</span> <span class="o">=</span> <span class="n">kernalSize</span>
</span><span id="Conv1DLayer-8"><a href="#Conv1DLayer-8"><span class="linenos"> 8</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">numKernals</span> <span class="o">=</span> <span class="n">numKernals</span>
</span><span id="Conv1DLayer-9"><a href="#Conv1DLayer-9"><span class="linenos"> 9</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">kernalWeights</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="Conv1DLayer-10"><a href="#Conv1DLayer-10"><span class="linenos">10</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">kernalBiases</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="Conv1DLayer-11"><a href="#Conv1DLayer-11"><span class="linenos">11</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">depth</span> <span class="o">=</span> <span class="n">depth</span>
</span><span id="Conv1DLayer-12"><a href="#Conv1DLayer-12"><span class="linenos">12</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>
</span><span id="Conv1DLayer-13"><a href="#Conv1DLayer-13"><span class="linenos">13</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">padding</span>
</span><span id="Conv1DLayer-14"><a href="#Conv1DLayer-14"><span class="linenos">14</span></a>
</span><span id="Conv1DLayer-15"><a href="#Conv1DLayer-15"><span class="linenos">15</span></a>        <span class="c1"># cant do padding.lower() because if user puts 1 it will throw a error</span>
</span><span id="Conv1DLayer-16"><a href="#Conv1DLayer-16"><span class="linenos">16</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">padding</span> <span class="o">==</span> <span class="s2">&quot;no&quot;</span> <span class="ow">or</span> <span class="n">padding</span> <span class="o">==</span> <span class="s2">&quot;n&quot;</span> <span class="ow">or</span> <span class="n">padding</span> <span class="o">==</span> <span class="s2">&quot;NO&quot;</span> <span class="ow">or</span> <span class="n">padding</span> <span class="o">==</span> <span class="s2">&quot;N&quot;</span><span class="p">):</span>
</span><span id="Conv1DLayer-17"><a href="#Conv1DLayer-17"><span class="linenos">17</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">usePadding</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="Conv1DLayer-18"><a href="#Conv1DLayer-18"><span class="linenos">18</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="Conv1DLayer-19"><a href="#Conv1DLayer-19"><span class="linenos">19</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">)</span>
</span><span id="Conv1DLayer-20"><a href="#Conv1DLayer-20"><span class="linenos">20</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">usePadding</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="Conv1DLayer-21"><a href="#Conv1DLayer-21"><span class="linenos">21</span></a>    
</span><span id="Conv1DLayer-22"><a href="#Conv1DLayer-22"><span class="linenos">22</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_padImage</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputTensor</span><span class="p">,</span> <span class="n">kernalSize</span><span class="p">,</span> <span class="n">strideLength</span><span class="p">,</span> <span class="n">typeOfPadding</span><span class="p">):</span> <span class="c1">#pads image</span>
</span><span id="Conv1DLayer-23"><a href="#Conv1DLayer-23"><span class="linenos">23</span></a>        <span class="n">batch_size</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">length</span> <span class="o">=</span> <span class="n">inputTensor</span><span class="o">.</span><span class="n">shape</span>
</span><span id="Conv1DLayer-24"><a href="#Conv1DLayer-24"><span class="linenos">24</span></a>        <span class="n">paddingSize</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(((</span><span class="n">strideLength</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">length</span> <span class="o">-</span> <span class="n">strideLength</span> <span class="o">+</span> <span class="n">kernalSize</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="Conv1DLayer-25"><a href="#Conv1DLayer-25"><span class="linenos">25</span></a>        <span class="n">padded_length</span> <span class="o">=</span> <span class="n">length</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">paddingSize</span>
</span><span id="Conv1DLayer-26"><a href="#Conv1DLayer-26"><span class="linenos">26</span></a>        
</span><span id="Conv1DLayer-27"><a href="#Conv1DLayer-27"><span class="linenos">27</span></a>        <span class="n">padded</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">padded_length</span><span class="p">),</span> <span class="n">typeOfPadding</span><span class="p">)</span>
</span><span id="Conv1DLayer-28"><a href="#Conv1DLayer-28"><span class="linenos">28</span></a>        
</span><span id="Conv1DLayer-29"><a href="#Conv1DLayer-29"><span class="linenos">29</span></a>        <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
</span><span id="Conv1DLayer-30"><a href="#Conv1DLayer-30"><span class="linenos">30</span></a>            <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">depth</span><span class="p">):</span>
</span><span id="Conv1DLayer-31"><a href="#Conv1DLayer-31"><span class="linenos">31</span></a>                <span class="n">padded</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">paddingSize</span><span class="p">:</span><span class="n">paddingSize</span> <span class="o">+</span> <span class="n">length</span><span class="p">]</span> <span class="o">=</span> <span class="n">inputTensor</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">d</span><span class="p">]</span>
</span><span id="Conv1DLayer-32"><a href="#Conv1DLayer-32"><span class="linenos">32</span></a>        <span class="k">return</span> <span class="n">padded</span>
</span><span id="Conv1DLayer-33"><a href="#Conv1DLayer-33"><span class="linenos">33</span></a>
</span><span id="Conv1DLayer-34"><a href="#Conv1DLayer-34"><span class="linenos">34</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputTensor</span><span class="p">):</span>
</span><span id="Conv1DLayer-35"><a href="#Conv1DLayer-35"><span class="linenos">35</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">usePadding</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="Conv1DLayer-36"><a href="#Conv1DLayer-36"><span class="linenos">36</span></a>            <span class="n">inputTensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_padImage</span><span class="p">(</span><span class="n">inputTensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernalSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">)</span>
</span><span id="Conv1DLayer-37"><a href="#Conv1DLayer-37"><span class="linenos">37</span></a>
</span><span id="Conv1DLayer-38"><a href="#Conv1DLayer-38"><span class="linenos">38</span></a>        <span class="n">batchSize</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">seqLength</span> <span class="o">=</span> <span class="n">inputTensor</span><span class="o">.</span><span class="n">shape</span>
</span><span id="Conv1DLayer-39"><a href="#Conv1DLayer-39"><span class="linenos">39</span></a>        <span class="n">outputLength</span> <span class="o">=</span> <span class="p">(</span><span class="n">seqLength</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernalSize</span><span class="p">)</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="Conv1DLayer-40"><a href="#Conv1DLayer-40"><span class="linenos">40</span></a>        <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batchSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">numKernals</span><span class="p">,</span> <span class="n">outputLength</span><span class="p">))</span>
</span><span id="Conv1DLayer-41"><a href="#Conv1DLayer-41"><span class="linenos">41</span></a>        
</span><span id="Conv1DLayer-42"><a href="#Conv1DLayer-42"><span class="linenos">42</span></a>        <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batchSize</span><span class="p">):</span>
</span><span id="Conv1DLayer-43"><a href="#Conv1DLayer-43"><span class="linenos">43</span></a>            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">numKernals</span><span class="p">):</span>
</span><span id="Conv1DLayer-44"><a href="#Conv1DLayer-44"><span class="linenos">44</span></a>                <span class="n">kernal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernalWeights</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
</span><span id="Conv1DLayer-45"><a href="#Conv1DLayer-45"><span class="linenos">45</span></a>                <span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernalBiases</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
</span><span id="Conv1DLayer-46"><a href="#Conv1DLayer-46"><span class="linenos">46</span></a>                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">outputLength</span><span class="p">):</span>
</span><span id="Conv1DLayer-47"><a href="#Conv1DLayer-47"><span class="linenos">47</span></a>                    <span class="n">start</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
</span><span id="Conv1DLayer-48"><a href="#Conv1DLayer-48"><span class="linenos">48</span></a>                    <span class="n">end</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernalSize</span>
</span><span id="Conv1DLayer-49"><a href="#Conv1DLayer-49"><span class="linenos">49</span></a>                    <span class="n">region</span> <span class="o">=</span> <span class="n">inputTensor</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="p">:,</span> <span class="n">start</span><span class="p">:</span> <span class="n">end</span><span class="p">]</span>
</span><span id="Conv1DLayer-50"><a href="#Conv1DLayer-50"><span class="linenos">50</span></a>
</span><span id="Conv1DLayer-51"><a href="#Conv1DLayer-51"><span class="linenos">51</span></a>                    <span class="k">if</span><span class="p">(</span><span class="n">region</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">kernal</span><span class="o">.</span><span class="n">shape</span><span class="p">):</span>
</span><span id="Conv1DLayer-52"><a href="#Conv1DLayer-52"><span class="linenos">52</span></a>                        <span class="k">continue</span>
</span><span id="Conv1DLayer-53"><a href="#Conv1DLayer-53"><span class="linenos">53</span></a>
</span><span id="Conv1DLayer-54"><a href="#Conv1DLayer-54"><span class="linenos">54</span></a>                    <span class="n">output</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">region</span> <span class="o">*</span> <span class="n">kernal</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias</span>
</span><span id="Conv1DLayer-55"><a href="#Conv1DLayer-55"><span class="linenos">55</span></a>        <span class="k">return</span> <span class="n">output</span>
</span><span id="Conv1DLayer-56"><a href="#Conv1DLayer-56"><span class="linenos">56</span></a>                    
</span><span id="Conv1DLayer-57"><a href="#Conv1DLayer-57"><span class="linenos">57</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_backpropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">errorPatch</span><span class="p">,</span> <span class="n">inputTensor</span><span class="p">):</span> 
</span><span id="Conv1DLayer-58"><a href="#Conv1DLayer-58"><span class="linenos">58</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">usePadding</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="Conv1DLayer-59"><a href="#Conv1DLayer-59"><span class="linenos">59</span></a>            <span class="n">inputTensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_padImage</span><span class="p">(</span><span class="n">inputTensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernalSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">)</span>
</span><span id="Conv1DLayer-60"><a href="#Conv1DLayer-60"><span class="linenos">60</span></a>        
</span><span id="Conv1DLayer-61"><a href="#Conv1DLayer-61"><span class="linenos">61</span></a>        <span class="n">batchSize</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">seqLength</span> <span class="o">=</span> <span class="n">inputTensor</span><span class="o">.</span><span class="n">shape</span>
</span><span id="Conv1DLayer-62"><a href="#Conv1DLayer-62"><span class="linenos">62</span></a>        <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">outputLength</span> <span class="o">=</span> <span class="n">errorPatch</span><span class="o">.</span><span class="n">shape</span>
</span><span id="Conv1DLayer-63"><a href="#Conv1DLayer-63"><span class="linenos">63</span></a>
</span><span id="Conv1DLayer-64"><a href="#Conv1DLayer-64"><span class="linenos">64</span></a>        <span class="n">weightGradients</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernalWeights</span><span class="p">)</span>
</span><span id="Conv1DLayer-65"><a href="#Conv1DLayer-65"><span class="linenos">65</span></a>        <span class="n">inputErrorTerms</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">inputTensor</span><span class="p">)</span>
</span><span id="Conv1DLayer-66"><a href="#Conv1DLayer-66"><span class="linenos">66</span></a>
</span><span id="Conv1DLayer-67"><a href="#Conv1DLayer-67"><span class="linenos">67</span></a>        <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batchSize</span><span class="p">):</span>
</span><span id="Conv1DLayer-68"><a href="#Conv1DLayer-68"><span class="linenos">68</span></a>            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">numKernals</span><span class="p">):</span>
</span><span id="Conv1DLayer-69"><a href="#Conv1DLayer-69"><span class="linenos">69</span></a>                <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">depth</span><span class="p">):</span>
</span><span id="Conv1DLayer-70"><a href="#Conv1DLayer-70"><span class="linenos">70</span></a>                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">outputLength</span><span class="p">):</span>
</span><span id="Conv1DLayer-71"><a href="#Conv1DLayer-71"><span class="linenos">71</span></a>                        <span class="n">start</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
</span><span id="Conv1DLayer-72"><a href="#Conv1DLayer-72"><span class="linenos">72</span></a>                        <span class="n">end</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernalSize</span>
</span><span id="Conv1DLayer-73"><a href="#Conv1DLayer-73"><span class="linenos">73</span></a>                        <span class="k">if</span><span class="p">(</span><span class="n">end</span> <span class="o">&gt;</span> <span class="n">seqLength</span><span class="p">):</span>
</span><span id="Conv1DLayer-74"><a href="#Conv1DLayer-74"><span class="linenos">74</span></a>                            <span class="k">continue</span>
</span><span id="Conv1DLayer-75"><a href="#Conv1DLayer-75"><span class="linenos">75</span></a>                        <span class="n">region</span> <span class="o">=</span> <span class="n">inputTensor</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span> <span class="n">end</span><span class="p">]</span>
</span><span id="Conv1DLayer-76"><a href="#Conv1DLayer-76"><span class="linenos">76</span></a>                        <span class="n">weightGradients</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">d</span><span class="p">]</span> <span class="o">+=</span> <span class="n">errorPatch</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">region</span>
</span><span id="Conv1DLayer-77"><a href="#Conv1DLayer-77"><span class="linenos">77</span></a>        
</span><span id="Conv1DLayer-78"><a href="#Conv1DLayer-78"><span class="linenos">78</span></a>        <span class="n">biasGradients</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">errorPatch</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</span><span id="Conv1DLayer-79"><a href="#Conv1DLayer-79"><span class="linenos">79</span></a>
</span><span id="Conv1DLayer-80"><a href="#Conv1DLayer-80"><span class="linenos">80</span></a>        <span class="n">flippedKernels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernalWeights</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span><span id="Conv1DLayer-81"><a href="#Conv1DLayer-81"><span class="linenos">81</span></a>        <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batchSize</span><span class="p">):</span>
</span><span id="Conv1DLayer-82"><a href="#Conv1DLayer-82"><span class="linenos">82</span></a>            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">numKernals</span><span class="p">):</span>
</span><span id="Conv1DLayer-83"><a href="#Conv1DLayer-83"><span class="linenos">83</span></a>                <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">depth</span><span class="p">):</span>
</span><span id="Conv1DLayer-84"><a href="#Conv1DLayer-84"><span class="linenos">84</span></a>                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">outputLength</span><span class="p">):</span>
</span><span id="Conv1DLayer-85"><a href="#Conv1DLayer-85"><span class="linenos">85</span></a>                        <span class="n">start</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
</span><span id="Conv1DLayer-86"><a href="#Conv1DLayer-86"><span class="linenos">86</span></a>                        <span class="n">end</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernalSize</span>
</span><span id="Conv1DLayer-87"><a href="#Conv1DLayer-87"><span class="linenos">87</span></a>                        <span class="k">if</span><span class="p">(</span><span class="n">end</span> <span class="o">&gt;</span> <span class="n">seqLength</span><span class="p">):</span>
</span><span id="Conv1DLayer-88"><a href="#Conv1DLayer-88"><span class="linenos">88</span></a>                            <span class="k">continue</span>
</span><span id="Conv1DLayer-89"><a href="#Conv1DLayer-89"><span class="linenos">89</span></a>                        <span class="n">inputErrorTerms</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span> <span class="n">end</span><span class="p">]</span> <span class="o">+=</span> <span class="n">errorPatch</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">flippedKernels</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">d</span><span class="p">]</span>
</span><span id="Conv1DLayer-90"><a href="#Conv1DLayer-90"><span class="linenos">90</span></a>                
</span><span id="Conv1DLayer-91"><a href="#Conv1DLayer-91"><span class="linenos">91</span></a>        <span class="k">return</span> <span class="n">weightGradients</span><span class="p">,</span> <span class="n">biasGradients</span><span class="p">,</span> <span class="n">inputErrorTerms</span>
</span></pre></div>


    

                            <div id="Conv1DLayer.__init__" class="classattr">
                                        <input id="Conv1DLayer.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">Conv1DLayer</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">kernalSize</span>, </span><span class="param"><span class="n">depth</span>, </span><span class="param"><span class="n">numKernals</span>, </span><span class="param"><span class="n">stride</span>, </span><span class="param"><span class="n">padding</span><span class="o">=</span><span class="s1">&#39;no&#39;</span></span>)</span>

                <label class="view-source-button" for="Conv1DLayer.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Conv1DLayer.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Conv1DLayer.__init__-6"><a href="#Conv1DLayer.__init__-6"><span class="linenos"> 6</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernalSize</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">numKernals</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span> <span class="o">=</span> <span class="s2">&quot;no&quot;</span><span class="p">):</span>
</span><span id="Conv1DLayer.__init__-7"><a href="#Conv1DLayer.__init__-7"><span class="linenos"> 7</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">kernalSize</span> <span class="o">=</span> <span class="n">kernalSize</span>
</span><span id="Conv1DLayer.__init__-8"><a href="#Conv1DLayer.__init__-8"><span class="linenos"> 8</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">numKernals</span> <span class="o">=</span> <span class="n">numKernals</span>
</span><span id="Conv1DLayer.__init__-9"><a href="#Conv1DLayer.__init__-9"><span class="linenos"> 9</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">kernalWeights</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="Conv1DLayer.__init__-10"><a href="#Conv1DLayer.__init__-10"><span class="linenos">10</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">kernalBiases</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="Conv1DLayer.__init__-11"><a href="#Conv1DLayer.__init__-11"><span class="linenos">11</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">depth</span> <span class="o">=</span> <span class="n">depth</span>
</span><span id="Conv1DLayer.__init__-12"><a href="#Conv1DLayer.__init__-12"><span class="linenos">12</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>
</span><span id="Conv1DLayer.__init__-13"><a href="#Conv1DLayer.__init__-13"><span class="linenos">13</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">padding</span>
</span><span id="Conv1DLayer.__init__-14"><a href="#Conv1DLayer.__init__-14"><span class="linenos">14</span></a>
</span><span id="Conv1DLayer.__init__-15"><a href="#Conv1DLayer.__init__-15"><span class="linenos">15</span></a>        <span class="c1"># cant do padding.lower() because if user puts 1 it will throw a error</span>
</span><span id="Conv1DLayer.__init__-16"><a href="#Conv1DLayer.__init__-16"><span class="linenos">16</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">padding</span> <span class="o">==</span> <span class="s2">&quot;no&quot;</span> <span class="ow">or</span> <span class="n">padding</span> <span class="o">==</span> <span class="s2">&quot;n&quot;</span> <span class="ow">or</span> <span class="n">padding</span> <span class="o">==</span> <span class="s2">&quot;NO&quot;</span> <span class="ow">or</span> <span class="n">padding</span> <span class="o">==</span> <span class="s2">&quot;N&quot;</span><span class="p">):</span>
</span><span id="Conv1DLayer.__init__-17"><a href="#Conv1DLayer.__init__-17"><span class="linenos">17</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">usePadding</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="Conv1DLayer.__init__-18"><a href="#Conv1DLayer.__init__-18"><span class="linenos">18</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="Conv1DLayer.__init__-19"><a href="#Conv1DLayer.__init__-19"><span class="linenos">19</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">)</span>
</span><span id="Conv1DLayer.__init__-20"><a href="#Conv1DLayer.__init__-20"><span class="linenos">20</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">usePadding</span> <span class="o">=</span> <span class="kc">True</span>
</span></pre></div>


    

                            </div>
                            <div id="Conv1DLayer.kernalSize" class="classattr">
                                <div class="attr variable">
            <span class="name">kernalSize</span>

        
    </div>
    <a class="headerlink" href="#Conv1DLayer.kernalSize"></a>
    
    

                            </div>
                            <div id="Conv1DLayer.numKernals" class="classattr">
                                <div class="attr variable">
            <span class="name">numKernals</span>

        
    </div>
    <a class="headerlink" href="#Conv1DLayer.numKernals"></a>
    
    

                            </div>
                            <div id="Conv1DLayer.kernalWeights" class="classattr">
                                <div class="attr variable">
            <span class="name">kernalWeights</span>

        
    </div>
    <a class="headerlink" href="#Conv1DLayer.kernalWeights"></a>
    
    

                            </div>
                            <div id="Conv1DLayer.kernalBiases" class="classattr">
                                <div class="attr variable">
            <span class="name">kernalBiases</span>

        
    </div>
    <a class="headerlink" href="#Conv1DLayer.kernalBiases"></a>
    
    

                            </div>
                            <div id="Conv1DLayer.depth" class="classattr">
                                <div class="attr variable">
            <span class="name">depth</span>

        
    </div>
    <a class="headerlink" href="#Conv1DLayer.depth"></a>
    
    

                            </div>
                            <div id="Conv1DLayer.stride" class="classattr">
                                <div class="attr variable">
            <span class="name">stride</span>

        
    </div>
    <a class="headerlink" href="#Conv1DLayer.stride"></a>
    
    

                            </div>
                            <div id="Conv1DLayer.padding" class="classattr">
                                <div class="attr variable">
            <span class="name">padding</span>

        
    </div>
    <a class="headerlink" href="#Conv1DLayer.padding"></a>
    
    

                            </div>
                            <div id="Conv1DLayer.forward" class="classattr">
                                        <input id="Conv1DLayer.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">inputTensor</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="Conv1DLayer.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Conv1DLayer.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Conv1DLayer.forward-34"><a href="#Conv1DLayer.forward-34"><span class="linenos">34</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputTensor</span><span class="p">):</span>
</span><span id="Conv1DLayer.forward-35"><a href="#Conv1DLayer.forward-35"><span class="linenos">35</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">usePadding</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="Conv1DLayer.forward-36"><a href="#Conv1DLayer.forward-36"><span class="linenos">36</span></a>            <span class="n">inputTensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_padImage</span><span class="p">(</span><span class="n">inputTensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernalSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">)</span>
</span><span id="Conv1DLayer.forward-37"><a href="#Conv1DLayer.forward-37"><span class="linenos">37</span></a>
</span><span id="Conv1DLayer.forward-38"><a href="#Conv1DLayer.forward-38"><span class="linenos">38</span></a>        <span class="n">batchSize</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">seqLength</span> <span class="o">=</span> <span class="n">inputTensor</span><span class="o">.</span><span class="n">shape</span>
</span><span id="Conv1DLayer.forward-39"><a href="#Conv1DLayer.forward-39"><span class="linenos">39</span></a>        <span class="n">outputLength</span> <span class="o">=</span> <span class="p">(</span><span class="n">seqLength</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernalSize</span><span class="p">)</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="Conv1DLayer.forward-40"><a href="#Conv1DLayer.forward-40"><span class="linenos">40</span></a>        <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batchSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">numKernals</span><span class="p">,</span> <span class="n">outputLength</span><span class="p">))</span>
</span><span id="Conv1DLayer.forward-41"><a href="#Conv1DLayer.forward-41"><span class="linenos">41</span></a>        
</span><span id="Conv1DLayer.forward-42"><a href="#Conv1DLayer.forward-42"><span class="linenos">42</span></a>        <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batchSize</span><span class="p">):</span>
</span><span id="Conv1DLayer.forward-43"><a href="#Conv1DLayer.forward-43"><span class="linenos">43</span></a>            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">numKernals</span><span class="p">):</span>
</span><span id="Conv1DLayer.forward-44"><a href="#Conv1DLayer.forward-44"><span class="linenos">44</span></a>                <span class="n">kernal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernalWeights</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
</span><span id="Conv1DLayer.forward-45"><a href="#Conv1DLayer.forward-45"><span class="linenos">45</span></a>                <span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernalBiases</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
</span><span id="Conv1DLayer.forward-46"><a href="#Conv1DLayer.forward-46"><span class="linenos">46</span></a>                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">outputLength</span><span class="p">):</span>
</span><span id="Conv1DLayer.forward-47"><a href="#Conv1DLayer.forward-47"><span class="linenos">47</span></a>                    <span class="n">start</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
</span><span id="Conv1DLayer.forward-48"><a href="#Conv1DLayer.forward-48"><span class="linenos">48</span></a>                    <span class="n">end</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernalSize</span>
</span><span id="Conv1DLayer.forward-49"><a href="#Conv1DLayer.forward-49"><span class="linenos">49</span></a>                    <span class="n">region</span> <span class="o">=</span> <span class="n">inputTensor</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="p">:,</span> <span class="n">start</span><span class="p">:</span> <span class="n">end</span><span class="p">]</span>
</span><span id="Conv1DLayer.forward-50"><a href="#Conv1DLayer.forward-50"><span class="linenos">50</span></a>
</span><span id="Conv1DLayer.forward-51"><a href="#Conv1DLayer.forward-51"><span class="linenos">51</span></a>                    <span class="k">if</span><span class="p">(</span><span class="n">region</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">kernal</span><span class="o">.</span><span class="n">shape</span><span class="p">):</span>
</span><span id="Conv1DLayer.forward-52"><a href="#Conv1DLayer.forward-52"><span class="linenos">52</span></a>                        <span class="k">continue</span>
</span><span id="Conv1DLayer.forward-53"><a href="#Conv1DLayer.forward-53"><span class="linenos">53</span></a>
</span><span id="Conv1DLayer.forward-54"><a href="#Conv1DLayer.forward-54"><span class="linenos">54</span></a>                    <span class="n">output</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">region</span> <span class="o">*</span> <span class="n">kernal</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias</span>
</span><span id="Conv1DLayer.forward-55"><a href="#Conv1DLayer.forward-55"><span class="linenos">55</span></a>        <span class="k">return</span> <span class="n">output</span>
</span></pre></div>


    

                            </div>
                </section>
                <section id="Conv2DLayer">
                            <input id="Conv2DLayer-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">Conv2DLayer</span>:

                <label class="view-source-button" for="Conv2DLayer-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Conv2DLayer"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Conv2DLayer-5"><a href="#Conv2DLayer-5"><span class="linenos">  5</span></a><span class="k">class</span><span class="w"> </span><span class="nc">Conv2DLayer</span><span class="p">():</span>
</span><span id="Conv2DLayer-6"><a href="#Conv2DLayer-6"><span class="linenos">  6</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernalSize</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">numKernals</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span> <span class="o">=</span> <span class="s2">&quot;no&quot;</span><span class="p">):</span>
</span><span id="Conv2DLayer-7"><a href="#Conv2DLayer-7"><span class="linenos">  7</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="Conv2DLayer-8"><a href="#Conv2DLayer-8"><span class="linenos">  8</span></a><span class="sd">        Initialises a convolutional layer.</span>
</span><span id="Conv2DLayer-9"><a href="#Conv2DLayer-9"><span class="linenos">  9</span></a>
</span><span id="Conv2DLayer-10"><a href="#Conv2DLayer-10"><span class="linenos"> 10</span></a><span class="sd">        Args:</span>
</span><span id="Conv2DLayer-11"><a href="#Conv2DLayer-11"><span class="linenos"> 11</span></a><span class="sd">            kernalSize (int): The size of the covolution kernel (assumed it is a square).</span>
</span><span id="Conv2DLayer-12"><a href="#Conv2DLayer-12"><span class="linenos"> 12</span></a><span class="sd">            depth (int): Depth of the input tensor.</span>
</span><span id="Conv2DLayer-13"><a href="#Conv2DLayer-13"><span class="linenos"> 13</span></a><span class="sd">            numKernals (int): Number of kernels in this layer.</span>
</span><span id="Conv2DLayer-14"><a href="#Conv2DLayer-14"><span class="linenos"> 14</span></a><span class="sd">            stride (int): The stride length for convolution.</span>
</span><span id="Conv2DLayer-15"><a href="#Conv2DLayer-15"><span class="linenos"> 15</span></a><span class="sd">            padding (str or int, optional): Padding size or &quot;no&quot; for no padding. Default is &quot;no&quot;.</span>
</span><span id="Conv2DLayer-16"><a href="#Conv2DLayer-16"><span class="linenos"> 16</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="Conv2DLayer-17"><a href="#Conv2DLayer-17"><span class="linenos"> 17</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">kernalSize</span> <span class="o">=</span> <span class="n">kernalSize</span>
</span><span id="Conv2DLayer-18"><a href="#Conv2DLayer-18"><span class="linenos"> 18</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">numKernals</span> <span class="o">=</span> <span class="n">numKernals</span>
</span><span id="Conv2DLayer-19"><a href="#Conv2DLayer-19"><span class="linenos"> 19</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">kernalWeights</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="Conv2DLayer-20"><a href="#Conv2DLayer-20"><span class="linenos"> 20</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">kernalBiases</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="Conv2DLayer-21"><a href="#Conv2DLayer-21"><span class="linenos"> 21</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">depth</span> <span class="o">=</span> <span class="n">depth</span>
</span><span id="Conv2DLayer-22"><a href="#Conv2DLayer-22"><span class="linenos"> 22</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>
</span><span id="Conv2DLayer-23"><a href="#Conv2DLayer-23"><span class="linenos"> 23</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">padding</span>
</span><span id="Conv2DLayer-24"><a href="#Conv2DLayer-24"><span class="linenos"> 24</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">padding</span> <span class="o">==</span> <span class="s2">&quot;no&quot;</span> <span class="ow">or</span> <span class="n">padding</span> <span class="o">==</span> <span class="s2">&quot;n&quot;</span> <span class="ow">or</span> <span class="n">padding</span> <span class="o">==</span> <span class="s2">&quot;NO&quot;</span> <span class="ow">or</span> <span class="n">padding</span> <span class="o">==</span> <span class="s2">&quot;N&quot;</span><span class="p">):</span>
</span><span id="Conv2DLayer-25"><a href="#Conv2DLayer-25"><span class="linenos"> 25</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">usePadding</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="Conv2DLayer-26"><a href="#Conv2DLayer-26"><span class="linenos"> 26</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="Conv2DLayer-27"><a href="#Conv2DLayer-27"><span class="linenos"> 27</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">)</span>
</span><span id="Conv2DLayer-28"><a href="#Conv2DLayer-28"><span class="linenos"> 28</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">usePadding</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="Conv2DLayer-29"><a href="#Conv2DLayer-29"><span class="linenos"> 29</span></a>    
</span><span id="Conv2DLayer-30"><a href="#Conv2DLayer-30"><span class="linenos"> 30</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_padImage</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputTensor</span><span class="p">,</span> <span class="n">kernalSize</span><span class="p">,</span> <span class="n">strideLength</span><span class="p">,</span> <span class="n">typeOfPadding</span><span class="p">):</span> <span class="c1">#pads image</span>
</span><span id="Conv2DLayer-31"><a href="#Conv2DLayer-31"><span class="linenos"> 31</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="Conv2DLayer-32"><a href="#Conv2DLayer-32"><span class="linenos"> 32</span></a><span class="sd">        Pads each image in the input tensor.</span>
</span><span id="Conv2DLayer-33"><a href="#Conv2DLayer-33"><span class="linenos"> 33</span></a>
</span><span id="Conv2DLayer-34"><a href="#Conv2DLayer-34"><span class="linenos"> 34</span></a><span class="sd">        Args:</span>
</span><span id="Conv2DLayer-35"><a href="#Conv2DLayer-35"><span class="linenos"> 35</span></a><span class="sd">            inputTensor (ndarray): A 3D array representing the images with shape (number of images, height, width).</span>
</span><span id="Conv2DLayer-36"><a href="#Conv2DLayer-36"><span class="linenos"> 36</span></a><span class="sd">            kernalSize (int): The size of the covolution kernel (assumed it is a square).</span>
</span><span id="Conv2DLayer-37"><a href="#Conv2DLayer-37"><span class="linenos"> 37</span></a><span class="sd">            strideLength (int): The stride length for convolution.</span>
</span><span id="Conv2DLayer-38"><a href="#Conv2DLayer-38"><span class="linenos"> 38</span></a><span class="sd">            typeOfPadding (int): The value used for padding the images.</span>
</span><span id="Conv2DLayer-39"><a href="#Conv2DLayer-39"><span class="linenos"> 39</span></a><span class="sd">        </span>
</span><span id="Conv2DLayer-40"><a href="#Conv2DLayer-40"><span class="linenos"> 40</span></a><span class="sd">        Returns:</span>
</span><span id="Conv2DLayer-41"><a href="#Conv2DLayer-41"><span class="linenos"> 41</span></a><span class="sd">            ndarray: A 3D array of padded images.</span>
</span><span id="Conv2DLayer-42"><a href="#Conv2DLayer-42"><span class="linenos"> 42</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="Conv2DLayer-43"><a href="#Conv2DLayer-43"><span class="linenos"> 43</span></a>        <span class="n">batch_size</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">inputTensor</span><span class="o">.</span><span class="n">shape</span>
</span><span id="Conv2DLayer-44"><a href="#Conv2DLayer-44"><span class="linenos"> 44</span></a>        <span class="n">paddingSize</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(((</span><span class="n">strideLength</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputTensor</span><span class="p">)</span> <span class="o">-</span> <span class="n">strideLength</span> <span class="o">+</span> <span class="n">kernalSize</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="Conv2DLayer-45"><a href="#Conv2DLayer-45"><span class="linenos"> 45</span></a>        <span class="n">padded_height</span> <span class="o">=</span> <span class="n">height</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">paddingSize</span>
</span><span id="Conv2DLayer-46"><a href="#Conv2DLayer-46"><span class="linenos"> 46</span></a>        <span class="n">padded_width</span> <span class="o">=</span> <span class="n">width</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">paddingSize</span>
</span><span id="Conv2DLayer-47"><a href="#Conv2DLayer-47"><span class="linenos"> 47</span></a>
</span><span id="Conv2DLayer-48"><a href="#Conv2DLayer-48"><span class="linenos"> 48</span></a>        <span class="n">padded</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">padded_height</span><span class="p">,</span> <span class="n">padded_width</span><span class="p">),</span> <span class="n">typeOfPadding</span><span class="p">)</span>
</span><span id="Conv2DLayer-49"><a href="#Conv2DLayer-49"><span class="linenos"> 49</span></a>
</span><span id="Conv2DLayer-50"><a href="#Conv2DLayer-50"><span class="linenos"> 50</span></a>        <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
</span><span id="Conv2DLayer-51"><a href="#Conv2DLayer-51"><span class="linenos"> 51</span></a>            <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">depth</span><span class="p">):</span>
</span><span id="Conv2DLayer-52"><a href="#Conv2DLayer-52"><span class="linenos"> 52</span></a>                <span class="n">padded</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">paddingSize</span><span class="p">:</span><span class="n">paddingSize</span> <span class="o">+</span> <span class="n">height</span><span class="p">,</span> <span class="n">paddingSize</span><span class="p">:</span><span class="n">paddingSize</span> <span class="o">+</span> <span class="n">width</span><span class="p">]</span> <span class="o">=</span> <span class="n">inputTensor</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">d</span><span class="p">]</span>
</span><span id="Conv2DLayer-53"><a href="#Conv2DLayer-53"><span class="linenos"> 53</span></a>
</span><span id="Conv2DLayer-54"><a href="#Conv2DLayer-54"><span class="linenos"> 54</span></a>        <span class="k">return</span> <span class="n">padded</span>
</span><span id="Conv2DLayer-55"><a href="#Conv2DLayer-55"><span class="linenos"> 55</span></a>
</span><span id="Conv2DLayer-56"><a href="#Conv2DLayer-56"><span class="linenos"> 56</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">imageTensor</span><span class="p">):</span>
</span><span id="Conv2DLayer-57"><a href="#Conv2DLayer-57"><span class="linenos"> 57</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">usePadding</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="Conv2DLayer-58"><a href="#Conv2DLayer-58"><span class="linenos"> 58</span></a>            <span class="n">imageTensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_padImage</span><span class="p">(</span><span class="n">imageTensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernalSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">)</span>
</span><span id="Conv2DLayer-59"><a href="#Conv2DLayer-59"><span class="linenos"> 59</span></a>        <span class="n">outputHeight</span> <span class="o">=</span> <span class="p">(</span><span class="n">imageTensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernalSize</span><span class="p">)</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="Conv2DLayer-60"><a href="#Conv2DLayer-60"><span class="linenos"> 60</span></a>        <span class="n">outputWidth</span> <span class="o">=</span> <span class="p">(</span><span class="n">imageTensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernalSize</span><span class="p">)</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="Conv2DLayer-61"><a href="#Conv2DLayer-61"><span class="linenos"> 61</span></a>        
</span><span id="Conv2DLayer-62"><a href="#Conv2DLayer-62"><span class="linenos"> 62</span></a>        <span class="k">assert</span> <span class="n">outputHeight</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">outputWidth</span> <span class="o">&gt;</span> <span class="mi">0</span>
</span><span id="Conv2DLayer-63"><a href="#Conv2DLayer-63"><span class="linenos"> 63</span></a>
</span><span id="Conv2DLayer-64"><a href="#Conv2DLayer-64"><span class="linenos"> 64</span></a>        <span class="n">batchSize</span><span class="p">,</span> <span class="n">inputDepth</span><span class="p">,</span> <span class="n">inputHeight</span><span class="p">,</span> <span class="n">inputWidth</span> <span class="o">=</span> <span class="n">imageTensor</span><span class="o">.</span><span class="n">shape</span>
</span><span id="Conv2DLayer-65"><a href="#Conv2DLayer-65"><span class="linenos"> 65</span></a>
</span><span id="Conv2DLayer-66"><a href="#Conv2DLayer-66"><span class="linenos"> 66</span></a>        <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batchSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">numKernals</span><span class="p">,</span> <span class="n">outputHeight</span><span class="p">,</span> <span class="n">outputWidth</span><span class="p">))</span>
</span><span id="Conv2DLayer-67"><a href="#Conv2DLayer-67"><span class="linenos"> 67</span></a>
</span><span id="Conv2DLayer-68"><a href="#Conv2DLayer-68"><span class="linenos"> 68</span></a>        <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batchSize</span><span class="p">):</span>
</span><span id="Conv2DLayer-69"><a href="#Conv2DLayer-69"><span class="linenos"> 69</span></a>            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">numKernals</span><span class="p">):</span>
</span><span id="Conv2DLayer-70"><a href="#Conv2DLayer-70"><span class="linenos"> 70</span></a>                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">outputHeight</span><span class="p">):</span>
</span><span id="Conv2DLayer-71"><a href="#Conv2DLayer-71"><span class="linenos"> 71</span></a>                    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">outputWidth</span><span class="p">):</span>
</span><span id="Conv2DLayer-72"><a href="#Conv2DLayer-72"><span class="linenos"> 72</span></a>                        <span class="n">startI</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
</span><span id="Conv2DLayer-73"><a href="#Conv2DLayer-73"><span class="linenos"> 73</span></a>                        <span class="n">startJ</span> <span class="o">=</span> <span class="n">j</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
</span><span id="Conv2DLayer-74"><a href="#Conv2DLayer-74"><span class="linenos"> 74</span></a>                        <span class="n">region</span> <span class="o">=</span> <span class="n">imageTensor</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="p">:,</span> <span class="n">startI</span><span class="p">:</span> <span class="n">startI</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernalSize</span><span class="p">,</span> <span class="n">startJ</span><span class="p">:</span><span class="n">startJ</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernalSize</span><span class="p">]</span>
</span><span id="Conv2DLayer-75"><a href="#Conv2DLayer-75"><span class="linenos"> 75</span></a>                        <span class="n">output</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">region</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernalWeights</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernalBiases</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
</span><span id="Conv2DLayer-76"><a href="#Conv2DLayer-76"><span class="linenos"> 76</span></a>        <span class="k">return</span> <span class="n">output</span> 
</span><span id="Conv2DLayer-77"><a href="#Conv2DLayer-77"><span class="linenos"> 77</span></a>                    
</span><span id="Conv2DLayer-78"><a href="#Conv2DLayer-78"><span class="linenos"> 78</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_backpropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">errorPatch</span><span class="p">,</span> <span class="n">inputTensor</span><span class="p">):</span>
</span><span id="Conv2DLayer-79"><a href="#Conv2DLayer-79"><span class="linenos"> 79</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="Conv2DLayer-80"><a href="#Conv2DLayer-80"><span class="linenos"> 80</span></a><span class="sd">        Compute gradients for conolutional layer weights, biases and input errors during backpropagation.</span>
</span><span id="Conv2DLayer-81"><a href="#Conv2DLayer-81"><span class="linenos"> 81</span></a>
</span><span id="Conv2DLayer-82"><a href="#Conv2DLayer-82"><span class="linenos"> 82</span></a><span class="sd">        Args:</span>
</span><span id="Conv2DLayer-83"><a href="#Conv2DLayer-83"><span class="linenos"> 83</span></a><span class="sd">            errorPatch (ndarray): Error gradient from the next layer.</span>
</span><span id="Conv2DLayer-84"><a href="#Conv2DLayer-84"><span class="linenos"> 84</span></a><span class="sd">            inputTensor (ndarray): Input to the convolutional layer during forward propagation.</span>
</span><span id="Conv2DLayer-85"><a href="#Conv2DLayer-85"><span class="linenos"> 85</span></a><span class="sd">        </span>
</span><span id="Conv2DLayer-86"><a href="#Conv2DLayer-86"><span class="linenos"> 86</span></a><span class="sd">        Returns:</span>
</span><span id="Conv2DLayer-87"><a href="#Conv2DLayer-87"><span class="linenos"> 87</span></a><span class="sd">            weightGradients (ndarray): Gradients of the loss with respect to kernels.</span>
</span><span id="Conv2DLayer-88"><a href="#Conv2DLayer-88"><span class="linenos"> 88</span></a><span class="sd">            biasGradients (ndarray): Gradients of the loss with respect to biases for each kernel.</span>
</span><span id="Conv2DLayer-89"><a href="#Conv2DLayer-89"><span class="linenos"> 89</span></a><span class="sd">            inputErrorTerms (ndarray): Error terms propagated to the previous layer.</span>
</span><span id="Conv2DLayer-90"><a href="#Conv2DLayer-90"><span class="linenos"> 90</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="Conv2DLayer-91"><a href="#Conv2DLayer-91"><span class="linenos"> 91</span></a>        <span class="c1">###################################        </span>
</span><span id="Conv2DLayer-92"><a href="#Conv2DLayer-92"><span class="linenos"> 92</span></a>        <span class="c1"># gets the error gradient from the layer infront and it is a error patch</span>
</span><span id="Conv2DLayer-93"><a href="#Conv2DLayer-93"><span class="linenos"> 93</span></a>        <span class="c1"># this error patch is the same size as what the convolutional layer outputed during forward propgation</span>
</span><span id="Conv2DLayer-94"><a href="#Conv2DLayer-94"><span class="linenos"> 94</span></a>        <span class="c1"># get the kernal (as in a patch of the image) again, but this time you are multipling each value in the kernal by 1 value that is inside the error patch</span>
</span><span id="Conv2DLayer-95"><a href="#Conv2DLayer-95"><span class="linenos"> 95</span></a>        <span class="c1"># this makes the gradient of the loss of one kernal&#39;s weight</span>
</span><span id="Conv2DLayer-96"><a href="#Conv2DLayer-96"><span class="linenos"> 96</span></a>        
</span><span id="Conv2DLayer-97"><a href="#Conv2DLayer-97"><span class="linenos"> 97</span></a>        <span class="c1"># the gradient of the loss of one kernal&#39;s bias is the summ of all the error terms</span>
</span><span id="Conv2DLayer-98"><a href="#Conv2DLayer-98"><span class="linenos"> 98</span></a>        <span class="c1"># because bias is applied to every input in forward propgation</span>
</span><span id="Conv2DLayer-99"><a href="#Conv2DLayer-99"><span class="linenos"> 99</span></a>        
</span><span id="Conv2DLayer-100"><a href="#Conv2DLayer-100"><span class="linenos">100</span></a>        <span class="c1"># the gradient of the loss of the input, which is the error terms for the layer behind it</span>
</span><span id="Conv2DLayer-101"><a href="#Conv2DLayer-101"><span class="linenos">101</span></a>        <span class="c1"># firstly the kernal has to be flipped, meaning flip the kernal left to right and then top to bottom, but not flipping the layers,</span>
</span><span id="Conv2DLayer-102"><a href="#Conv2DLayer-102"><span class="linenos">102</span></a>        <span class="c1"># the gradient of one pixel, is the summ of each error term multiplied by the flipped kernal </span>
</span><span id="Conv2DLayer-103"><a href="#Conv2DLayer-103"><span class="linenos">103</span></a>        <span class="c1">###################################     </span>
</span><span id="Conv2DLayer-104"><a href="#Conv2DLayer-104"><span class="linenos">104</span></a>        
</span><span id="Conv2DLayer-105"><a href="#Conv2DLayer-105"><span class="linenos">105</span></a>        <span class="n">batch_size</span><span class="p">,</span> <span class="n">in_depth</span><span class="p">,</span> <span class="n">in_h</span><span class="p">,</span> <span class="n">in_w</span> <span class="o">=</span> <span class="n">inputTensor</span><span class="o">.</span><span class="n">shape</span>
</span><span id="Conv2DLayer-106"><a href="#Conv2DLayer-106"><span class="linenos">106</span></a>        <span class="n">_</span><span class="p">,</span> <span class="n">num_kernels</span><span class="p">,</span> <span class="n">out_h</span><span class="p">,</span> <span class="n">out_w</span> <span class="o">=</span> <span class="n">errorPatch</span><span class="o">.</span><span class="n">shape</span>
</span><span id="Conv2DLayer-107"><a href="#Conv2DLayer-107"><span class="linenos">107</span></a>        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernalSize</span>
</span><span id="Conv2DLayer-108"><a href="#Conv2DLayer-108"><span class="linenos">108</span></a>        <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
</span><span id="Conv2DLayer-109"><a href="#Conv2DLayer-109"><span class="linenos">109</span></a>
</span><span id="Conv2DLayer-110"><a href="#Conv2DLayer-110"><span class="linenos">110</span></a>        <span class="n">weightGradients</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">numKernals</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">depth</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">k</span><span class="p">))</span>
</span><span id="Conv2DLayer-111"><a href="#Conv2DLayer-111"><span class="linenos">111</span></a>        <span class="n">biasGradients</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">numKernals</span><span class="p">,))</span>
</span><span id="Conv2DLayer-112"><a href="#Conv2DLayer-112"><span class="linenos">112</span></a>        <span class="n">inputErrorTerms</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">inputTensor</span><span class="p">)</span>
</span><span id="Conv2DLayer-113"><a href="#Conv2DLayer-113"><span class="linenos">113</span></a>
</span><span id="Conv2DLayer-114"><a href="#Conv2DLayer-114"><span class="linenos">114</span></a>        <span class="n">flippedKernels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernalWeights</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">::</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span><span id="Conv2DLayer-115"><a href="#Conv2DLayer-115"><span class="linenos">115</span></a>
</span><span id="Conv2DLayer-116"><a href="#Conv2DLayer-116"><span class="linenos">116</span></a>        <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
</span><span id="Conv2DLayer-117"><a href="#Conv2DLayer-117"><span class="linenos">117</span></a>            <span class="k">for</span> <span class="n">k_i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">numKernals</span><span class="p">):</span>
</span><span id="Conv2DLayer-118"><a href="#Conv2DLayer-118"><span class="linenos">118</span></a>                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">out_h</span><span class="p">):</span>
</span><span id="Conv2DLayer-119"><a href="#Conv2DLayer-119"><span class="linenos">119</span></a>                    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">out_w</span><span class="p">):</span>
</span><span id="Conv2DLayer-120"><a href="#Conv2DLayer-120"><span class="linenos">120</span></a>                        <span class="n">start_i</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">s</span>
</span><span id="Conv2DLayer-121"><a href="#Conv2DLayer-121"><span class="linenos">121</span></a>                        <span class="n">start_j</span> <span class="o">=</span> <span class="n">j</span> <span class="o">*</span> <span class="n">s</span>
</span><span id="Conv2DLayer-122"><a href="#Conv2DLayer-122"><span class="linenos">122</span></a>                        <span class="n">region</span> <span class="o">=</span> <span class="n">inputTensor</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="p">:,</span> <span class="n">start_i</span><span class="p">:</span><span class="n">start_i</span> <span class="o">+</span> <span class="n">k</span><span class="p">,</span> <span class="n">start_j</span><span class="p">:</span><span class="n">start_j</span> <span class="o">+</span> <span class="n">k</span><span class="p">]</span>
</span><span id="Conv2DLayer-123"><a href="#Conv2DLayer-123"><span class="linenos">123</span></a>                        <span class="n">weightGradients</span><span class="p">[</span><span class="n">k_i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">errorPatch</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">k_i</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">region</span>
</span><span id="Conv2DLayer-124"><a href="#Conv2DLayer-124"><span class="linenos">124</span></a>                        <span class="n">inputErrorTerms</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="p">:,</span> <span class="n">start_i</span><span class="p">:</span><span class="n">start_i</span> <span class="o">+</span> <span class="n">k</span><span class="p">,</span> <span class="n">start_j</span><span class="p">:</span><span class="n">start_j</span> <span class="o">+</span> <span class="n">k</span><span class="p">]</span> <span class="o">+=</span> <span class="n">errorPatch</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">k_i</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">flippedKernels</span><span class="p">[</span><span class="n">k_i</span><span class="p">]</span>
</span><span id="Conv2DLayer-125"><a href="#Conv2DLayer-125"><span class="linenos">125</span></a>
</span><span id="Conv2DLayer-126"><a href="#Conv2DLayer-126"><span class="linenos">126</span></a>        <span class="n">biasGradients</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">errorPatch</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</span><span id="Conv2DLayer-127"><a href="#Conv2DLayer-127"><span class="linenos">127</span></a>        <span class="k">return</span> <span class="n">weightGradients</span><span class="p">,</span> <span class="n">biasGradients</span><span class="p">,</span> <span class="n">inputErrorTerms</span>
</span></pre></div>


    

                            <div id="Conv2DLayer.__init__" class="classattr">
                                        <input id="Conv2DLayer.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">Conv2DLayer</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">kernalSize</span>, </span><span class="param"><span class="n">depth</span>, </span><span class="param"><span class="n">numKernals</span>, </span><span class="param"><span class="n">stride</span>, </span><span class="param"><span class="n">padding</span><span class="o">=</span><span class="s1">&#39;no&#39;</span></span>)</span>

                <label class="view-source-button" for="Conv2DLayer.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Conv2DLayer.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Conv2DLayer.__init__-6"><a href="#Conv2DLayer.__init__-6"><span class="linenos"> 6</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernalSize</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">numKernals</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span> <span class="o">=</span> <span class="s2">&quot;no&quot;</span><span class="p">):</span>
</span><span id="Conv2DLayer.__init__-7"><a href="#Conv2DLayer.__init__-7"><span class="linenos"> 7</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="Conv2DLayer.__init__-8"><a href="#Conv2DLayer.__init__-8"><span class="linenos"> 8</span></a><span class="sd">        Initialises a convolutional layer.</span>
</span><span id="Conv2DLayer.__init__-9"><a href="#Conv2DLayer.__init__-9"><span class="linenos"> 9</span></a>
</span><span id="Conv2DLayer.__init__-10"><a href="#Conv2DLayer.__init__-10"><span class="linenos">10</span></a><span class="sd">        Args:</span>
</span><span id="Conv2DLayer.__init__-11"><a href="#Conv2DLayer.__init__-11"><span class="linenos">11</span></a><span class="sd">            kernalSize (int): The size of the covolution kernel (assumed it is a square).</span>
</span><span id="Conv2DLayer.__init__-12"><a href="#Conv2DLayer.__init__-12"><span class="linenos">12</span></a><span class="sd">            depth (int): Depth of the input tensor.</span>
</span><span id="Conv2DLayer.__init__-13"><a href="#Conv2DLayer.__init__-13"><span class="linenos">13</span></a><span class="sd">            numKernals (int): Number of kernels in this layer.</span>
</span><span id="Conv2DLayer.__init__-14"><a href="#Conv2DLayer.__init__-14"><span class="linenos">14</span></a><span class="sd">            stride (int): The stride length for convolution.</span>
</span><span id="Conv2DLayer.__init__-15"><a href="#Conv2DLayer.__init__-15"><span class="linenos">15</span></a><span class="sd">            padding (str or int, optional): Padding size or &quot;no&quot; for no padding. Default is &quot;no&quot;.</span>
</span><span id="Conv2DLayer.__init__-16"><a href="#Conv2DLayer.__init__-16"><span class="linenos">16</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="Conv2DLayer.__init__-17"><a href="#Conv2DLayer.__init__-17"><span class="linenos">17</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">kernalSize</span> <span class="o">=</span> <span class="n">kernalSize</span>
</span><span id="Conv2DLayer.__init__-18"><a href="#Conv2DLayer.__init__-18"><span class="linenos">18</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">numKernals</span> <span class="o">=</span> <span class="n">numKernals</span>
</span><span id="Conv2DLayer.__init__-19"><a href="#Conv2DLayer.__init__-19"><span class="linenos">19</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">kernalWeights</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="Conv2DLayer.__init__-20"><a href="#Conv2DLayer.__init__-20"><span class="linenos">20</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">kernalBiases</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="Conv2DLayer.__init__-21"><a href="#Conv2DLayer.__init__-21"><span class="linenos">21</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">depth</span> <span class="o">=</span> <span class="n">depth</span>
</span><span id="Conv2DLayer.__init__-22"><a href="#Conv2DLayer.__init__-22"><span class="linenos">22</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>
</span><span id="Conv2DLayer.__init__-23"><a href="#Conv2DLayer.__init__-23"><span class="linenos">23</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">padding</span>
</span><span id="Conv2DLayer.__init__-24"><a href="#Conv2DLayer.__init__-24"><span class="linenos">24</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">padding</span> <span class="o">==</span> <span class="s2">&quot;no&quot;</span> <span class="ow">or</span> <span class="n">padding</span> <span class="o">==</span> <span class="s2">&quot;n&quot;</span> <span class="ow">or</span> <span class="n">padding</span> <span class="o">==</span> <span class="s2">&quot;NO&quot;</span> <span class="ow">or</span> <span class="n">padding</span> <span class="o">==</span> <span class="s2">&quot;N&quot;</span><span class="p">):</span>
</span><span id="Conv2DLayer.__init__-25"><a href="#Conv2DLayer.__init__-25"><span class="linenos">25</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">usePadding</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="Conv2DLayer.__init__-26"><a href="#Conv2DLayer.__init__-26"><span class="linenos">26</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="Conv2DLayer.__init__-27"><a href="#Conv2DLayer.__init__-27"><span class="linenos">27</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">)</span>
</span><span id="Conv2DLayer.__init__-28"><a href="#Conv2DLayer.__init__-28"><span class="linenos">28</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">usePadding</span> <span class="o">=</span> <span class="kc">True</span>
</span></pre></div>


            <div class="docstring"><p>Initialises a convolutional layer.</p>

<p>Args:
    kernalSize (int): The size of the covolution kernel (assumed it is a square).
    depth (int): Depth of the input tensor.
    numKernals (int): Number of kernels in this layer.
    stride (int): The stride length for convolution.
    padding (str or int, optional): Padding size or "no" for no padding. Default is "no".</p>
</div>


                            </div>
                            <div id="Conv2DLayer.kernalSize" class="classattr">
                                <div class="attr variable">
            <span class="name">kernalSize</span>

        
    </div>
    <a class="headerlink" href="#Conv2DLayer.kernalSize"></a>
    
    

                            </div>
                            <div id="Conv2DLayer.numKernals" class="classattr">
                                <div class="attr variable">
            <span class="name">numKernals</span>

        
    </div>
    <a class="headerlink" href="#Conv2DLayer.numKernals"></a>
    
    

                            </div>
                            <div id="Conv2DLayer.kernalWeights" class="classattr">
                                <div class="attr variable">
            <span class="name">kernalWeights</span>

        
    </div>
    <a class="headerlink" href="#Conv2DLayer.kernalWeights"></a>
    
    

                            </div>
                            <div id="Conv2DLayer.kernalBiases" class="classattr">
                                <div class="attr variable">
            <span class="name">kernalBiases</span>

        
    </div>
    <a class="headerlink" href="#Conv2DLayer.kernalBiases"></a>
    
    

                            </div>
                            <div id="Conv2DLayer.depth" class="classattr">
                                <div class="attr variable">
            <span class="name">depth</span>

        
    </div>
    <a class="headerlink" href="#Conv2DLayer.depth"></a>
    
    

                            </div>
                            <div id="Conv2DLayer.stride" class="classattr">
                                <div class="attr variable">
            <span class="name">stride</span>

        
    </div>
    <a class="headerlink" href="#Conv2DLayer.stride"></a>
    
    

                            </div>
                            <div id="Conv2DLayer.padding" class="classattr">
                                <div class="attr variable">
            <span class="name">padding</span>

        
    </div>
    <a class="headerlink" href="#Conv2DLayer.padding"></a>
    
    

                            </div>
                            <div id="Conv2DLayer.forward" class="classattr">
                                        <input id="Conv2DLayer.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">imageTensor</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="Conv2DLayer.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Conv2DLayer.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Conv2DLayer.forward-56"><a href="#Conv2DLayer.forward-56"><span class="linenos">56</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">imageTensor</span><span class="p">):</span>
</span><span id="Conv2DLayer.forward-57"><a href="#Conv2DLayer.forward-57"><span class="linenos">57</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">usePadding</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="Conv2DLayer.forward-58"><a href="#Conv2DLayer.forward-58"><span class="linenos">58</span></a>            <span class="n">imageTensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_padImage</span><span class="p">(</span><span class="n">imageTensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernalSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">)</span>
</span><span id="Conv2DLayer.forward-59"><a href="#Conv2DLayer.forward-59"><span class="linenos">59</span></a>        <span class="n">outputHeight</span> <span class="o">=</span> <span class="p">(</span><span class="n">imageTensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernalSize</span><span class="p">)</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="Conv2DLayer.forward-60"><a href="#Conv2DLayer.forward-60"><span class="linenos">60</span></a>        <span class="n">outputWidth</span> <span class="o">=</span> <span class="p">(</span><span class="n">imageTensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernalSize</span><span class="p">)</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="Conv2DLayer.forward-61"><a href="#Conv2DLayer.forward-61"><span class="linenos">61</span></a>        
</span><span id="Conv2DLayer.forward-62"><a href="#Conv2DLayer.forward-62"><span class="linenos">62</span></a>        <span class="k">assert</span> <span class="n">outputHeight</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">outputWidth</span> <span class="o">&gt;</span> <span class="mi">0</span>
</span><span id="Conv2DLayer.forward-63"><a href="#Conv2DLayer.forward-63"><span class="linenos">63</span></a>
</span><span id="Conv2DLayer.forward-64"><a href="#Conv2DLayer.forward-64"><span class="linenos">64</span></a>        <span class="n">batchSize</span><span class="p">,</span> <span class="n">inputDepth</span><span class="p">,</span> <span class="n">inputHeight</span><span class="p">,</span> <span class="n">inputWidth</span> <span class="o">=</span> <span class="n">imageTensor</span><span class="o">.</span><span class="n">shape</span>
</span><span id="Conv2DLayer.forward-65"><a href="#Conv2DLayer.forward-65"><span class="linenos">65</span></a>
</span><span id="Conv2DLayer.forward-66"><a href="#Conv2DLayer.forward-66"><span class="linenos">66</span></a>        <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batchSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">numKernals</span><span class="p">,</span> <span class="n">outputHeight</span><span class="p">,</span> <span class="n">outputWidth</span><span class="p">))</span>
</span><span id="Conv2DLayer.forward-67"><a href="#Conv2DLayer.forward-67"><span class="linenos">67</span></a>
</span><span id="Conv2DLayer.forward-68"><a href="#Conv2DLayer.forward-68"><span class="linenos">68</span></a>        <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batchSize</span><span class="p">):</span>
</span><span id="Conv2DLayer.forward-69"><a href="#Conv2DLayer.forward-69"><span class="linenos">69</span></a>            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">numKernals</span><span class="p">):</span>
</span><span id="Conv2DLayer.forward-70"><a href="#Conv2DLayer.forward-70"><span class="linenos">70</span></a>                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">outputHeight</span><span class="p">):</span>
</span><span id="Conv2DLayer.forward-71"><a href="#Conv2DLayer.forward-71"><span class="linenos">71</span></a>                    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">outputWidth</span><span class="p">):</span>
</span><span id="Conv2DLayer.forward-72"><a href="#Conv2DLayer.forward-72"><span class="linenos">72</span></a>                        <span class="n">startI</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
</span><span id="Conv2DLayer.forward-73"><a href="#Conv2DLayer.forward-73"><span class="linenos">73</span></a>                        <span class="n">startJ</span> <span class="o">=</span> <span class="n">j</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
</span><span id="Conv2DLayer.forward-74"><a href="#Conv2DLayer.forward-74"><span class="linenos">74</span></a>                        <span class="n">region</span> <span class="o">=</span> <span class="n">imageTensor</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="p">:,</span> <span class="n">startI</span><span class="p">:</span> <span class="n">startI</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernalSize</span><span class="p">,</span> <span class="n">startJ</span><span class="p">:</span><span class="n">startJ</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernalSize</span><span class="p">]</span>
</span><span id="Conv2DLayer.forward-75"><a href="#Conv2DLayer.forward-75"><span class="linenos">75</span></a>                        <span class="n">output</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">region</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernalWeights</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernalBiases</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
</span><span id="Conv2DLayer.forward-76"><a href="#Conv2DLayer.forward-76"><span class="linenos">76</span></a>        <span class="k">return</span> <span class="n">output</span> 
</span></pre></div>


    

                            </div>
                </section>
                <section id="ActivationLayer">
                            <input id="ActivationLayer-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">ActivationLayer</span>:

                <label class="view-source-button" for="ActivationLayer-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ActivationLayer"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ActivationLayer-5"><a href="#ActivationLayer-5"><span class="linenos"> 5</span></a><span class="k">class</span><span class="w"> </span><span class="nc">ActivationLayer</span><span class="p">:</span> <span class="c1"># basically aplies an activation function over the whole Tensor (eg. leaky relu)</span>
</span><span id="ActivationLayer-6"><a href="#ActivationLayer-6"><span class="linenos"> 6</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputTensor</span><span class="p">):</span>
</span><span id="ActivationLayer-7"><a href="#ActivationLayer-7"><span class="linenos"> 7</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="ActivationLayer-8"><a href="#ActivationLayer-8"><span class="linenos"> 8</span></a><span class="sd">        Applies the Leaky ReLU activation function to the input tensor.</span>
</span><span id="ActivationLayer-9"><a href="#ActivationLayer-9"><span class="linenos"> 9</span></a>
</span><span id="ActivationLayer-10"><a href="#ActivationLayer-10"><span class="linenos">10</span></a><span class="sd">        Args:</span>
</span><span id="ActivationLayer-11"><a href="#ActivationLayer-11"><span class="linenos">11</span></a><span class="sd">            inputTensor (ndarray): A 3D array representing the input.</span>
</span><span id="ActivationLayer-12"><a href="#ActivationLayer-12"><span class="linenos">12</span></a><span class="sd">        </span>
</span><span id="ActivationLayer-13"><a href="#ActivationLayer-13"><span class="linenos">13</span></a><span class="sd">        Returns:</span>
</span><span id="ActivationLayer-14"><a href="#ActivationLayer-14"><span class="linenos">14</span></a><span class="sd">            ndarray: A tensor with the same shape as the input with Leaky ReLU applied to it.</span>
</span><span id="ActivationLayer-15"><a href="#ActivationLayer-15"><span class="linenos">15</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ActivationLayer-16"><a href="#ActivationLayer-16"><span class="linenos">16</span></a>        <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.01</span>
</span><span id="ActivationLayer-17"><a href="#ActivationLayer-17"><span class="linenos">17</span></a>        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">inputTensor</span><span class="p">,</span> <span class="n">inputTensor</span> <span class="o">*</span> <span class="n">alpha</span><span class="p">)</span>
</span><span id="ActivationLayer-18"><a href="#ActivationLayer-18"><span class="linenos">18</span></a>
</span><span id="ActivationLayer-19"><a href="#ActivationLayer-19"><span class="linenos">19</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_backpropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">errorPatch</span><span class="p">,</span> <span class="n">inputTensor</span><span class="p">):</span>
</span><span id="ActivationLayer-20"><a href="#ActivationLayer-20"><span class="linenos">20</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="ActivationLayer-21"><a href="#ActivationLayer-21"><span class="linenos">21</span></a><span class="sd">        Compute the gradient of the loss with respect to the input of the activation layer during backpropagation.</span>
</span><span id="ActivationLayer-22"><a href="#ActivationLayer-22"><span class="linenos">22</span></a>
</span><span id="ActivationLayer-23"><a href="#ActivationLayer-23"><span class="linenos">23</span></a><span class="sd">        Args:</span>
</span><span id="ActivationLayer-24"><a href="#ActivationLayer-24"><span class="linenos">24</span></a><span class="sd">            errorPatch (ndarray): Error gradient from the next layer.</span>
</span><span id="ActivationLayer-25"><a href="#ActivationLayer-25"><span class="linenos">25</span></a><span class="sd">            inputTensor (ndarray): Input to the activation layer during forward propagation.</span>
</span><span id="ActivationLayer-26"><a href="#ActivationLayer-26"><span class="linenos">26</span></a><span class="sd">        </span>
</span><span id="ActivationLayer-27"><a href="#ActivationLayer-27"><span class="linenos">27</span></a><span class="sd">        Returns:</span>
</span><span id="ActivationLayer-28"><a href="#ActivationLayer-28"><span class="linenos">28</span></a><span class="sd">            inputGradient (ndarray): Gradient of the loss with respect to the inputTensor</span>
</span><span id="ActivationLayer-29"><a href="#ActivationLayer-29"><span class="linenos">29</span></a><span class="sd">        &quot;&quot;&quot;</span>  
</span><span id="ActivationLayer-30"><a href="#ActivationLayer-30"><span class="linenos">30</span></a>        <span class="k">return</span> <span class="n">errorPatch</span> <span class="o">*</span> <span class="n">ReLUDerivative</span><span class="p">(</span><span class="n">inputTensor</span><span class="p">)</span>
</span></pre></div>


    

                            <div id="ActivationLayer.forward" class="classattr">
                                        <input id="ActivationLayer.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">inputTensor</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="ActivationLayer.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ActivationLayer.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ActivationLayer.forward-6"><a href="#ActivationLayer.forward-6"><span class="linenos"> 6</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputTensor</span><span class="p">):</span>
</span><span id="ActivationLayer.forward-7"><a href="#ActivationLayer.forward-7"><span class="linenos"> 7</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="ActivationLayer.forward-8"><a href="#ActivationLayer.forward-8"><span class="linenos"> 8</span></a><span class="sd">        Applies the Leaky ReLU activation function to the input tensor.</span>
</span><span id="ActivationLayer.forward-9"><a href="#ActivationLayer.forward-9"><span class="linenos"> 9</span></a>
</span><span id="ActivationLayer.forward-10"><a href="#ActivationLayer.forward-10"><span class="linenos">10</span></a><span class="sd">        Args:</span>
</span><span id="ActivationLayer.forward-11"><a href="#ActivationLayer.forward-11"><span class="linenos">11</span></a><span class="sd">            inputTensor (ndarray): A 3D array representing the input.</span>
</span><span id="ActivationLayer.forward-12"><a href="#ActivationLayer.forward-12"><span class="linenos">12</span></a><span class="sd">        </span>
</span><span id="ActivationLayer.forward-13"><a href="#ActivationLayer.forward-13"><span class="linenos">13</span></a><span class="sd">        Returns:</span>
</span><span id="ActivationLayer.forward-14"><a href="#ActivationLayer.forward-14"><span class="linenos">14</span></a><span class="sd">            ndarray: A tensor with the same shape as the input with Leaky ReLU applied to it.</span>
</span><span id="ActivationLayer.forward-15"><a href="#ActivationLayer.forward-15"><span class="linenos">15</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="ActivationLayer.forward-16"><a href="#ActivationLayer.forward-16"><span class="linenos">16</span></a>        <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.01</span>
</span><span id="ActivationLayer.forward-17"><a href="#ActivationLayer.forward-17"><span class="linenos">17</span></a>        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">inputTensor</span><span class="p">,</span> <span class="n">inputTensor</span> <span class="o">*</span> <span class="n">alpha</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Applies the Leaky ReLU activation function to the input tensor.</p>

<p>Args:
    inputTensor (ndarray): A 3D array representing the input.</p>

<p>Returns:
    ndarray: A tensor with the same shape as the input with Leaky ReLU applied to it.</p>
</div>


                            </div>
                </section>
                <section id="DenseLayer">
                            <input id="DenseLayer-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">DenseLayer</span>:

                <label class="view-source-button" for="DenseLayer-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#DenseLayer"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="DenseLayer-4"><a href="#DenseLayer-4"><span class="linenos"> 4</span></a><span class="k">class</span><span class="w"> </span><span class="nc">DenseLayer</span><span class="p">:</span> <span class="c1"># basically a fancy neural network</span>
</span><span id="DenseLayer-5"><a href="#DenseLayer-5"><span class="linenos"> 5</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">NeuralNetworkClass</span><span class="p">):</span>
</span><span id="DenseLayer-6"><a href="#DenseLayer-6"><span class="linenos"> 6</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="DenseLayer-7"><a href="#DenseLayer-7"><span class="linenos"> 7</span></a><span class="sd">        Initialises a dense layer using a NeuralNetworkClass.</span>
</span><span id="DenseLayer-8"><a href="#DenseLayer-8"><span class="linenos"> 8</span></a>
</span><span id="DenseLayer-9"><a href="#DenseLayer-9"><span class="linenos"> 9</span></a><span class="sd">        Args:</span>
</span><span id="DenseLayer-10"><a href="#DenseLayer-10"><span class="linenos">10</span></a><span class="sd">            NeuralNetworkClass (class): the fully connected neural network class.</span>
</span><span id="DenseLayer-11"><a href="#DenseLayer-11"><span class="linenos">11</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="DenseLayer-12"><a href="#DenseLayer-12"><span class="linenos">12</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">NeuralNetworkClass</span> <span class="o">=</span> <span class="n">NeuralNetworkClass</span>
</span><span id="DenseLayer-13"><a href="#DenseLayer-13"><span class="linenos">13</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">orignalShape</span> <span class="o">=</span> <span class="mi">0</span>   <span class="c1"># orignalShape is the original shape of the input tensor</span>
</span><span id="DenseLayer-14"><a href="#DenseLayer-14"><span class="linenos">14</span></a>        
</span><span id="DenseLayer-15"><a href="#DenseLayer-15"><span class="linenos">15</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_flatternTensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputTensor</span><span class="p">):</span>
</span><span id="DenseLayer-16"><a href="#DenseLayer-16"><span class="linenos">16</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="DenseLayer-17"><a href="#DenseLayer-17"><span class="linenos">17</span></a><span class="sd">        Flattens a tensor into a 1D array.</span>
</span><span id="DenseLayer-18"><a href="#DenseLayer-18"><span class="linenos">18</span></a>
</span><span id="DenseLayer-19"><a href="#DenseLayer-19"><span class="linenos">19</span></a><span class="sd">        Args:</span>
</span><span id="DenseLayer-20"><a href="#DenseLayer-20"><span class="linenos">20</span></a><span class="sd">            inputTensor (ndarray): A tensor of any shape.</span>
</span><span id="DenseLayer-21"><a href="#DenseLayer-21"><span class="linenos">21</span></a><span class="sd">        </span>
</span><span id="DenseLayer-22"><a href="#DenseLayer-22"><span class="linenos">22</span></a><span class="sd">        Returns:</span>
</span><span id="DenseLayer-23"><a href="#DenseLayer-23"><span class="linenos">23</span></a><span class="sd">            ndarray: A 1D array containing every element of the input tensor.</span>
</span><span id="DenseLayer-24"><a href="#DenseLayer-24"><span class="linenos">24</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="DenseLayer-25"><a href="#DenseLayer-25"><span class="linenos">25</span></a>        <span class="n">inputTensor</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputTensor</span><span class="p">)</span>
</span><span id="DenseLayer-26"><a href="#DenseLayer-26"><span class="linenos">26</span></a>        <span class="n">batchSize</span> <span class="o">=</span> <span class="n">inputTensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="DenseLayer-27"><a href="#DenseLayer-27"><span class="linenos">27</span></a>        <span class="k">return</span> <span class="n">inputTensor</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batchSize</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="DenseLayer-28"><a href="#DenseLayer-28"><span class="linenos">28</span></a>
</span><span id="DenseLayer-29"><a href="#DenseLayer-29"><span class="linenos">29</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputTensor</span><span class="p">):</span>
</span><span id="DenseLayer-30"><a href="#DenseLayer-30"><span class="linenos">30</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="DenseLayer-31"><a href="#DenseLayer-31"><span class="linenos">31</span></a><span class="sd">        Flattens the input tensor and performs a forward pass.</span>
</span><span id="DenseLayer-32"><a href="#DenseLayer-32"><span class="linenos">32</span></a>
</span><span id="DenseLayer-33"><a href="#DenseLayer-33"><span class="linenos">33</span></a><span class="sd">        Args:</span>
</span><span id="DenseLayer-34"><a href="#DenseLayer-34"><span class="linenos">34</span></a><span class="sd">            inputTensor (ndarray): Input tensor to flatten and process.</span>
</span><span id="DenseLayer-35"><a href="#DenseLayer-35"><span class="linenos">35</span></a><span class="sd">        </span>
</span><span id="DenseLayer-36"><a href="#DenseLayer-36"><span class="linenos">36</span></a><span class="sd">        Returns:</span>
</span><span id="DenseLayer-37"><a href="#DenseLayer-37"><span class="linenos">37</span></a><span class="sd">            ndarray: Output of the dense layer.</span>
</span><span id="DenseLayer-38"><a href="#DenseLayer-38"><span class="linenos">38</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="DenseLayer-39"><a href="#DenseLayer-39"><span class="linenos">39</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">orignalShape</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputTensor</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</span><span id="DenseLayer-40"><a href="#DenseLayer-40"><span class="linenos">40</span></a>        <span class="n">inputArray</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flatternTensor</span><span class="p">(</span><span class="n">inputTensor</span><span class="p">)</span>
</span><span id="DenseLayer-41"><a href="#DenseLayer-41"><span class="linenos">41</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">layerNodes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">NeuralNetworkClass</span><span class="o">.</span><span class="n">forwardPropagation</span><span class="p">(</span><span class="n">inputArray</span><span class="p">)</span>
</span><span id="DenseLayer-42"><a href="#DenseLayer-42"><span class="linenos">42</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">layerNodes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span><span id="DenseLayer-43"><a href="#DenseLayer-43"><span class="linenos">43</span></a>    
</span><span id="DenseLayer-44"><a href="#DenseLayer-44"><span class="linenos">44</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_backpropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trueValues</span><span class="p">):</span> <span class="c1">#return weigtGradients, biasGradients, errorTerms</span>
</span><span id="DenseLayer-45"><a href="#DenseLayer-45"><span class="linenos">45</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="DenseLayer-46"><a href="#DenseLayer-46"><span class="linenos">46</span></a><span class="sd">        Performs backpropagation through the dense layer.</span>
</span><span id="DenseLayer-47"><a href="#DenseLayer-47"><span class="linenos">47</span></a>
</span><span id="DenseLayer-48"><a href="#DenseLayer-48"><span class="linenos">48</span></a><span class="sd">        Args:</span>
</span><span id="DenseLayer-49"><a href="#DenseLayer-49"><span class="linenos">49</span></a><span class="sd">            trueValues (ndarray): True labels for the input data.</span>
</span><span id="DenseLayer-50"><a href="#DenseLayer-50"><span class="linenos">50</span></a><span class="sd">        </span>
</span><span id="DenseLayer-51"><a href="#DenseLayer-51"><span class="linenos">51</span></a><span class="sd">        Returns:</span>
</span><span id="DenseLayer-52"><a href="#DenseLayer-52"><span class="linenos">52</span></a><span class="sd">            weightGradients (list of ndarray): Gradients of weights for each layer.</span>
</span><span id="DenseLayer-53"><a href="#DenseLayer-53"><span class="linenos">53</span></a><span class="sd">            biasGradients (list of ndarray): Gradients of biases for each layer.</span>
</span><span id="DenseLayer-54"><a href="#DenseLayer-54"><span class="linenos">54</span></a><span class="sd">            errorTerms (ndarray): Error terms from the output layer weights, reshaped to the input tensor.   </span>
</span><span id="DenseLayer-55"><a href="#DenseLayer-55"><span class="linenos">55</span></a><span class="sd">        &quot;&quot;&quot;</span>  
</span><span id="DenseLayer-56"><a href="#DenseLayer-56"><span class="linenos">56</span></a>        <span class="n">weightGradients</span><span class="p">,</span> <span class="n">biasGradients</span><span class="p">,</span> <span class="n">errorTerms</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">NeuralNetworkClass</span><span class="o">.</span><span class="n">_backPropgation</span><span class="p">(</span>
</span><span id="DenseLayer-57"><a href="#DenseLayer-57"><span class="linenos">57</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">layerNodes</span><span class="p">,</span> 
</span><span id="DenseLayer-58"><a href="#DenseLayer-58"><span class="linenos">58</span></a>            <span class="n">trueValues</span><span class="p">,</span>
</span><span id="DenseLayer-59"><a href="#DenseLayer-59"><span class="linenos">59</span></a>            <span class="kc">True</span>
</span><span id="DenseLayer-60"><a href="#DenseLayer-60"><span class="linenos">60</span></a>        <span class="p">)</span>
</span><span id="DenseLayer-61"><a href="#DenseLayer-61"><span class="linenos">61</span></a>        <span class="c1">#errorTerms = np.array(self.NeuralNetworkClass.weights).T @ errorTerms </span>
</span><span id="DenseLayer-62"><a href="#DenseLayer-62"><span class="linenos">62</span></a>        <span class="c1">#errorTerms = errorTerms.reshape(self.orignalShape)</span>
</span><span id="DenseLayer-63"><a href="#DenseLayer-63"><span class="linenos">63</span></a>
</span><span id="DenseLayer-64"><a href="#DenseLayer-64"><span class="linenos">64</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">NeuralNetworkClass</span><span class="o">.</span><span class="n">weights</span><span class="p">))):</span>
</span><span id="DenseLayer-65"><a href="#DenseLayer-65"><span class="linenos">65</span></a>            <span class="n">errorTerms</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">NeuralNetworkClass</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">@</span> <span class="n">errorTerms</span><span class="o">.</span><span class="n">T</span>
</span><span id="DenseLayer-66"><a href="#DenseLayer-66"><span class="linenos">66</span></a>            <span class="n">errorTerms</span> <span class="o">=</span> <span class="n">errorTerms</span><span class="o">.</span><span class="n">T</span>
</span><span id="DenseLayer-67"><a href="#DenseLayer-67"><span class="linenos">67</span></a>        <span class="n">errorTerms</span> <span class="o">=</span> <span class="n">errorTerms</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">orignalShape</span><span class="p">)</span>
</span><span id="DenseLayer-68"><a href="#DenseLayer-68"><span class="linenos">68</span></a>
</span><span id="DenseLayer-69"><a href="#DenseLayer-69"><span class="linenos">69</span></a>        <span class="k">return</span> <span class="n">weightGradients</span><span class="p">,</span> <span class="n">biasGradients</span><span class="p">,</span> <span class="n">errorTerms</span>
</span></pre></div>


    

                            <div id="DenseLayer.__init__" class="classattr">
                                        <input id="DenseLayer.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">DenseLayer</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">NeuralNetworkClass</span></span>)</span>

                <label class="view-source-button" for="DenseLayer.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#DenseLayer.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="DenseLayer.__init__-5"><a href="#DenseLayer.__init__-5"><span class="linenos"> 5</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">NeuralNetworkClass</span><span class="p">):</span>
</span><span id="DenseLayer.__init__-6"><a href="#DenseLayer.__init__-6"><span class="linenos"> 6</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="DenseLayer.__init__-7"><a href="#DenseLayer.__init__-7"><span class="linenos"> 7</span></a><span class="sd">        Initialises a dense layer using a NeuralNetworkClass.</span>
</span><span id="DenseLayer.__init__-8"><a href="#DenseLayer.__init__-8"><span class="linenos"> 8</span></a>
</span><span id="DenseLayer.__init__-9"><a href="#DenseLayer.__init__-9"><span class="linenos"> 9</span></a><span class="sd">        Args:</span>
</span><span id="DenseLayer.__init__-10"><a href="#DenseLayer.__init__-10"><span class="linenos">10</span></a><span class="sd">            NeuralNetworkClass (class): the fully connected neural network class.</span>
</span><span id="DenseLayer.__init__-11"><a href="#DenseLayer.__init__-11"><span class="linenos">11</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="DenseLayer.__init__-12"><a href="#DenseLayer.__init__-12"><span class="linenos">12</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">NeuralNetworkClass</span> <span class="o">=</span> <span class="n">NeuralNetworkClass</span>
</span><span id="DenseLayer.__init__-13"><a href="#DenseLayer.__init__-13"><span class="linenos">13</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">orignalShape</span> <span class="o">=</span> <span class="mi">0</span>   <span class="c1"># orignalShape is the original shape of the input tensor</span>
</span></pre></div>


            <div class="docstring"><p>Initialises a dense layer using a NeuralNetworkClass.</p>

<p>Args:
    NeuralNetworkClass (class): the fully connected neural network class.</p>
</div>


                            </div>
                            <div id="DenseLayer.NeuralNetworkClass" class="classattr">
                                <div class="attr variable">
            <span class="name">NeuralNetworkClass</span>

        
    </div>
    <a class="headerlink" href="#DenseLayer.NeuralNetworkClass"></a>
    
    

                            </div>
                            <div id="DenseLayer.orignalShape" class="classattr">
                                <div class="attr variable">
            <span class="name">orignalShape</span>

        
    </div>
    <a class="headerlink" href="#DenseLayer.orignalShape"></a>
    
    

                            </div>
                            <div id="DenseLayer.forward" class="classattr">
                                        <input id="DenseLayer.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">inputTensor</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="DenseLayer.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#DenseLayer.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="DenseLayer.forward-29"><a href="#DenseLayer.forward-29"><span class="linenos">29</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputTensor</span><span class="p">):</span>
</span><span id="DenseLayer.forward-30"><a href="#DenseLayer.forward-30"><span class="linenos">30</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="DenseLayer.forward-31"><a href="#DenseLayer.forward-31"><span class="linenos">31</span></a><span class="sd">        Flattens the input tensor and performs a forward pass.</span>
</span><span id="DenseLayer.forward-32"><a href="#DenseLayer.forward-32"><span class="linenos">32</span></a>
</span><span id="DenseLayer.forward-33"><a href="#DenseLayer.forward-33"><span class="linenos">33</span></a><span class="sd">        Args:</span>
</span><span id="DenseLayer.forward-34"><a href="#DenseLayer.forward-34"><span class="linenos">34</span></a><span class="sd">            inputTensor (ndarray): Input tensor to flatten and process.</span>
</span><span id="DenseLayer.forward-35"><a href="#DenseLayer.forward-35"><span class="linenos">35</span></a><span class="sd">        </span>
</span><span id="DenseLayer.forward-36"><a href="#DenseLayer.forward-36"><span class="linenos">36</span></a><span class="sd">        Returns:</span>
</span><span id="DenseLayer.forward-37"><a href="#DenseLayer.forward-37"><span class="linenos">37</span></a><span class="sd">            ndarray: Output of the dense layer.</span>
</span><span id="DenseLayer.forward-38"><a href="#DenseLayer.forward-38"><span class="linenos">38</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="DenseLayer.forward-39"><a href="#DenseLayer.forward-39"><span class="linenos">39</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">orignalShape</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputTensor</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</span><span id="DenseLayer.forward-40"><a href="#DenseLayer.forward-40"><span class="linenos">40</span></a>        <span class="n">inputArray</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flatternTensor</span><span class="p">(</span><span class="n">inputTensor</span><span class="p">)</span>
</span><span id="DenseLayer.forward-41"><a href="#DenseLayer.forward-41"><span class="linenos">41</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">layerNodes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">NeuralNetworkClass</span><span class="o">.</span><span class="n">forwardPropagation</span><span class="p">(</span><span class="n">inputArray</span><span class="p">)</span>
</span><span id="DenseLayer.forward-42"><a href="#DenseLayer.forward-42"><span class="linenos">42</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">layerNodes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span></pre></div>


            <div class="docstring"><p>Flattens the input tensor and performs a forward pass.</p>

<p>Args:
    inputTensor (ndarray): Input tensor to flatten and process.</p>

<p>Returns:
    ndarray: Output of the dense layer.</p>
</div>


                            </div>
                </section>
                <section id="PoolingLayer">
                            <input id="PoolingLayer-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">PoolingLayer</span>:

                <label class="view-source-button" for="PoolingLayer-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#PoolingLayer"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="PoolingLayer-4"><a href="#PoolingLayer-4"><span class="linenos">  4</span></a><span class="k">class</span><span class="w"> </span><span class="nc">PoolingLayer</span><span class="p">():</span>
</span><span id="PoolingLayer-5"><a href="#PoolingLayer-5"><span class="linenos">  5</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gridSize</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">mode</span> <span class="o">=</span> <span class="s2">&quot;max&quot;</span><span class="p">):</span>
</span><span id="PoolingLayer-6"><a href="#PoolingLayer-6"><span class="linenos">  6</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="PoolingLayer-7"><a href="#PoolingLayer-7"><span class="linenos">  7</span></a><span class="sd">        Initialises a pooling layer.</span>
</span><span id="PoolingLayer-8"><a href="#PoolingLayer-8"><span class="linenos">  8</span></a>
</span><span id="PoolingLayer-9"><a href="#PoolingLayer-9"><span class="linenos">  9</span></a><span class="sd">        Args:</span>
</span><span id="PoolingLayer-10"><a href="#PoolingLayer-10"><span class="linenos"> 10</span></a><span class="sd">            gridSize (int): The size of the pooling window.</span>
</span><span id="PoolingLayer-11"><a href="#PoolingLayer-11"><span class="linenos"> 11</span></a><span class="sd">            stride (int): The stride length for pooling.</span>
</span><span id="PoolingLayer-12"><a href="#PoolingLayer-12"><span class="linenos"> 12</span></a><span class="sd">            mode (str, optional): Pooling mode of &quot;max&quot;, &quot;ave&quot; (average). Default is &quot;max&quot;.        </span>
</span><span id="PoolingLayer-13"><a href="#PoolingLayer-13"><span class="linenos"> 13</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="PoolingLayer-14"><a href="#PoolingLayer-14"><span class="linenos"> 14</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">gridSize</span> <span class="o">=</span> <span class="n">gridSize</span>
</span><span id="PoolingLayer-15"><a href="#PoolingLayer-15"><span class="linenos"> 15</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>
</span><span id="PoolingLayer-16"><a href="#PoolingLayer-16"><span class="linenos"> 16</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
</span><span id="PoolingLayer-17"><a href="#PoolingLayer-17"><span class="linenos"> 17</span></a>    
</span><span id="PoolingLayer-18"><a href="#PoolingLayer-18"><span class="linenos"> 18</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputTensor</span><span class="p">):</span>
</span><span id="PoolingLayer-19"><a href="#PoolingLayer-19"><span class="linenos"> 19</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="PoolingLayer-20"><a href="#PoolingLayer-20"><span class="linenos"> 20</span></a><span class="sd">        Applies pooling (max or average) to reduce the size of the batch of inputs.</span>
</span><span id="PoolingLayer-21"><a href="#PoolingLayer-21"><span class="linenos"> 21</span></a>
</span><span id="PoolingLayer-22"><a href="#PoolingLayer-22"><span class="linenos"> 22</span></a><span class="sd">        Args:</span>
</span><span id="PoolingLayer-23"><a href="#PoolingLayer-23"><span class="linenos"> 23</span></a><span class="sd">            inputTensor (ndarray): A 4D or 3D array representing the images with shape (number of images, height, width).</span>
</span><span id="PoolingLayer-24"><a href="#PoolingLayer-24"><span class="linenos"> 24</span></a>
</span><span id="PoolingLayer-25"><a href="#PoolingLayer-25"><span class="linenos"> 25</span></a><span class="sd">        Returns:</span>
</span><span id="PoolingLayer-26"><a href="#PoolingLayer-26"><span class="linenos"> 26</span></a><span class="sd">            ndarray: A 3D array of feuture maps with reduced shape.</span>
</span><span id="PoolingLayer-27"><a href="#PoolingLayer-27"><span class="linenos"> 27</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="PoolingLayer-28"><a href="#PoolingLayer-28"><span class="linenos"> 28</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;max&quot;</span><span class="p">):</span>
</span><span id="PoolingLayer-29"><a href="#PoolingLayer-29"><span class="linenos"> 29</span></a>            <span class="n">poolFunc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span>
</span><span id="PoolingLayer-30"><a href="#PoolingLayer-30"><span class="linenos"> 30</span></a>        <span class="k">elif</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;ave&quot;</span><span class="p">):</span>
</span><span id="PoolingLayer-31"><a href="#PoolingLayer-31"><span class="linenos"> 31</span></a>            <span class="n">poolFunc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span>
</span><span id="PoolingLayer-32"><a href="#PoolingLayer-32"><span class="linenos"> 32</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="PoolingLayer-33"><a href="#PoolingLayer-33"><span class="linenos"> 33</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;pooling mode isnt correct: &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="si">}</span><span class="s2">&#39;, expected &#39;max&#39; or &#39;ave&#39;&quot;</span><span class="p">)</span>
</span><span id="PoolingLayer-34"><a href="#PoolingLayer-34"><span class="linenos"> 34</span></a>
</span><span id="PoolingLayer-35"><a href="#PoolingLayer-35"><span class="linenos"> 35</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">inputTensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">):</span>
</span><span id="PoolingLayer-36"><a href="#PoolingLayer-36"><span class="linenos"> 36</span></a>            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">inputTensor</span><span class="o">.</span><span class="n">shape</span>
</span><span id="PoolingLayer-37"><a href="#PoolingLayer-37"><span class="linenos"> 37</span></a>            <span class="n">outputHeight</span> <span class="o">=</span> <span class="p">(</span><span class="n">height</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">gridSize</span><span class="p">)</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="PoolingLayer-38"><a href="#PoolingLayer-38"><span class="linenos"> 38</span></a>            <span class="n">outputWidth</span> <span class="o">=</span> <span class="p">(</span><span class="n">width</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">gridSize</span><span class="p">)</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="PoolingLayer-39"><a href="#PoolingLayer-39"><span class="linenos"> 39</span></a>
</span><span id="PoolingLayer-40"><a href="#PoolingLayer-40"><span class="linenos"> 40</span></a>            <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">outputHeight</span><span class="p">,</span> <span class="n">outputWidth</span><span class="p">))</span>
</span><span id="PoolingLayer-41"><a href="#PoolingLayer-41"><span class="linenos"> 41</span></a>            
</span><span id="PoolingLayer-42"><a href="#PoolingLayer-42"><span class="linenos"> 42</span></a>            <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
</span><span id="PoolingLayer-43"><a href="#PoolingLayer-43"><span class="linenos"> 43</span></a>                <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">channels</span><span class="p">):</span>
</span><span id="PoolingLayer-44"><a href="#PoolingLayer-44"><span class="linenos"> 44</span></a>                    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">outputHeight</span><span class="p">):</span>
</span><span id="PoolingLayer-45"><a href="#PoolingLayer-45"><span class="linenos"> 45</span></a>                        <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">outputWidth</span><span class="p">):</span>
</span><span id="PoolingLayer-46"><a href="#PoolingLayer-46"><span class="linenos"> 46</span></a>                            <span class="n">startX</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
</span><span id="PoolingLayer-47"><a href="#PoolingLayer-47"><span class="linenos"> 47</span></a>                            <span class="n">startY</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
</span><span id="PoolingLayer-48"><a href="#PoolingLayer-48"><span class="linenos"> 48</span></a>                            <span class="n">window</span> <span class="o">=</span> <span class="n">inputTensor</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">startX</span><span class="p">:</span><span class="n">startX</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">gridSize</span><span class="p">,</span> <span class="n">startY</span><span class="p">:</span><span class="n">startY</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">gridSize</span><span class="p">]</span>
</span><span id="PoolingLayer-49"><a href="#PoolingLayer-49"><span class="linenos"> 49</span></a>                            <span class="n">output</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="n">poolFunc</span><span class="p">(</span><span class="n">window</span><span class="p">)</span>
</span><span id="PoolingLayer-50"><a href="#PoolingLayer-50"><span class="linenos"> 50</span></a>        
</span><span id="PoolingLayer-51"><a href="#PoolingLayer-51"><span class="linenos"> 51</span></a>        <span class="k">elif</span><span class="p">(</span><span class="n">inputTensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">):</span>
</span><span id="PoolingLayer-52"><a href="#PoolingLayer-52"><span class="linenos"> 52</span></a>            <span class="n">batch_size</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">inputTensor</span><span class="o">.</span><span class="n">shape</span>
</span><span id="PoolingLayer-53"><a href="#PoolingLayer-53"><span class="linenos"> 53</span></a>            <span class="n">outputHeight</span> <span class="o">=</span> <span class="p">(</span><span class="n">height</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">gridSize</span><span class="p">)</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="PoolingLayer-54"><a href="#PoolingLayer-54"><span class="linenos"> 54</span></a>            <span class="n">outputWidth</span> <span class="o">=</span> <span class="p">(</span><span class="n">width</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">gridSize</span><span class="p">)</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="PoolingLayer-55"><a href="#PoolingLayer-55"><span class="linenos"> 55</span></a>
</span><span id="PoolingLayer-56"><a href="#PoolingLayer-56"><span class="linenos"> 56</span></a>            <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">outputHeight</span><span class="p">,</span> <span class="n">outputWidth</span><span class="p">))</span>
</span><span id="PoolingLayer-57"><a href="#PoolingLayer-57"><span class="linenos"> 57</span></a>            
</span><span id="PoolingLayer-58"><a href="#PoolingLayer-58"><span class="linenos"> 58</span></a>            <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
</span><span id="PoolingLayer-59"><a href="#PoolingLayer-59"><span class="linenos"> 59</span></a>                <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">outputHeight</span><span class="p">):</span>
</span><span id="PoolingLayer-60"><a href="#PoolingLayer-60"><span class="linenos"> 60</span></a>                    <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">outputWidth</span><span class="p">):</span>
</span><span id="PoolingLayer-61"><a href="#PoolingLayer-61"><span class="linenos"> 61</span></a>                        <span class="n">startX</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
</span><span id="PoolingLayer-62"><a href="#PoolingLayer-62"><span class="linenos"> 62</span></a>                        <span class="n">startY</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
</span><span id="PoolingLayer-63"><a href="#PoolingLayer-63"><span class="linenos"> 63</span></a>                        <span class="n">window</span> <span class="o">=</span> <span class="n">inputTensor</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">startX</span><span class="p">:</span><span class="n">startX</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">gridSize</span><span class="p">,</span> <span class="n">startY</span><span class="p">:</span><span class="n">startY</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">gridSize</span><span class="p">]</span>
</span><span id="PoolingLayer-64"><a href="#PoolingLayer-64"><span class="linenos"> 64</span></a>                        <span class="n">output</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="n">poolFunc</span><span class="p">(</span><span class="n">window</span><span class="p">)</span>
</span><span id="PoolingLayer-65"><a href="#PoolingLayer-65"><span class="linenos"> 65</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="PoolingLayer-66"><a href="#PoolingLayer-66"><span class="linenos"> 66</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input tensor wrong dimension&quot;</span><span class="p">)</span>
</span><span id="PoolingLayer-67"><a href="#PoolingLayer-67"><span class="linenos"> 67</span></a>
</span><span id="PoolingLayer-68"><a href="#PoolingLayer-68"><span class="linenos"> 68</span></a>        <span class="k">return</span> <span class="n">output</span>
</span><span id="PoolingLayer-69"><a href="#PoolingLayer-69"><span class="linenos"> 69</span></a>
</span><span id="PoolingLayer-70"><a href="#PoolingLayer-70"><span class="linenos"> 70</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_backpropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">errorPatch</span><span class="p">,</span> <span class="n">inputTensor</span><span class="p">):</span>
</span><span id="PoolingLayer-71"><a href="#PoolingLayer-71"><span class="linenos"> 71</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="PoolingLayer-72"><a href="#PoolingLayer-72"><span class="linenos"> 72</span></a><span class="sd">        Performs backpropagation through the pooling layer.</span>
</span><span id="PoolingLayer-73"><a href="#PoolingLayer-73"><span class="linenos"> 73</span></a>
</span><span id="PoolingLayer-74"><a href="#PoolingLayer-74"><span class="linenos"> 74</span></a><span class="sd">        Args:</span>
</span><span id="PoolingLayer-75"><a href="#PoolingLayer-75"><span class="linenos"> 75</span></a><span class="sd">            errorPatch (ndarray): Error gradient from the next layer.</span>
</span><span id="PoolingLayer-76"><a href="#PoolingLayer-76"><span class="linenos"> 76</span></a><span class="sd">            inputTensor (ndarray): Input tensor during forward propagation.</span>
</span><span id="PoolingLayer-77"><a href="#PoolingLayer-77"><span class="linenos"> 77</span></a><span class="sd">        </span>
</span><span id="PoolingLayer-78"><a href="#PoolingLayer-78"><span class="linenos"> 78</span></a><span class="sd">        Returns:</span>
</span><span id="PoolingLayer-79"><a href="#PoolingLayer-79"><span class="linenos"> 79</span></a><span class="sd">            inputGradient (ndarray): Gradient of the loss.</span>
</span><span id="PoolingLayer-80"><a href="#PoolingLayer-80"><span class="linenos"> 80</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="PoolingLayer-81"><a href="#PoolingLayer-81"><span class="linenos"> 81</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;max&quot;</span><span class="p">):</span>
</span><span id="PoolingLayer-82"><a href="#PoolingLayer-82"><span class="linenos"> 82</span></a>            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MaxPoolingDerivative</span><span class="p">(</span><span class="n">errorPatch</span><span class="p">,</span> <span class="n">inputTensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">gridSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
</span><span id="PoolingLayer-83"><a href="#PoolingLayer-83"><span class="linenos"> 83</span></a>        <span class="k">elif</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;ave&quot;</span><span class="p">):</span>
</span><span id="PoolingLayer-84"><a href="#PoolingLayer-84"><span class="linenos"> 84</span></a>            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AveragePoolingDerivative</span><span class="p">(</span><span class="n">errorPatch</span><span class="p">,</span> <span class="n">inputTensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">gridSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
</span><span id="PoolingLayer-85"><a href="#PoolingLayer-85"><span class="linenos"> 85</span></a>
</span><span id="PoolingLayer-86"><a href="#PoolingLayer-86"><span class="linenos"> 86</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_MaxPoolingDerivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">errorPatch</span><span class="p">,</span> <span class="n">inputTensor</span><span class="p">,</span> <span class="n">sizeOfGrid</span><span class="p">,</span> <span class="n">strideLength</span><span class="p">):</span>
</span><span id="PoolingLayer-87"><a href="#PoolingLayer-87"><span class="linenos"> 87</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="PoolingLayer-88"><a href="#PoolingLayer-88"><span class="linenos"> 88</span></a><span class="sd">        Compute the gradient of the loss with respect to the input of the max pooling layer during backpropagation.</span>
</span><span id="PoolingLayer-89"><a href="#PoolingLayer-89"><span class="linenos"> 89</span></a>
</span><span id="PoolingLayer-90"><a href="#PoolingLayer-90"><span class="linenos"> 90</span></a><span class="sd">        Args:</span>
</span><span id="PoolingLayer-91"><a href="#PoolingLayer-91"><span class="linenos"> 91</span></a><span class="sd">            errorPatch (ndarray): Error gradient from the next layer.</span>
</span><span id="PoolingLayer-92"><a href="#PoolingLayer-92"><span class="linenos"> 92</span></a><span class="sd">            inputTensor (ndarray): Input to the max pooling layer during forward propagation.</span>
</span><span id="PoolingLayer-93"><a href="#PoolingLayer-93"><span class="linenos"> 93</span></a><span class="sd">            sizeOfGrid (int): Size of the pooling window.</span>
</span><span id="PoolingLayer-94"><a href="#PoolingLayer-94"><span class="linenos"> 94</span></a><span class="sd">            strideLength (int): Stride length used during pooling.</span>
</span><span id="PoolingLayer-95"><a href="#PoolingLayer-95"><span class="linenos"> 95</span></a><span class="sd">        </span>
</span><span id="PoolingLayer-96"><a href="#PoolingLayer-96"><span class="linenos"> 96</span></a><span class="sd">        Returns:</span>
</span><span id="PoolingLayer-97"><a href="#PoolingLayer-97"><span class="linenos"> 97</span></a><span class="sd">            inputGradient (ndarray): Gradient of the loss with respect to the inputTensor</span>
</span><span id="PoolingLayer-98"><a href="#PoolingLayer-98"><span class="linenos"> 98</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="PoolingLayer-99"><a href="#PoolingLayer-99"><span class="linenos"> 99</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">inputTensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">):</span>
</span><span id="PoolingLayer-100"><a href="#PoolingLayer-100"><span class="linenos">100</span></a>            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">inputTensor</span><span class="o">.</span><span class="n">shape</span>
</span><span id="PoolingLayer-101"><a href="#PoolingLayer-101"><span class="linenos">101</span></a>            <span class="n">sizeOfGrid</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gridSize</span>
</span><span id="PoolingLayer-102"><a href="#PoolingLayer-102"><span class="linenos">102</span></a>            <span class="n">strideLength</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
</span><span id="PoolingLayer-103"><a href="#PoolingLayer-103"><span class="linenos">103</span></a>            
</span><span id="PoolingLayer-104"><a href="#PoolingLayer-104"><span class="linenos">104</span></a>            <span class="n">inputGradient</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">inputTensor</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
</span><span id="PoolingLayer-105"><a href="#PoolingLayer-105"><span class="linenos">105</span></a>            <span class="n">outputHeight</span> <span class="o">=</span> <span class="p">(</span><span class="n">height</span> <span class="o">-</span> <span class="n">sizeOfGrid</span><span class="p">)</span> <span class="o">//</span> <span class="n">strideLength</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="PoolingLayer-106"><a href="#PoolingLayer-106"><span class="linenos">106</span></a>            <span class="n">outputWidth</span> <span class="o">=</span> <span class="p">(</span><span class="n">width</span> <span class="o">-</span> <span class="n">sizeOfGrid</span><span class="p">)</span> <span class="o">//</span> <span class="n">strideLength</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="PoolingLayer-107"><a href="#PoolingLayer-107"><span class="linenos">107</span></a>            
</span><span id="PoolingLayer-108"><a href="#PoolingLayer-108"><span class="linenos">108</span></a>            <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
</span><span id="PoolingLayer-109"><a href="#PoolingLayer-109"><span class="linenos">109</span></a>                <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">channels</span><span class="p">):</span>
</span><span id="PoolingLayer-110"><a href="#PoolingLayer-110"><span class="linenos">110</span></a>                    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">outputHeight</span><span class="p">):</span>
</span><span id="PoolingLayer-111"><a href="#PoolingLayer-111"><span class="linenos">111</span></a>                        <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">outputWidth</span><span class="p">):</span>
</span><span id="PoolingLayer-112"><a href="#PoolingLayer-112"><span class="linenos">112</span></a>                            <span class="n">startX</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">strideLength</span>
</span><span id="PoolingLayer-113"><a href="#PoolingLayer-113"><span class="linenos">113</span></a>                            <span class="n">startY</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">strideLength</span>
</span><span id="PoolingLayer-114"><a href="#PoolingLayer-114"><span class="linenos">114</span></a>                            <span class="n">window</span> <span class="o">=</span> <span class="n">inputTensor</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">startX</span><span class="p">:</span><span class="n">startX</span><span class="o">+</span><span class="n">sizeOfGrid</span><span class="p">,</span> <span class="n">startY</span><span class="p">:</span><span class="n">startY</span><span class="o">+</span><span class="n">sizeOfGrid</span><span class="p">]</span>
</span><span id="PoolingLayer-115"><a href="#PoolingLayer-115"><span class="linenos">115</span></a>                            <span class="n">maxIndex</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">window</span><span class="p">)</span>
</span><span id="PoolingLayer-116"><a href="#PoolingLayer-116"><span class="linenos">116</span></a>                            <span class="n">maxX</span><span class="p">,</span> <span class="n">maxY</span> <span class="o">=</span> <span class="nb">divmod</span><span class="p">(</span><span class="n">maxIndex</span><span class="p">,</span> <span class="n">sizeOfGrid</span><span class="p">)</span>
</span><span id="PoolingLayer-117"><a href="#PoolingLayer-117"><span class="linenos">117</span></a>                            <span class="n">inputGradient</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">startX</span><span class="o">+</span><span class="n">maxX</span><span class="p">,</span> <span class="n">startY</span><span class="o">+</span><span class="n">maxY</span><span class="p">]</span> <span class="o">+=</span> <span class="n">errorPatch</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span>
</span><span id="PoolingLayer-118"><a href="#PoolingLayer-118"><span class="linenos">118</span></a>        
</span><span id="PoolingLayer-119"><a href="#PoolingLayer-119"><span class="linenos">119</span></a>        <span class="k">elif</span><span class="p">(</span><span class="n">inputTensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">):</span>
</span><span id="PoolingLayer-120"><a href="#PoolingLayer-120"><span class="linenos">120</span></a>            <span class="n">batch_size</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">inputTensor</span><span class="o">.</span><span class="n">shape</span>
</span><span id="PoolingLayer-121"><a href="#PoolingLayer-121"><span class="linenos">121</span></a>            <span class="n">sizeOfGrid</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gridSize</span>
</span><span id="PoolingLayer-122"><a href="#PoolingLayer-122"><span class="linenos">122</span></a>            <span class="n">strideLength</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
</span><span id="PoolingLayer-123"><a href="#PoolingLayer-123"><span class="linenos">123</span></a>            
</span><span id="PoolingLayer-124"><a href="#PoolingLayer-124"><span class="linenos">124</span></a>            <span class="n">inputGradient</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">inputTensor</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
</span><span id="PoolingLayer-125"><a href="#PoolingLayer-125"><span class="linenos">125</span></a>            <span class="n">outputHeight</span> <span class="o">=</span> <span class="p">(</span><span class="n">height</span> <span class="o">-</span> <span class="n">sizeOfGrid</span><span class="p">)</span> <span class="o">//</span> <span class="n">strideLength</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="PoolingLayer-126"><a href="#PoolingLayer-126"><span class="linenos">126</span></a>            <span class="n">outputWidth</span> <span class="o">=</span> <span class="p">(</span><span class="n">width</span> <span class="o">-</span> <span class="n">sizeOfGrid</span><span class="p">)</span> <span class="o">//</span> <span class="n">strideLength</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="PoolingLayer-127"><a href="#PoolingLayer-127"><span class="linenos">127</span></a>            
</span><span id="PoolingLayer-128"><a href="#PoolingLayer-128"><span class="linenos">128</span></a>            <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
</span><span id="PoolingLayer-129"><a href="#PoolingLayer-129"><span class="linenos">129</span></a>                <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">outputHeight</span><span class="p">):</span>
</span><span id="PoolingLayer-130"><a href="#PoolingLayer-130"><span class="linenos">130</span></a>                    <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">outputWidth</span><span class="p">):</span>
</span><span id="PoolingLayer-131"><a href="#PoolingLayer-131"><span class="linenos">131</span></a>                        <span class="n">startX</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">strideLength</span>
</span><span id="PoolingLayer-132"><a href="#PoolingLayer-132"><span class="linenos">132</span></a>                        <span class="n">startY</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">strideLength</span>
</span><span id="PoolingLayer-133"><a href="#PoolingLayer-133"><span class="linenos">133</span></a>                        <span class="n">window</span> <span class="o">=</span> <span class="n">inputTensor</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">startX</span><span class="p">:</span><span class="n">startX</span><span class="o">+</span><span class="n">sizeOfGrid</span><span class="p">,</span> <span class="n">startY</span><span class="p">:</span><span class="n">startY</span><span class="o">+</span><span class="n">sizeOfGrid</span><span class="p">]</span>
</span><span id="PoolingLayer-134"><a href="#PoolingLayer-134"><span class="linenos">134</span></a>                        <span class="n">maxIndex</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">window</span><span class="p">)</span>
</span><span id="PoolingLayer-135"><a href="#PoolingLayer-135"><span class="linenos">135</span></a>                        <span class="n">maxX</span><span class="p">,</span> <span class="n">maxY</span> <span class="o">=</span> <span class="nb">divmod</span><span class="p">(</span><span class="n">maxIndex</span><span class="p">,</span> <span class="n">sizeOfGrid</span><span class="p">)</span>
</span><span id="PoolingLayer-136"><a href="#PoolingLayer-136"><span class="linenos">136</span></a>                        <span class="n">inputGradient</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">startX</span><span class="o">+</span><span class="n">maxX</span><span class="p">,</span> <span class="n">startY</span><span class="o">+</span><span class="n">maxY</span><span class="p">]</span> <span class="o">+=</span> <span class="n">errorPatch</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span>
</span><span id="PoolingLayer-137"><a href="#PoolingLayer-137"><span class="linenos">137</span></a>        <span class="k">else</span><span class="p">:</span> 
</span><span id="PoolingLayer-138"><a href="#PoolingLayer-138"><span class="linenos">138</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input tensor wrong dimension&quot;</span><span class="p">)</span>
</span><span id="PoolingLayer-139"><a href="#PoolingLayer-139"><span class="linenos">139</span></a>
</span><span id="PoolingLayer-140"><a href="#PoolingLayer-140"><span class="linenos">140</span></a>        <span class="k">return</span> <span class="n">inputGradient</span>
</span><span id="PoolingLayer-141"><a href="#PoolingLayer-141"><span class="linenos">141</span></a>
</span><span id="PoolingLayer-142"><a href="#PoolingLayer-142"><span class="linenos">142</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_AveragePoolingDerivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">errorPatch</span><span class="p">,</span> <span class="n">inputTensor</span><span class="p">,</span> <span class="n">sizeOfGrid</span><span class="p">,</span> <span class="n">strideLength</span><span class="p">):</span>
</span><span id="PoolingLayer-143"><a href="#PoolingLayer-143"><span class="linenos">143</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="PoolingLayer-144"><a href="#PoolingLayer-144"><span class="linenos">144</span></a><span class="sd">        Compute the gradient of the loss with respect to the input of the average pooling layer during backpropagation.</span>
</span><span id="PoolingLayer-145"><a href="#PoolingLayer-145"><span class="linenos">145</span></a>
</span><span id="PoolingLayer-146"><a href="#PoolingLayer-146"><span class="linenos">146</span></a><span class="sd">        Args:</span>
</span><span id="PoolingLayer-147"><a href="#PoolingLayer-147"><span class="linenos">147</span></a><span class="sd">            errorPatch (ndarray): Error gradient from the next layer.</span>
</span><span id="PoolingLayer-148"><a href="#PoolingLayer-148"><span class="linenos">148</span></a><span class="sd">            inputTensor (ndarray): Input to the average pooling layer during forward propagation.</span>
</span><span id="PoolingLayer-149"><a href="#PoolingLayer-149"><span class="linenos">149</span></a><span class="sd">            sizeOfGrid (int): Size of the pooling window.</span>
</span><span id="PoolingLayer-150"><a href="#PoolingLayer-150"><span class="linenos">150</span></a><span class="sd">            strideLength (int): Stride length used during pooling.</span>
</span><span id="PoolingLayer-151"><a href="#PoolingLayer-151"><span class="linenos">151</span></a><span class="sd">        </span>
</span><span id="PoolingLayer-152"><a href="#PoolingLayer-152"><span class="linenos">152</span></a><span class="sd">        Returns:</span>
</span><span id="PoolingLayer-153"><a href="#PoolingLayer-153"><span class="linenos">153</span></a><span class="sd">            inputGradient (ndarray): Gradient of the loss with respect to the inputTensor</span>
</span><span id="PoolingLayer-154"><a href="#PoolingLayer-154"><span class="linenos">154</span></a><span class="sd">        &quot;&quot;&quot;</span>       
</span><span id="PoolingLayer-155"><a href="#PoolingLayer-155"><span class="linenos">155</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">inputTensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">):</span>
</span><span id="PoolingLayer-156"><a href="#PoolingLayer-156"><span class="linenos">156</span></a>            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">inputTensor</span><span class="o">.</span><span class="n">shape</span>
</span><span id="PoolingLayer-157"><a href="#PoolingLayer-157"><span class="linenos">157</span></a>            <span class="n">sizeOfGrid</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gridSize</span>
</span><span id="PoolingLayer-158"><a href="#PoolingLayer-158"><span class="linenos">158</span></a>            <span class="n">strideLength</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
</span><span id="PoolingLayer-159"><a href="#PoolingLayer-159"><span class="linenos">159</span></a>            
</span><span id="PoolingLayer-160"><a href="#PoolingLayer-160"><span class="linenos">160</span></a>            <span class="n">inputGradient</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">inputTensor</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span><span id="PoolingLayer-161"><a href="#PoolingLayer-161"><span class="linenos">161</span></a>            <span class="n">outputHeight</span> <span class="o">=</span> <span class="p">(</span><span class="n">height</span> <span class="o">-</span> <span class="n">sizeOfGrid</span><span class="p">)</span> <span class="o">//</span> <span class="n">strideLength</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="PoolingLayer-162"><a href="#PoolingLayer-162"><span class="linenos">162</span></a>            <span class="n">outputWidth</span> <span class="o">=</span> <span class="p">(</span><span class="n">width</span> <span class="o">-</span> <span class="n">sizeOfGrid</span><span class="p">)</span> <span class="o">//</span> <span class="n">strideLength</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="PoolingLayer-163"><a href="#PoolingLayer-163"><span class="linenos">163</span></a>            <span class="n">avgMultiplier</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">sizeOfGrid</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="PoolingLayer-164"><a href="#PoolingLayer-164"><span class="linenos">164</span></a>            
</span><span id="PoolingLayer-165"><a href="#PoolingLayer-165"><span class="linenos">165</span></a>            <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
</span><span id="PoolingLayer-166"><a href="#PoolingLayer-166"><span class="linenos">166</span></a>                <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">channels</span><span class="p">):</span>
</span><span id="PoolingLayer-167"><a href="#PoolingLayer-167"><span class="linenos">167</span></a>                    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">outputHeight</span><span class="p">):</span>
</span><span id="PoolingLayer-168"><a href="#PoolingLayer-168"><span class="linenos">168</span></a>                        <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">outputWidth</span><span class="p">):</span>
</span><span id="PoolingLayer-169"><a href="#PoolingLayer-169"><span class="linenos">169</span></a>                            <span class="n">startX</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">strideLength</span>
</span><span id="PoolingLayer-170"><a href="#PoolingLayer-170"><span class="linenos">170</span></a>                            <span class="n">startY</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">strideLength</span>
</span><span id="PoolingLayer-171"><a href="#PoolingLayer-171"><span class="linenos">171</span></a>                            <span class="n">inputGradient</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">startX</span><span class="p">:</span><span class="n">startX</span><span class="o">+</span><span class="n">sizeOfGrid</span><span class="p">,</span> <span class="n">startY</span><span class="p">:</span><span class="n">startY</span><span class="o">+</span><span class="n">sizeOfGrid</span><span class="p">]</span> <span class="o">+=</span> <span class="n">errorPatch</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span> <span class="o">*</span> <span class="n">avgMultiplier</span>
</span><span id="PoolingLayer-172"><a href="#PoolingLayer-172"><span class="linenos">172</span></a>        
</span><span id="PoolingLayer-173"><a href="#PoolingLayer-173"><span class="linenos">173</span></a>        <span class="k">elif</span><span class="p">(</span><span class="n">inputTensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">):</span>
</span><span id="PoolingLayer-174"><a href="#PoolingLayer-174"><span class="linenos">174</span></a>            <span class="n">batch_size</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">inputTensor</span><span class="o">.</span><span class="n">shape</span>
</span><span id="PoolingLayer-175"><a href="#PoolingLayer-175"><span class="linenos">175</span></a>            <span class="n">sizeOfGrid</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gridSize</span>
</span><span id="PoolingLayer-176"><a href="#PoolingLayer-176"><span class="linenos">176</span></a>            <span class="n">strideLength</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
</span><span id="PoolingLayer-177"><a href="#PoolingLayer-177"><span class="linenos">177</span></a>            
</span><span id="PoolingLayer-178"><a href="#PoolingLayer-178"><span class="linenos">178</span></a>            <span class="n">inputGradient</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">inputTensor</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span><span id="PoolingLayer-179"><a href="#PoolingLayer-179"><span class="linenos">179</span></a>            <span class="n">outputHeight</span> <span class="o">=</span> <span class="p">(</span><span class="n">height</span> <span class="o">-</span> <span class="n">sizeOfGrid</span><span class="p">)</span> <span class="o">//</span> <span class="n">strideLength</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="PoolingLayer-180"><a href="#PoolingLayer-180"><span class="linenos">180</span></a>            <span class="n">outputWidth</span> <span class="o">=</span> <span class="p">(</span><span class="n">width</span> <span class="o">-</span> <span class="n">sizeOfGrid</span><span class="p">)</span> <span class="o">//</span> <span class="n">strideLength</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="PoolingLayer-181"><a href="#PoolingLayer-181"><span class="linenos">181</span></a>            <span class="n">avgMultiplier</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">sizeOfGrid</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="PoolingLayer-182"><a href="#PoolingLayer-182"><span class="linenos">182</span></a>            
</span><span id="PoolingLayer-183"><a href="#PoolingLayer-183"><span class="linenos">183</span></a>            <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
</span><span id="PoolingLayer-184"><a href="#PoolingLayer-184"><span class="linenos">184</span></a>                <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">outputHeight</span><span class="p">):</span>
</span><span id="PoolingLayer-185"><a href="#PoolingLayer-185"><span class="linenos">185</span></a>                    <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">outputWidth</span><span class="p">):</span>
</span><span id="PoolingLayer-186"><a href="#PoolingLayer-186"><span class="linenos">186</span></a>                        <span class="n">startX</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">strideLength</span>
</span><span id="PoolingLayer-187"><a href="#PoolingLayer-187"><span class="linenos">187</span></a>                        <span class="n">startY</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">strideLength</span>
</span><span id="PoolingLayer-188"><a href="#PoolingLayer-188"><span class="linenos">188</span></a>                        <span class="n">inputGradient</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">startX</span><span class="p">:</span><span class="n">startX</span><span class="o">+</span><span class="n">sizeOfGrid</span><span class="p">,</span> <span class="n">startY</span><span class="p">:</span><span class="n">startY</span><span class="o">+</span><span class="n">sizeOfGrid</span><span class="p">]</span> <span class="o">+=</span> <span class="n">errorPatch</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span> <span class="o">*</span> <span class="n">avgMultiplier</span>
</span><span id="PoolingLayer-189"><a href="#PoolingLayer-189"><span class="linenos">189</span></a>        <span class="k">else</span><span class="p">:</span> 
</span><span id="PoolingLayer-190"><a href="#PoolingLayer-190"><span class="linenos">190</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input tensor wrong dimension&quot;</span><span class="p">)</span>
</span><span id="PoolingLayer-191"><a href="#PoolingLayer-191"><span class="linenos">191</span></a>        
</span><span id="PoolingLayer-192"><a href="#PoolingLayer-192"><span class="linenos">192</span></a>        <span class="k">return</span> <span class="n">inputGradient</span>
</span></pre></div>


    

                            <div id="PoolingLayer.__init__" class="classattr">
                                        <input id="PoolingLayer.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">PoolingLayer</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">gridSize</span>, </span><span class="param"><span class="n">stride</span>, </span><span class="param"><span class="n">mode</span><span class="o">=</span><span class="s1">&#39;max&#39;</span></span>)</span>

                <label class="view-source-button" for="PoolingLayer.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#PoolingLayer.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="PoolingLayer.__init__-5"><a href="#PoolingLayer.__init__-5"><span class="linenos"> 5</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gridSize</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">mode</span> <span class="o">=</span> <span class="s2">&quot;max&quot;</span><span class="p">):</span>
</span><span id="PoolingLayer.__init__-6"><a href="#PoolingLayer.__init__-6"><span class="linenos"> 6</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="PoolingLayer.__init__-7"><a href="#PoolingLayer.__init__-7"><span class="linenos"> 7</span></a><span class="sd">        Initialises a pooling layer.</span>
</span><span id="PoolingLayer.__init__-8"><a href="#PoolingLayer.__init__-8"><span class="linenos"> 8</span></a>
</span><span id="PoolingLayer.__init__-9"><a href="#PoolingLayer.__init__-9"><span class="linenos"> 9</span></a><span class="sd">        Args:</span>
</span><span id="PoolingLayer.__init__-10"><a href="#PoolingLayer.__init__-10"><span class="linenos">10</span></a><span class="sd">            gridSize (int): The size of the pooling window.</span>
</span><span id="PoolingLayer.__init__-11"><a href="#PoolingLayer.__init__-11"><span class="linenos">11</span></a><span class="sd">            stride (int): The stride length for pooling.</span>
</span><span id="PoolingLayer.__init__-12"><a href="#PoolingLayer.__init__-12"><span class="linenos">12</span></a><span class="sd">            mode (str, optional): Pooling mode of &quot;max&quot;, &quot;ave&quot; (average). Default is &quot;max&quot;.        </span>
</span><span id="PoolingLayer.__init__-13"><a href="#PoolingLayer.__init__-13"><span class="linenos">13</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="PoolingLayer.__init__-14"><a href="#PoolingLayer.__init__-14"><span class="linenos">14</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">gridSize</span> <span class="o">=</span> <span class="n">gridSize</span>
</span><span id="PoolingLayer.__init__-15"><a href="#PoolingLayer.__init__-15"><span class="linenos">15</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>
</span><span id="PoolingLayer.__init__-16"><a href="#PoolingLayer.__init__-16"><span class="linenos">16</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
</span></pre></div>


            <div class="docstring"><p>Initialises a pooling layer.</p>

<p>Args:
    gridSize (int): The size of the pooling window.
    stride (int): The stride length for pooling.
    mode (str, optional): Pooling mode of "max", "ave" (average). Default is "max".</p>
</div>


                            </div>
                            <div id="PoolingLayer.gridSize" class="classattr">
                                <div class="attr variable">
            <span class="name">gridSize</span>

        
    </div>
    <a class="headerlink" href="#PoolingLayer.gridSize"></a>
    
    

                            </div>
                            <div id="PoolingLayer.stride" class="classattr">
                                <div class="attr variable">
            <span class="name">stride</span>

        
    </div>
    <a class="headerlink" href="#PoolingLayer.stride"></a>
    
    

                            </div>
                            <div id="PoolingLayer.mode" class="classattr">
                                <div class="attr variable">
            <span class="name">mode</span>

        
    </div>
    <a class="headerlink" href="#PoolingLayer.mode"></a>
    
    

                            </div>
                            <div id="PoolingLayer.forward" class="classattr">
                                        <input id="PoolingLayer.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">inputTensor</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="PoolingLayer.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#PoolingLayer.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="PoolingLayer.forward-18"><a href="#PoolingLayer.forward-18"><span class="linenos">18</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputTensor</span><span class="p">):</span>
</span><span id="PoolingLayer.forward-19"><a href="#PoolingLayer.forward-19"><span class="linenos">19</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="PoolingLayer.forward-20"><a href="#PoolingLayer.forward-20"><span class="linenos">20</span></a><span class="sd">        Applies pooling (max or average) to reduce the size of the batch of inputs.</span>
</span><span id="PoolingLayer.forward-21"><a href="#PoolingLayer.forward-21"><span class="linenos">21</span></a>
</span><span id="PoolingLayer.forward-22"><a href="#PoolingLayer.forward-22"><span class="linenos">22</span></a><span class="sd">        Args:</span>
</span><span id="PoolingLayer.forward-23"><a href="#PoolingLayer.forward-23"><span class="linenos">23</span></a><span class="sd">            inputTensor (ndarray): A 4D or 3D array representing the images with shape (number of images, height, width).</span>
</span><span id="PoolingLayer.forward-24"><a href="#PoolingLayer.forward-24"><span class="linenos">24</span></a>
</span><span id="PoolingLayer.forward-25"><a href="#PoolingLayer.forward-25"><span class="linenos">25</span></a><span class="sd">        Returns:</span>
</span><span id="PoolingLayer.forward-26"><a href="#PoolingLayer.forward-26"><span class="linenos">26</span></a><span class="sd">            ndarray: A 3D array of feuture maps with reduced shape.</span>
</span><span id="PoolingLayer.forward-27"><a href="#PoolingLayer.forward-27"><span class="linenos">27</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="PoolingLayer.forward-28"><a href="#PoolingLayer.forward-28"><span class="linenos">28</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;max&quot;</span><span class="p">):</span>
</span><span id="PoolingLayer.forward-29"><a href="#PoolingLayer.forward-29"><span class="linenos">29</span></a>            <span class="n">poolFunc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span>
</span><span id="PoolingLayer.forward-30"><a href="#PoolingLayer.forward-30"><span class="linenos">30</span></a>        <span class="k">elif</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;ave&quot;</span><span class="p">):</span>
</span><span id="PoolingLayer.forward-31"><a href="#PoolingLayer.forward-31"><span class="linenos">31</span></a>            <span class="n">poolFunc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span>
</span><span id="PoolingLayer.forward-32"><a href="#PoolingLayer.forward-32"><span class="linenos">32</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="PoolingLayer.forward-33"><a href="#PoolingLayer.forward-33"><span class="linenos">33</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;pooling mode isnt correct: &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="si">}</span><span class="s2">&#39;, expected &#39;max&#39; or &#39;ave&#39;&quot;</span><span class="p">)</span>
</span><span id="PoolingLayer.forward-34"><a href="#PoolingLayer.forward-34"><span class="linenos">34</span></a>
</span><span id="PoolingLayer.forward-35"><a href="#PoolingLayer.forward-35"><span class="linenos">35</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">inputTensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">):</span>
</span><span id="PoolingLayer.forward-36"><a href="#PoolingLayer.forward-36"><span class="linenos">36</span></a>            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">inputTensor</span><span class="o">.</span><span class="n">shape</span>
</span><span id="PoolingLayer.forward-37"><a href="#PoolingLayer.forward-37"><span class="linenos">37</span></a>            <span class="n">outputHeight</span> <span class="o">=</span> <span class="p">(</span><span class="n">height</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">gridSize</span><span class="p">)</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="PoolingLayer.forward-38"><a href="#PoolingLayer.forward-38"><span class="linenos">38</span></a>            <span class="n">outputWidth</span> <span class="o">=</span> <span class="p">(</span><span class="n">width</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">gridSize</span><span class="p">)</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="PoolingLayer.forward-39"><a href="#PoolingLayer.forward-39"><span class="linenos">39</span></a>
</span><span id="PoolingLayer.forward-40"><a href="#PoolingLayer.forward-40"><span class="linenos">40</span></a>            <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">outputHeight</span><span class="p">,</span> <span class="n">outputWidth</span><span class="p">))</span>
</span><span id="PoolingLayer.forward-41"><a href="#PoolingLayer.forward-41"><span class="linenos">41</span></a>            
</span><span id="PoolingLayer.forward-42"><a href="#PoolingLayer.forward-42"><span class="linenos">42</span></a>            <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
</span><span id="PoolingLayer.forward-43"><a href="#PoolingLayer.forward-43"><span class="linenos">43</span></a>                <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">channels</span><span class="p">):</span>
</span><span id="PoolingLayer.forward-44"><a href="#PoolingLayer.forward-44"><span class="linenos">44</span></a>                    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">outputHeight</span><span class="p">):</span>
</span><span id="PoolingLayer.forward-45"><a href="#PoolingLayer.forward-45"><span class="linenos">45</span></a>                        <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">outputWidth</span><span class="p">):</span>
</span><span id="PoolingLayer.forward-46"><a href="#PoolingLayer.forward-46"><span class="linenos">46</span></a>                            <span class="n">startX</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
</span><span id="PoolingLayer.forward-47"><a href="#PoolingLayer.forward-47"><span class="linenos">47</span></a>                            <span class="n">startY</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
</span><span id="PoolingLayer.forward-48"><a href="#PoolingLayer.forward-48"><span class="linenos">48</span></a>                            <span class="n">window</span> <span class="o">=</span> <span class="n">inputTensor</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">startX</span><span class="p">:</span><span class="n">startX</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">gridSize</span><span class="p">,</span> <span class="n">startY</span><span class="p">:</span><span class="n">startY</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">gridSize</span><span class="p">]</span>
</span><span id="PoolingLayer.forward-49"><a href="#PoolingLayer.forward-49"><span class="linenos">49</span></a>                            <span class="n">output</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="n">poolFunc</span><span class="p">(</span><span class="n">window</span><span class="p">)</span>
</span><span id="PoolingLayer.forward-50"><a href="#PoolingLayer.forward-50"><span class="linenos">50</span></a>        
</span><span id="PoolingLayer.forward-51"><a href="#PoolingLayer.forward-51"><span class="linenos">51</span></a>        <span class="k">elif</span><span class="p">(</span><span class="n">inputTensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">):</span>
</span><span id="PoolingLayer.forward-52"><a href="#PoolingLayer.forward-52"><span class="linenos">52</span></a>            <span class="n">batch_size</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">inputTensor</span><span class="o">.</span><span class="n">shape</span>
</span><span id="PoolingLayer.forward-53"><a href="#PoolingLayer.forward-53"><span class="linenos">53</span></a>            <span class="n">outputHeight</span> <span class="o">=</span> <span class="p">(</span><span class="n">height</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">gridSize</span><span class="p">)</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="PoolingLayer.forward-54"><a href="#PoolingLayer.forward-54"><span class="linenos">54</span></a>            <span class="n">outputWidth</span> <span class="o">=</span> <span class="p">(</span><span class="n">width</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">gridSize</span><span class="p">)</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">+</span> <span class="mi">1</span>
</span><span id="PoolingLayer.forward-55"><a href="#PoolingLayer.forward-55"><span class="linenos">55</span></a>
</span><span id="PoolingLayer.forward-56"><a href="#PoolingLayer.forward-56"><span class="linenos">56</span></a>            <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">outputHeight</span><span class="p">,</span> <span class="n">outputWidth</span><span class="p">))</span>
</span><span id="PoolingLayer.forward-57"><a href="#PoolingLayer.forward-57"><span class="linenos">57</span></a>            
</span><span id="PoolingLayer.forward-58"><a href="#PoolingLayer.forward-58"><span class="linenos">58</span></a>            <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
</span><span id="PoolingLayer.forward-59"><a href="#PoolingLayer.forward-59"><span class="linenos">59</span></a>                <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">outputHeight</span><span class="p">):</span>
</span><span id="PoolingLayer.forward-60"><a href="#PoolingLayer.forward-60"><span class="linenos">60</span></a>                    <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">outputWidth</span><span class="p">):</span>
</span><span id="PoolingLayer.forward-61"><a href="#PoolingLayer.forward-61"><span class="linenos">61</span></a>                        <span class="n">startX</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
</span><span id="PoolingLayer.forward-62"><a href="#PoolingLayer.forward-62"><span class="linenos">62</span></a>                        <span class="n">startY</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
</span><span id="PoolingLayer.forward-63"><a href="#PoolingLayer.forward-63"><span class="linenos">63</span></a>                        <span class="n">window</span> <span class="o">=</span> <span class="n">inputTensor</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">startX</span><span class="p">:</span><span class="n">startX</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">gridSize</span><span class="p">,</span> <span class="n">startY</span><span class="p">:</span><span class="n">startY</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">gridSize</span><span class="p">]</span>
</span><span id="PoolingLayer.forward-64"><a href="#PoolingLayer.forward-64"><span class="linenos">64</span></a>                        <span class="n">output</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="n">poolFunc</span><span class="p">(</span><span class="n">window</span><span class="p">)</span>
</span><span id="PoolingLayer.forward-65"><a href="#PoolingLayer.forward-65"><span class="linenos">65</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="PoolingLayer.forward-66"><a href="#PoolingLayer.forward-66"><span class="linenos">66</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input tensor wrong dimension&quot;</span><span class="p">)</span>
</span><span id="PoolingLayer.forward-67"><a href="#PoolingLayer.forward-67"><span class="linenos">67</span></a>
</span><span id="PoolingLayer.forward-68"><a href="#PoolingLayer.forward-68"><span class="linenos">68</span></a>        <span class="k">return</span> <span class="n">output</span>
</span></pre></div>


            <div class="docstring"><p>Applies pooling (max or average) to reduce the size of the batch of inputs.</p>

<p>Args:
    inputTensor (ndarray): A 4D or 3D array representing the images with shape (number of images, height, width).</p>

<p>Returns:
    ndarray: A 3D array of feuture maps with reduced shape.</p>
</div>


                            </div>
                </section>
                <section id="GlobalAveragePooling">
                            <input id="GlobalAveragePooling-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">GlobalAveragePooling</span>:

                <label class="view-source-button" for="GlobalAveragePooling-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#GlobalAveragePooling"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="GlobalAveragePooling-4"><a href="#GlobalAveragePooling-4"><span class="linenos"> 4</span></a><span class="k">class</span><span class="w"> </span><span class="nc">GlobalAveragePooling</span><span class="p">():</span>
</span><span id="GlobalAveragePooling-5"><a href="#GlobalAveragePooling-5"><span class="linenos"> 5</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputTensor</span><span class="p">):</span>
</span><span id="GlobalAveragePooling-6"><a href="#GlobalAveragePooling-6"><span class="linenos"> 6</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="GlobalAveragePooling-7"><a href="#GlobalAveragePooling-7"><span class="linenos"> 7</span></a><span class="sd">        Performs global average pooling, reducing each feuture map to a single value.</span>
</span><span id="GlobalAveragePooling-8"><a href="#GlobalAveragePooling-8"><span class="linenos"> 8</span></a>
</span><span id="GlobalAveragePooling-9"><a href="#GlobalAveragePooling-9"><span class="linenos"> 9</span></a><span class="sd">        Args:</span>
</span><span id="GlobalAveragePooling-10"><a href="#GlobalAveragePooling-10"><span class="linenos">10</span></a><span class="sd">            inputTensor (ndarray): A 4D or 3D array representing the images with shape (batches, number of images, height, width).</span>
</span><span id="GlobalAveragePooling-11"><a href="#GlobalAveragePooling-11"><span class="linenos">11</span></a><span class="sd">        </span>
</span><span id="GlobalAveragePooling-12"><a href="#GlobalAveragePooling-12"><span class="linenos">12</span></a><span class="sd">        Returns:</span>
</span><span id="GlobalAveragePooling-13"><a href="#GlobalAveragePooling-13"><span class="linenos">13</span></a><span class="sd">            ndarray: A 3D array containing global averages for each feuture map for each batch.</span>
</span><span id="GlobalAveragePooling-14"><a href="#GlobalAveragePooling-14"><span class="linenos">14</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="GlobalAveragePooling-15"><a href="#GlobalAveragePooling-15"><span class="linenos">15</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">inputShape</span> <span class="o">=</span> <span class="n">inputTensor</span><span class="o">.</span><span class="n">shape</span>
</span><span id="GlobalAveragePooling-16"><a href="#GlobalAveragePooling-16"><span class="linenos">16</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">inputTensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">):</span>
</span><span id="GlobalAveragePooling-17"><a href="#GlobalAveragePooling-17"><span class="linenos">17</span></a>            <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">inputTensor</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</span><span id="GlobalAveragePooling-18"><a href="#GlobalAveragePooling-18"><span class="linenos">18</span></a>        <span class="k">elif</span><span class="p">(</span><span class="n">inputTensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">):</span>
</span><span id="GlobalAveragePooling-19"><a href="#GlobalAveragePooling-19"><span class="linenos">19</span></a>            <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">inputTensor</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</span><span id="GlobalAveragePooling-20"><a href="#GlobalAveragePooling-20"><span class="linenos">20</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="GlobalAveragePooling-21"><a href="#GlobalAveragePooling-21"><span class="linenos">21</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input tensor incorrect dimension&quot;</span><span class="p">)</span>
</span><span id="GlobalAveragePooling-22"><a href="#GlobalAveragePooling-22"><span class="linenos">22</span></a>        <span class="k">return</span> <span class="n">output</span>
</span><span id="GlobalAveragePooling-23"><a href="#GlobalAveragePooling-23"><span class="linenos">23</span></a>
</span><span id="GlobalAveragePooling-24"><a href="#GlobalAveragePooling-24"><span class="linenos">24</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_backpropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gradient</span><span class="p">):</span>   
</span><span id="GlobalAveragePooling-25"><a href="#GlobalAveragePooling-25"><span class="linenos">25</span></a>        <span class="k">if</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inputShape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">):</span>
</span><span id="GlobalAveragePooling-26"><a href="#GlobalAveragePooling-26"><span class="linenos">26</span></a>            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputShape</span>
</span><span id="GlobalAveragePooling-27"><a href="#GlobalAveragePooling-27"><span class="linenos">27</span></a>            <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
</span><span id="GlobalAveragePooling-28"><a href="#GlobalAveragePooling-28"><span class="linenos">28</span></a>            <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
</span><span id="GlobalAveragePooling-29"><a href="#GlobalAveragePooling-29"><span class="linenos">29</span></a>                <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">channels</span><span class="p">):</span>
</span><span id="GlobalAveragePooling-30"><a href="#GlobalAveragePooling-30"><span class="linenos">30</span></a>                    <span class="n">grad</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="n">height</span> <span class="o">*</span> <span class="n">width</span><span class="p">)</span>
</span><span id="GlobalAveragePooling-31"><a href="#GlobalAveragePooling-31"><span class="linenos">31</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="GlobalAveragePooling-32"><a href="#GlobalAveragePooling-32"><span class="linenos">32</span></a>            <span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputShape</span>
</span><span id="GlobalAveragePooling-33"><a href="#GlobalAveragePooling-33"><span class="linenos">33</span></a>            <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">width</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
</span><span id="GlobalAveragePooling-34"><a href="#GlobalAveragePooling-34"><span class="linenos">34</span></a>            <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
</span><span id="GlobalAveragePooling-35"><a href="#GlobalAveragePooling-35"><span class="linenos">35</span></a>                <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">channels</span><span class="p">):</span>
</span><span id="GlobalAveragePooling-36"><a href="#GlobalAveragePooling-36"><span class="linenos">36</span></a>                    <span class="n">grad</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">]</span> <span class="o">/</span> <span class="n">width</span>
</span><span id="GlobalAveragePooling-37"><a href="#GlobalAveragePooling-37"><span class="linenos">37</span></a>        <span class="k">return</span> <span class="n">grad</span>
</span></pre></div>


    

                            <div id="GlobalAveragePooling.forward" class="classattr">
                                        <input id="GlobalAveragePooling.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">inputTensor</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="GlobalAveragePooling.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#GlobalAveragePooling.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="GlobalAveragePooling.forward-5"><a href="#GlobalAveragePooling.forward-5"><span class="linenos"> 5</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputTensor</span><span class="p">):</span>
</span><span id="GlobalAveragePooling.forward-6"><a href="#GlobalAveragePooling.forward-6"><span class="linenos"> 6</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="GlobalAveragePooling.forward-7"><a href="#GlobalAveragePooling.forward-7"><span class="linenos"> 7</span></a><span class="sd">        Performs global average pooling, reducing each feuture map to a single value.</span>
</span><span id="GlobalAveragePooling.forward-8"><a href="#GlobalAveragePooling.forward-8"><span class="linenos"> 8</span></a>
</span><span id="GlobalAveragePooling.forward-9"><a href="#GlobalAveragePooling.forward-9"><span class="linenos"> 9</span></a><span class="sd">        Args:</span>
</span><span id="GlobalAveragePooling.forward-10"><a href="#GlobalAveragePooling.forward-10"><span class="linenos">10</span></a><span class="sd">            inputTensor (ndarray): A 4D or 3D array representing the images with shape (batches, number of images, height, width).</span>
</span><span id="GlobalAveragePooling.forward-11"><a href="#GlobalAveragePooling.forward-11"><span class="linenos">11</span></a><span class="sd">        </span>
</span><span id="GlobalAveragePooling.forward-12"><a href="#GlobalAveragePooling.forward-12"><span class="linenos">12</span></a><span class="sd">        Returns:</span>
</span><span id="GlobalAveragePooling.forward-13"><a href="#GlobalAveragePooling.forward-13"><span class="linenos">13</span></a><span class="sd">            ndarray: A 3D array containing global averages for each feuture map for each batch.</span>
</span><span id="GlobalAveragePooling.forward-14"><a href="#GlobalAveragePooling.forward-14"><span class="linenos">14</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="GlobalAveragePooling.forward-15"><a href="#GlobalAveragePooling.forward-15"><span class="linenos">15</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">inputShape</span> <span class="o">=</span> <span class="n">inputTensor</span><span class="o">.</span><span class="n">shape</span>
</span><span id="GlobalAveragePooling.forward-16"><a href="#GlobalAveragePooling.forward-16"><span class="linenos">16</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">inputTensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">):</span>
</span><span id="GlobalAveragePooling.forward-17"><a href="#GlobalAveragePooling.forward-17"><span class="linenos">17</span></a>            <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">inputTensor</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</span><span id="GlobalAveragePooling.forward-18"><a href="#GlobalAveragePooling.forward-18"><span class="linenos">18</span></a>        <span class="k">elif</span><span class="p">(</span><span class="n">inputTensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">):</span>
</span><span id="GlobalAveragePooling.forward-19"><a href="#GlobalAveragePooling.forward-19"><span class="linenos">19</span></a>            <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">inputTensor</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</span><span id="GlobalAveragePooling.forward-20"><a href="#GlobalAveragePooling.forward-20"><span class="linenos">20</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="GlobalAveragePooling.forward-21"><a href="#GlobalAveragePooling.forward-21"><span class="linenos">21</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input tensor incorrect dimension&quot;</span><span class="p">)</span>
</span><span id="GlobalAveragePooling.forward-22"><a href="#GlobalAveragePooling.forward-22"><span class="linenos">22</span></a>        <span class="k">return</span> <span class="n">output</span>
</span></pre></div>


            <div class="docstring"><p>Performs global average pooling, reducing each feuture map to a single value.</p>

<p>Args:
    inputTensor (ndarray): A 4D or 3D array representing the images with shape (batches, number of images, height, width).</p>

<p>Returns:
    ndarray: A 3D array containing global averages for each feuture map for each batch.</p>
</div>


                            </div>
                </section>
                <section id="StackedRNN">
                            <input id="StackedRNN-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">StackedRNN</span><wbr>(<span class="base">quacknet.RNN.Stacked.StackedBackPropRNN.RNNBackProp</span>):

                <label class="view-source-button" for="StackedRNN-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#StackedRNN"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="StackedRNN-16"><a href="#StackedRNN-16"><span class="linenos"> 16</span></a><span class="k">class</span><span class="w"> </span><span class="nc">StackedRNN</span><span class="p">(</span><span class="n">RNNBackProp</span><span class="p">):</span> 
</span><span id="StackedRNN-17"><a href="#StackedRNN-17"><span class="linenos"> 17</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hiddenStateActivationFunction</span><span class="p">,</span> <span class="n">outputLayerActivationFunction</span><span class="p">,</span> <span class="n">lossFunction</span><span class="p">,</span> <span class="n">numberOfHiddenStates</span><span class="p">,</span> <span class="n">hiddenSizes</span><span class="p">,</span> <span class="n">useBatches</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">batchSize</span> <span class="o">=</span> <span class="mi">64</span><span class="p">):</span>
</span><span id="StackedRNN-18"><a href="#StackedRNN-18"><span class="linenos"> 18</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="StackedRNN-19"><a href="#StackedRNN-19"><span class="linenos"> 19</span></a><span class="sd">        Initializes the RNN model with specified activation functions, loss function,</span>
</span><span id="StackedRNN-20"><a href="#StackedRNN-20"><span class="linenos"> 20</span></a><span class="sd">        hidden layer configuration, and batching options.</span>
</span><span id="StackedRNN-21"><a href="#StackedRNN-21"><span class="linenos"> 21</span></a>
</span><span id="StackedRNN-22"><a href="#StackedRNN-22"><span class="linenos"> 22</span></a><span class="sd">        Args:</span>
</span><span id="StackedRNN-23"><a href="#StackedRNN-23"><span class="linenos"> 23</span></a><span class="sd">            hiddenStateActivationFunction (str): Name of activation function for hidden states (e.g., &quot;relu&quot;, &quot;sigmoid&quot;).</span>
</span><span id="StackedRNN-24"><a href="#StackedRNN-24"><span class="linenos"> 24</span></a><span class="sd">            outputLayerActivationFunction (str): Name of activation function for output layer.</span>
</span><span id="StackedRNN-25"><a href="#StackedRNN-25"><span class="linenos"> 25</span></a><span class="sd">            lossFunction (str): Name of the loss function to use (e.g., &quot;mse&quot;, &quot;mae&quot;, &quot;cross entropy&quot;).</span>
</span><span id="StackedRNN-26"><a href="#StackedRNN-26"><span class="linenos"> 26</span></a><span class="sd">            numberOfHiddenStates (int): Number of hidden layers in the RNN.</span>
</span><span id="StackedRNN-27"><a href="#StackedRNN-27"><span class="linenos"> 27</span></a><span class="sd">            hiddenSizes (list of int): List specifying the size of each hidden layer.</span>
</span><span id="StackedRNN-28"><a href="#StackedRNN-28"><span class="linenos"> 28</span></a><span class="sd">            useBatches (bool, optional): Whether to use batching during training. Default is False.</span>
</span><span id="StackedRNN-29"><a href="#StackedRNN-29"><span class="linenos"> 29</span></a><span class="sd">            batchSize (int, optional): Batch size if batching is enabled. Default is 64.</span>
</span><span id="StackedRNN-30"><a href="#StackedRNN-30"><span class="linenos"> 30</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="StackedRNN-31"><a href="#StackedRNN-31"><span class="linenos"> 31</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">inputWeights</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="StackedRNN-32"><a href="#StackedRNN-32"><span class="linenos"> 32</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hiddenWeights</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="StackedRNN-33"><a href="#StackedRNN-33"><span class="linenos"> 33</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="StackedRNN-34"><a href="#StackedRNN-34"><span class="linenos"> 34</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputWeight</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="StackedRNN-35"><a href="#StackedRNN-35"><span class="linenos"> 35</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputBias</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="StackedRNN-36"><a href="#StackedRNN-36"><span class="linenos"> 36</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hiddenStates</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="StackedRNN-37"><a href="#StackedRNN-37"><span class="linenos"> 37</span></a>        
</span><span id="StackedRNN-38"><a href="#StackedRNN-38"><span class="linenos"> 38</span></a>        <span class="n">funcs</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="StackedRNN-39"><a href="#StackedRNN-39"><span class="linenos"> 39</span></a>            <span class="s2">&quot;relu&quot;</span><span class="p">:</span> <span class="n">relu</span><span class="p">,</span>
</span><span id="StackedRNN-40"><a href="#StackedRNN-40"><span class="linenos"> 40</span></a>            <span class="s2">&quot;sigmoid&quot;</span><span class="p">:</span> <span class="n">sigmoid</span><span class="p">,</span>
</span><span id="StackedRNN-41"><a href="#StackedRNN-41"><span class="linenos"> 41</span></a>            <span class="s2">&quot;linear&quot;</span><span class="p">:</span> <span class="n">linear</span><span class="p">,</span>
</span><span id="StackedRNN-42"><a href="#StackedRNN-42"><span class="linenos"> 42</span></a>            <span class="s2">&quot;tanh&quot;</span><span class="p">:</span> <span class="n">tanH</span><span class="p">,</span>
</span><span id="StackedRNN-43"><a href="#StackedRNN-43"><span class="linenos"> 43</span></a>            <span class="s2">&quot;softmax&quot;</span><span class="p">:</span> <span class="n">softMax</span><span class="p">,</span>
</span><span id="StackedRNN-44"><a href="#StackedRNN-44"><span class="linenos"> 44</span></a>        <span class="p">}</span>
</span><span id="StackedRNN-45"><a href="#StackedRNN-45"><span class="linenos"> 45</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">hiddenStateActivationFunction</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">funcs</span><span class="p">):</span>
</span><span id="StackedRNN-46"><a href="#StackedRNN-46"><span class="linenos"> 46</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Activation function not made: </span><span class="si">{</span><span class="n">hiddenStateActivationFunction</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="StackedRNN-47"><a href="#StackedRNN-47"><span class="linenos"> 47</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">outputLayerActivationFunction</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">funcs</span><span class="p">):</span>
</span><span id="StackedRNN-48"><a href="#StackedRNN-48"><span class="linenos"> 48</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Activation function not made: </span><span class="si">{</span><span class="n">outputLayerActivationFunction</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="StackedRNN-49"><a href="#StackedRNN-49"><span class="linenos"> 49</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hiddenStateActivationFunction</span> <span class="o">=</span> <span class="n">funcs</span><span class="p">[</span><span class="n">hiddenStateActivationFunction</span><span class="o">.</span><span class="n">lower</span><span class="p">()]</span>
</span><span id="StackedRNN-50"><a href="#StackedRNN-50"><span class="linenos"> 50</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputLayerActivationFunction</span> <span class="o">=</span> <span class="n">funcs</span><span class="p">[</span><span class="n">outputLayerActivationFunction</span><span class="o">.</span><span class="n">lower</span><span class="p">()]</span>
</span><span id="StackedRNN-51"><a href="#StackedRNN-51"><span class="linenos"> 51</span></a>
</span><span id="StackedRNN-52"><a href="#StackedRNN-52"><span class="linenos"> 52</span></a>        <span class="n">derivs</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="StackedRNN-53"><a href="#StackedRNN-53"><span class="linenos"> 53</span></a>            <span class="n">relu</span><span class="p">:</span> <span class="n">ReLUDerivative</span><span class="p">,</span>
</span><span id="StackedRNN-54"><a href="#StackedRNN-54"><span class="linenos"> 54</span></a>            <span class="n">sigmoid</span><span class="p">:</span> <span class="n">SigmoidDerivative</span><span class="p">,</span>
</span><span id="StackedRNN-55"><a href="#StackedRNN-55"><span class="linenos"> 55</span></a>            <span class="n">linear</span><span class="p">:</span> <span class="n">LinearDerivative</span><span class="p">,</span>
</span><span id="StackedRNN-56"><a href="#StackedRNN-56"><span class="linenos"> 56</span></a>            <span class="n">tanH</span><span class="p">:</span> <span class="n">TanHDerivative</span><span class="p">,</span>
</span><span id="StackedRNN-57"><a href="#StackedRNN-57"><span class="linenos"> 57</span></a>            <span class="n">softMax</span><span class="p">:</span> <span class="n">SoftMaxDerivative</span><span class="p">,</span>
</span><span id="StackedRNN-58"><a href="#StackedRNN-58"><span class="linenos"> 58</span></a>        <span class="p">}</span>
</span><span id="StackedRNN-59"><a href="#StackedRNN-59"><span class="linenos"> 59</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activationDerivative</span> <span class="o">=</span> <span class="n">derivs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">hiddenStateActivationFunction</span><span class="p">]</span>
</span><span id="StackedRNN-60"><a href="#StackedRNN-60"><span class="linenos"> 60</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputLayerDerivative</span> <span class="o">=</span> <span class="n">derivs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">outputLayerActivationFunction</span><span class="p">]</span>
</span><span id="StackedRNN-61"><a href="#StackedRNN-61"><span class="linenos"> 61</span></a>
</span><span id="StackedRNN-62"><a href="#StackedRNN-62"><span class="linenos"> 62</span></a>        <span class="n">lossFunctionDict</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="StackedRNN-63"><a href="#StackedRNN-63"><span class="linenos"> 63</span></a>            <span class="s2">&quot;mse&quot;</span><span class="p">:</span> <span class="n">MSELossFunction</span><span class="p">,</span>
</span><span id="StackedRNN-64"><a href="#StackedRNN-64"><span class="linenos"> 64</span></a>            <span class="s2">&quot;mae&quot;</span><span class="p">:</span> <span class="n">MAELossFunction</span><span class="p">,</span>
</span><span id="StackedRNN-65"><a href="#StackedRNN-65"><span class="linenos"> 65</span></a>            <span class="s2">&quot;cross entropy&quot;</span><span class="p">:</span> <span class="n">CrossEntropyLossFunction</span><span class="p">,</span><span class="s2">&quot;cross&quot;</span><span class="p">:</span> <span class="n">CrossEntropyLossFunction</span><span class="p">,</span>
</span><span id="StackedRNN-66"><a href="#StackedRNN-66"><span class="linenos"> 66</span></a>        <span class="p">}</span>
</span><span id="StackedRNN-67"><a href="#StackedRNN-67"><span class="linenos"> 67</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">lossFunction</span> <span class="o">=</span> <span class="n">lossFunctionDict</span><span class="p">[</span><span class="n">lossFunction</span><span class="o">.</span><span class="n">lower</span><span class="p">()]</span>
</span><span id="StackedRNN-68"><a href="#StackedRNN-68"><span class="linenos"> 68</span></a>        <span class="n">lossDerivs</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="StackedRNN-69"><a href="#StackedRNN-69"><span class="linenos"> 69</span></a>            <span class="n">MSELossFunction</span><span class="p">:</span> <span class="n">MSEDerivative</span><span class="p">,</span>
</span><span id="StackedRNN-70"><a href="#StackedRNN-70"><span class="linenos"> 70</span></a>            <span class="n">MAELossFunction</span><span class="p">:</span> <span class="n">MAEDerivative</span><span class="p">,</span>
</span><span id="StackedRNN-71"><a href="#StackedRNN-71"><span class="linenos"> 71</span></a>            <span class="n">CrossEntropyLossFunction</span><span class="p">:</span> <span class="n">CrossEntropyLossDerivative</span><span class="p">,</span>
</span><span id="StackedRNN-72"><a href="#StackedRNN-72"><span class="linenos"> 72</span></a>        <span class="p">}</span>
</span><span id="StackedRNN-73"><a href="#StackedRNN-73"><span class="linenos"> 73</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">lossDerivative</span> <span class="o">=</span> <span class="n">lossDerivs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">lossFunction</span><span class="p">]</span>
</span><span id="StackedRNN-74"><a href="#StackedRNN-74"><span class="linenos"> 74</span></a>
</span><span id="StackedRNN-75"><a href="#StackedRNN-75"><span class="linenos"> 75</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">useBatches</span> <span class="o">=</span> <span class="n">useBatches</span>
</span><span id="StackedRNN-76"><a href="#StackedRNN-76"><span class="linenos"> 76</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">useBatches</span> <span class="o">==</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="StackedRNN-77"><a href="#StackedRNN-77"><span class="linenos"> 77</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="StackedRNN-78"><a href="#StackedRNN-78"><span class="linenos"> 78</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="StackedRNN-79"><a href="#StackedRNN-79"><span class="linenos"> 79</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span> <span class="o">=</span> <span class="n">batchSize</span>
</span><span id="StackedRNN-80"><a href="#StackedRNN-80"><span class="linenos"> 80</span></a>
</span><span id="StackedRNN-81"><a href="#StackedRNN-81"><span class="linenos"> 81</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">numberOfHiddenStates</span> <span class="o">=</span> <span class="n">numberOfHiddenStates</span>
</span><span id="StackedRNN-82"><a href="#StackedRNN-82"><span class="linenos"> 82</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hiddenSizes</span> <span class="o">=</span> <span class="n">hiddenSizes</span>
</span><span id="StackedRNN-83"><a href="#StackedRNN-83"><span class="linenos"> 83</span></a>
</span><span id="StackedRNN-84"><a href="#StackedRNN-84"><span class="linenos"> 84</span></a>        <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hiddenSizes</span><span class="p">)</span> <span class="o">==</span> <span class="nb">list</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;hiddenSizes has to be a list&quot;</span>
</span><span id="StackedRNN-85"><a href="#StackedRNN-85"><span class="linenos"> 85</span></a>        <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">hiddenSizes</span><span class="p">:</span>
</span><span id="StackedRNN-86"><a href="#StackedRNN-86"><span class="linenos"> 86</span></a>            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="nb">int</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;hiddenSize has to be a list of integers&quot;</span>
</span><span id="StackedRNN-87"><a href="#StackedRNN-87"><span class="linenos"> 87</span></a>
</span><span id="StackedRNN-88"><a href="#StackedRNN-88"><span class="linenos"> 88</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">adam</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">forwardSequence</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">backwardPropagation</span><span class="p">,</span> <span class="n">giveInputsToBackprop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="StackedRNN-89"><a href="#StackedRNN-89"><span class="linenos"> 89</span></a>
</span><span id="StackedRNN-90"><a href="#StackedRNN-90"><span class="linenos"> 90</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forwardSequence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">):</span> <span class="c1"># goes through the whole sequence / time steps</span>
</span><span id="StackedRNN-91"><a href="#StackedRNN-91"><span class="linenos"> 91</span></a>        <span class="k">assert</span> <span class="n">inputData</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Input data isnt batched&quot;</span>
</span><span id="StackedRNN-92"><a href="#StackedRNN-92"><span class="linenos"> 92</span></a>        <span class="k">assert</span> <span class="n">inputData</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Input data isnt 3D (batchsize, sequenceLength, inputSize)&quot;</span>
</span><span id="StackedRNN-93"><a href="#StackedRNN-93"><span class="linenos"> 93</span></a>        <span class="n">preActivations</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="StackedRNN-94"><a href="#StackedRNN-94"><span class="linenos"> 94</span></a>        <span class="n">allHiddenStates</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="StackedRNN-95"><a href="#StackedRNN-95"><span class="linenos"> 95</span></a>        <span class="n">sequenceLength</span> <span class="o">=</span> <span class="n">inputData</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="StackedRNN-96"><a href="#StackedRNN-96"><span class="linenos"> 96</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">sequenceLength</span><span class="p">):</span>
</span><span id="StackedRNN-97"><a href="#StackedRNN-97"><span class="linenos"> 97</span></a>            <span class="n">xi</span> <span class="o">=</span> <span class="n">inputData</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:]</span>
</span><span id="StackedRNN-98"><a href="#StackedRNN-98"><span class="linenos"> 98</span></a>            <span class="n">allPreAct</span><span class="p">,</span> <span class="n">outputPreAct</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">allHidenStates</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_oneStep</span><span class="p">(</span><span class="n">xi</span><span class="p">)</span>
</span><span id="StackedRNN-99"><a href="#StackedRNN-99"><span class="linenos"> 99</span></a>            <span class="n">preActivations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">allPreAct</span><span class="p">)</span>
</span><span id="StackedRNN-100"><a href="#StackedRNN-100"><span class="linenos">100</span></a>            <span class="n">allHiddenStates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">allHidenStates</span><span class="p">)</span>
</span><span id="StackedRNN-101"><a href="#StackedRNN-101"><span class="linenos">101</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">preActivations</span> <span class="o">=</span> <span class="n">preActivations</span>
</span><span id="StackedRNN-102"><a href="#StackedRNN-102"><span class="linenos">102</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">allHiddenStates</span> <span class="o">=</span> <span class="n">allHiddenStates</span>
</span><span id="StackedRNN-103"><a href="#StackedRNN-103"><span class="linenos">103</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputPreAct</span> <span class="o">=</span> <span class="n">outputPreAct</span>
</span><span id="StackedRNN-104"><a href="#StackedRNN-104"><span class="linenos">104</span></a>        <span class="k">return</span> <span class="n">output</span>
</span><span id="StackedRNN-105"><a href="#StackedRNN-105"><span class="linenos">105</span></a>
</span><span id="StackedRNN-106"><a href="#StackedRNN-106"><span class="linenos">106</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_oneStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">):</span> <span class="c1"># forward prop on 1 time step</span>
</span><span id="StackedRNN-107"><a href="#StackedRNN-107"><span class="linenos">107</span></a>        <span class="n">allPreActivations</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="StackedRNN-108"><a href="#StackedRNN-108"><span class="linenos">108</span></a>        <span class="n">allHidenStates</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="StackedRNN-109"><a href="#StackedRNN-109"><span class="linenos">109</span></a>
</span><span id="StackedRNN-110"><a href="#StackedRNN-110"><span class="linenos">110</span></a>        <span class="n">currentInput</span> <span class="o">=</span> <span class="n">inputData</span>
</span><span id="StackedRNN-111"><a href="#StackedRNN-111"><span class="linenos">111</span></a>
</span><span id="StackedRNN-112"><a href="#StackedRNN-112"><span class="linenos">112</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">numberOfHiddenStates</span><span class="p">):</span>
</span><span id="StackedRNN-113"><a href="#StackedRNN-113"><span class="linenos">113</span></a>            <span class="n">preActivation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hiddenStates</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calculateHiddenLayer</span><span class="p">(</span><span class="n">currentInput</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hiddenStates</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputWeights</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">hiddenWeights</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">hiddenStateActivationFunction</span><span class="p">)</span>
</span><span id="StackedRNN-114"><a href="#StackedRNN-114"><span class="linenos">114</span></a>            
</span><span id="StackedRNN-115"><a href="#StackedRNN-115"><span class="linenos">115</span></a>            <span class="n">allPreActivations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">preActivation</span><span class="p">)</span>
</span><span id="StackedRNN-116"><a href="#StackedRNN-116"><span class="linenos">116</span></a>            <span class="n">allHidenStates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hiddenStates</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span><span id="StackedRNN-117"><a href="#StackedRNN-117"><span class="linenos">117</span></a>            <span class="n">currentInput</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hiddenStates</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</span><span id="StackedRNN-118"><a href="#StackedRNN-118"><span class="linenos">118</span></a>
</span><span id="StackedRNN-119"><a href="#StackedRNN-119"><span class="linenos">119</span></a>        <span class="n">preAct</span><span class="p">,</span> <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calculateOutputLayer</span><span class="p">(</span><span class="n">allHidenStates</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputWeight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputBias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputLayerActivationFunction</span><span class="p">)</span>
</span><span id="StackedRNN-120"><a href="#StackedRNN-120"><span class="linenos">120</span></a>        <span class="k">return</span> <span class="n">allPreActivations</span><span class="p">,</span> <span class="n">preAct</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">allHidenStates</span>
</span><span id="StackedRNN-121"><a href="#StackedRNN-121"><span class="linenos">121</span></a>
</span><span id="StackedRNN-122"><a href="#StackedRNN-122"><span class="linenos">122</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_calculateHiddenLayer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">lastHiddenState</span><span class="p">,</span> <span class="n">inputWeights</span><span class="p">,</span> <span class="n">hiddenWeights</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">activationFunction</span><span class="p">):</span> <span class="c1"># a( w_x * x + w_h * h + b )</span>
</span><span id="StackedRNN-123"><a href="#StackedRNN-123"><span class="linenos">123</span></a>        <span class="n">preActivation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">inputWeights</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">lastHiddenState</span><span class="p">,</span> <span class="n">hiddenWeights</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias</span><span class="o">.</span><span class="n">T</span>
</span><span id="StackedRNN-124"><a href="#StackedRNN-124"><span class="linenos">124</span></a>        <span class="n">newHiddenState</span> <span class="o">=</span> <span class="n">activationFunction</span><span class="p">(</span><span class="n">preActivation</span><span class="p">)</span>
</span><span id="StackedRNN-125"><a href="#StackedRNN-125"><span class="linenos">125</span></a>        <span class="k">return</span> <span class="n">preActivation</span><span class="p">,</span> <span class="n">newHiddenState</span>
</span><span id="StackedRNN-126"><a href="#StackedRNN-126"><span class="linenos">126</span></a>
</span><span id="StackedRNN-127"><a href="#StackedRNN-127"><span class="linenos">127</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_calculateOutputLayer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">outputWeight</span><span class="p">,</span> <span class="n">outputBias</span><span class="p">,</span> <span class="n">activationFunction</span><span class="p">):</span> <span class="c1"># a( w_o * o + b_o)</span>
</span><span id="StackedRNN-128"><a href="#StackedRNN-128"><span class="linenos">128</span></a>        <span class="n">preActivation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">outputWeight</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="n">outputBias</span><span class="o">.</span><span class="n">T</span>
</span><span id="StackedRNN-129"><a href="#StackedRNN-129"><span class="linenos">129</span></a>        <span class="n">output</span> <span class="o">=</span> <span class="n">activationFunction</span><span class="p">(</span><span class="n">preActivation</span><span class="p">)</span>
</span><span id="StackedRNN-130"><a href="#StackedRNN-130"><span class="linenos">130</span></a>        <span class="k">return</span> <span class="n">preActivation</span><span class="p">,</span> <span class="n">output</span>
</span><span id="StackedRNN-131"><a href="#StackedRNN-131"><span class="linenos">131</span></a>
</span><span id="StackedRNN-132"><a href="#StackedRNN-132"><span class="linenos">132</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_initialiseWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputSize</span><span class="p">,</span> <span class="n">inputSize</span><span class="p">,</span> <span class="n">activationFunction</span><span class="p">):</span>
</span><span id="StackedRNN-133"><a href="#StackedRNN-133"><span class="linenos">133</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">activationFunction</span> <span class="o">==</span> <span class="n">relu</span><span class="p">):</span>
</span><span id="StackedRNN-134"><a href="#StackedRNN-134"><span class="linenos">134</span></a>            <span class="n">bounds</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">inputSize</span><span class="p">)</span> <span class="c1"># He initialisation</span>
</span><span id="StackedRNN-135"><a href="#StackedRNN-135"><span class="linenos">135</span></a>        <span class="k">elif</span><span class="p">(</span><span class="n">activationFunction</span> <span class="o">==</span> <span class="n">sigmoid</span><span class="p">):</span>
</span><span id="StackedRNN-136"><a href="#StackedRNN-136"><span class="linenos">136</span></a>            <span class="n">bounds</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">6</span> <span class="o">/</span> <span class="p">(</span><span class="n">inputSize</span> <span class="o">+</span> <span class="n">outputSize</span><span class="p">))</span> <span class="c1"># Xavier initialisation</span>
</span><span id="StackedRNN-137"><a href="#StackedRNN-137"><span class="linenos">137</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="StackedRNN-138"><a href="#StackedRNN-138"><span class="linenos">138</span></a>            <span class="n">bounds</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">inputSize</span><span class="p">)</span> <span class="c1"># default</span>
</span><span id="StackedRNN-139"><a href="#StackedRNN-139"><span class="linenos">139</span></a>        <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">bounds</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">outputSize</span><span class="p">,</span> <span class="n">inputSize</span><span class="p">))</span>
</span><span id="StackedRNN-140"><a href="#StackedRNN-140"><span class="linenos">140</span></a>        <span class="k">return</span> <span class="n">w</span>
</span><span id="StackedRNN-141"><a href="#StackedRNN-141"><span class="linenos">141</span></a>    
</span><span id="StackedRNN-142"><a href="#StackedRNN-142"><span class="linenos">142</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">initialiseWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputSize</span><span class="p">,</span> <span class="n">outputSize</span><span class="p">):</span>
</span><span id="StackedRNN-143"><a href="#StackedRNN-143"><span class="linenos">143</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="StackedRNN-144"><a href="#StackedRNN-144"><span class="linenos">144</span></a><span class="sd">        Initializes weights, biases, and hidden states for each hidden layer and the output layer.</span>
</span><span id="StackedRNN-145"><a href="#StackedRNN-145"><span class="linenos">145</span></a>
</span><span id="StackedRNN-146"><a href="#StackedRNN-146"><span class="linenos">146</span></a><span class="sd">        Args:</span>
</span><span id="StackedRNN-147"><a href="#StackedRNN-147"><span class="linenos">147</span></a><span class="sd">            inputSize (int): Size of the input feature vector.</span>
</span><span id="StackedRNN-148"><a href="#StackedRNN-148"><span class="linenos">148</span></a><span class="sd">            outputSize (int): Size of the output vector.</span>
</span><span id="StackedRNN-149"><a href="#StackedRNN-149"><span class="linenos">149</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="StackedRNN-150"><a href="#StackedRNN-150"><span class="linenos">150</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">inputWeights</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="StackedRNN-151"><a href="#StackedRNN-151"><span class="linenos">151</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hiddenWeights</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="StackedRNN-152"><a href="#StackedRNN-152"><span class="linenos">152</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="StackedRNN-153"><a href="#StackedRNN-153"><span class="linenos">153</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hiddenStates</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="StackedRNN-154"><a href="#StackedRNN-154"><span class="linenos">154</span></a>
</span><span id="StackedRNN-155"><a href="#StackedRNN-155"><span class="linenos">155</span></a>        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">hiddenSize</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hiddenSizes</span><span class="p">):</span>
</span><span id="StackedRNN-156"><a href="#StackedRNN-156"><span class="linenos">156</span></a>            <span class="n">inSize</span> <span class="o">=</span> <span class="n">inputSize</span> 
</span><span id="StackedRNN-157"><a href="#StackedRNN-157"><span class="linenos">157</span></a>            <span class="k">if</span><span class="p">(</span><span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">):</span>
</span><span id="StackedRNN-158"><a href="#StackedRNN-158"><span class="linenos">158</span></a>                <span class="n">inSize</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hiddenSizes</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
</span><span id="StackedRNN-159"><a href="#StackedRNN-159"><span class="linenos">159</span></a>
</span><span id="StackedRNN-160"><a href="#StackedRNN-160"><span class="linenos">160</span></a>            <span class="n">inputW</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialiseWeights</span><span class="p">(</span><span class="n">hiddenSize</span><span class="p">,</span> <span class="n">inSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hiddenStateActivationFunction</span><span class="p">)</span>
</span><span id="StackedRNN-161"><a href="#StackedRNN-161"><span class="linenos">161</span></a>            <span class="n">hiddenW</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialiseWeights</span><span class="p">(</span><span class="n">hiddenSize</span><span class="p">,</span> <span class="n">hiddenSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hiddenStateActivationFunction</span><span class="p">)</span>
</span><span id="StackedRNN-162"><a href="#StackedRNN-162"><span class="linenos">162</span></a>            <span class="n">bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">hiddenSize</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span><span id="StackedRNN-163"><a href="#StackedRNN-163"><span class="linenos">163</span></a>            <span class="n">hiddenState</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span> <span class="n">hiddenSize</span><span class="p">))</span>
</span><span id="StackedRNN-164"><a href="#StackedRNN-164"><span class="linenos">164</span></a>        
</span><span id="StackedRNN-165"><a href="#StackedRNN-165"><span class="linenos">165</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">inputWeights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inputW</span><span class="p">)</span>
</span><span id="StackedRNN-166"><a href="#StackedRNN-166"><span class="linenos">166</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">hiddenWeights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">hiddenW</span><span class="p">)</span>
</span><span id="StackedRNN-167"><a href="#StackedRNN-167"><span class="linenos">167</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bias</span><span class="p">)</span>
</span><span id="StackedRNN-168"><a href="#StackedRNN-168"><span class="linenos">168</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">hiddenStates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">hiddenState</span><span class="p">)</span>
</span><span id="StackedRNN-169"><a href="#StackedRNN-169"><span class="linenos">169</span></a>
</span><span id="StackedRNN-170"><a href="#StackedRNN-170"><span class="linenos">170</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputWeight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialiseWeights</span><span class="p">(</span><span class="n">outputSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hiddenSizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputLayerActivationFunction</span><span class="p">)</span>
</span><span id="StackedRNN-171"><a href="#StackedRNN-171"><span class="linenos">171</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputBias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">outputSize</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span><span id="StackedRNN-172"><a href="#StackedRNN-172"><span class="linenos">172</span></a>    
</span><span id="StackedRNN-173"><a href="#StackedRNN-173"><span class="linenos">173</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">inputSize</span> <span class="o">=</span> <span class="n">inputSize</span>
</span><span id="StackedRNN-174"><a href="#StackedRNN-174"><span class="linenos">174</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputSize</span> <span class="o">=</span> <span class="n">outputSize</span>
</span><span id="StackedRNN-175"><a href="#StackedRNN-175"><span class="linenos">175</span></a>
</span><span id="StackedRNN-176"><a href="#StackedRNN-176"><span class="linenos">176</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">backwardPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
</span><span id="StackedRNN-177"><a href="#StackedRNN-177"><span class="linenos">177</span></a>        <span class="n">inputWeightGradients</span><span class="p">,</span> <span class="n">hiddenStateWeightGradients</span><span class="p">,</span> <span class="n">biasGradients</span><span class="p">,</span> <span class="n">outputWeightGradients</span><span class="p">,</span> <span class="n">outputbiasGradients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Stacked_BPTT</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">allHiddenStates</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">preActivations</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputPreAct</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
</span><span id="StackedRNN-178"><a href="#StackedRNN-178"><span class="linenos">178</span></a>        <span class="n">Parameters</span> <span class="o">=</span>  <span class="p">{</span>
</span><span id="StackedRNN-179"><a href="#StackedRNN-179"><span class="linenos">179</span></a>            <span class="s2">&quot;I_W&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputWeights</span><span class="p">,</span>
</span><span id="StackedRNN-180"><a href="#StackedRNN-180"><span class="linenos">180</span></a>            <span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">,</span>
</span><span id="StackedRNN-181"><a href="#StackedRNN-181"><span class="linenos">181</span></a>            <span class="s2">&quot;H_W&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">hiddenWeights</span><span class="p">,</span>
</span><span id="StackedRNN-182"><a href="#StackedRNN-182"><span class="linenos">182</span></a>            <span class="s2">&quot;O_W&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputWeight</span><span class="p">,</span>
</span><span id="StackedRNN-183"><a href="#StackedRNN-183"><span class="linenos">183</span></a>            <span class="s2">&quot;O_b&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputBias</span><span class="p">,</span>
</span><span id="StackedRNN-184"><a href="#StackedRNN-184"><span class="linenos">184</span></a>        <span class="p">}</span>
</span><span id="StackedRNN-185"><a href="#StackedRNN-185"><span class="linenos">185</span></a>  
</span><span id="StackedRNN-186"><a href="#StackedRNN-186"><span class="linenos">186</span></a>        <span class="n">Gradients</span> <span class="o">=</span>  <span class="p">{</span>
</span><span id="StackedRNN-187"><a href="#StackedRNN-187"><span class="linenos">187</span></a>            <span class="s2">&quot;I_W&quot;</span><span class="p">:</span> <span class="n">inputWeightGradients</span><span class="p">,</span>
</span><span id="StackedRNN-188"><a href="#StackedRNN-188"><span class="linenos">188</span></a>            <span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="n">biasGradients</span><span class="p">,</span>
</span><span id="StackedRNN-189"><a href="#StackedRNN-189"><span class="linenos">189</span></a>            <span class="s2">&quot;H_W&quot;</span><span class="p">:</span> <span class="n">hiddenStateWeightGradients</span><span class="p">,</span>
</span><span id="StackedRNN-190"><a href="#StackedRNN-190"><span class="linenos">190</span></a>            <span class="s2">&quot;O_W&quot;</span><span class="p">:</span> <span class="n">outputWeightGradients</span><span class="p">,</span>
</span><span id="StackedRNN-191"><a href="#StackedRNN-191"><span class="linenos">191</span></a>            <span class="s2">&quot;O_b&quot;</span><span class="p">:</span> <span class="n">outputbiasGradients</span><span class="p">,</span>
</span><span id="StackedRNN-192"><a href="#StackedRNN-192"><span class="linenos">192</span></a>        <span class="p">}</span>
</span><span id="StackedRNN-193"><a href="#StackedRNN-193"><span class="linenos">193</span></a>        <span class="k">return</span> <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span> 
</span><span id="StackedRNN-194"><a href="#StackedRNN-194"><span class="linenos">194</span></a>
</span><span id="StackedRNN-195"><a href="#StackedRNN-195"><span class="linenos">195</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">optimiser</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">):</span>
</span><span id="StackedRNN-196"><a href="#StackedRNN-196"><span class="linenos">196</span></a>        <span class="n">AllOutputs</span><span class="p">,</span> <span class="n">Parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adam</span><span class="o">.</span><span class="n">optimiser</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">useBatches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
</span><span id="StackedRNN-197"><a href="#StackedRNN-197"><span class="linenos">197</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">inputWeights</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;I_W&quot;</span><span class="p">]</span>
</span><span id="StackedRNN-198"><a href="#StackedRNN-198"><span class="linenos">198</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">]</span>
</span><span id="StackedRNN-199"><a href="#StackedRNN-199"><span class="linenos">199</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hiddenWeights</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;H_W&quot;</span><span class="p">]</span>
</span><span id="StackedRNN-200"><a href="#StackedRNN-200"><span class="linenos">200</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputWeight</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;O_W&quot;</span><span class="p">]</span>
</span><span id="StackedRNN-201"><a href="#StackedRNN-201"><span class="linenos">201</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputBias</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;O_b&quot;</span><span class="p">]</span>
</span><span id="StackedRNN-202"><a href="#StackedRNN-202"><span class="linenos">202</span></a>        <span class="k">return</span> <span class="n">AllOutputs</span>
</span><span id="StackedRNN-203"><a href="#StackedRNN-203"><span class="linenos">203</span></a>
</span><span id="StackedRNN-204"><a href="#StackedRNN-204"><span class="linenos">204</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span> <span class="n">beta1</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span> <span class="n">beta2</span> <span class="o">=</span> <span class="mf">0.999</span><span class="p">,</span> <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">):</span>
</span><span id="StackedRNN-205"><a href="#StackedRNN-205"><span class="linenos">205</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="StackedRNN-206"><a href="#StackedRNN-206"><span class="linenos">206</span></a><span class="sd">        Trains the model on the given input data and labels using the Adam optimizer,</span>
</span><span id="StackedRNN-207"><a href="#StackedRNN-207"><span class="linenos">207</span></a><span class="sd">        then calculates and returns the loss.</span>
</span><span id="StackedRNN-208"><a href="#StackedRNN-208"><span class="linenos">208</span></a>
</span><span id="StackedRNN-209"><a href="#StackedRNN-209"><span class="linenos">209</span></a><span class="sd">        Args:</span>
</span><span id="StackedRNN-210"><a href="#StackedRNN-210"><span class="linenos">210</span></a><span class="sd">            inputData (ndarray): 3D array of input sequences with shape (batchSize, sequenceLength, inputSize).</span>
</span><span id="StackedRNN-211"><a href="#StackedRNN-211"><span class="linenos">211</span></a><span class="sd">            labels (ndarray): True labels corresponding to the input data.</span>
</span><span id="StackedRNN-212"><a href="#StackedRNN-212"><span class="linenos">212</span></a><span class="sd">            alpha (float, optional): Learning rate for the optimizer. Default is 0.001.</span>
</span><span id="StackedRNN-213"><a href="#StackedRNN-213"><span class="linenos">213</span></a><span class="sd">            beta1 (float, optional): Beta1 parameter for Adam optimizer. Default is 0.9.</span>
</span><span id="StackedRNN-214"><a href="#StackedRNN-214"><span class="linenos">214</span></a><span class="sd">            beta2 (float, optional): Beta2 parameter for Adam optimizer. Default is 0.999.</span>
</span><span id="StackedRNN-215"><a href="#StackedRNN-215"><span class="linenos">215</span></a><span class="sd">            epsilon (float, optional): Epsilon parameter for Adam optimizer to avoid division by zero. Default is 1e-8.</span>
</span><span id="StackedRNN-216"><a href="#StackedRNN-216"><span class="linenos">216</span></a>
</span><span id="StackedRNN-217"><a href="#StackedRNN-217"><span class="linenos">217</span></a><span class="sd">        Returns:</span>
</span><span id="StackedRNN-218"><a href="#StackedRNN-218"><span class="linenos">218</span></a><span class="sd">            float: Calculated loss between the model output and true labels.</span>
</span><span id="StackedRNN-219"><a href="#StackedRNN-219"><span class="linenos">219</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="StackedRNN-220"><a href="#StackedRNN-220"><span class="linenos">220</span></a>        <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputData</span><span class="p">)</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Dimension wrong size, got </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputData</span><span class="p">)</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">, expected 3&quot;</span>
</span><span id="StackedRNN-221"><a href="#StackedRNN-221"><span class="linenos">221</span></a>        <span class="n">AllOutputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimiser</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
</span><span id="StackedRNN-222"><a href="#StackedRNN-222"><span class="linenos">222</span></a>        <span class="n">AllOutputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">AllOutputs</span><span class="p">,</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</span><span id="StackedRNN-223"><a href="#StackedRNN-223"><span class="linenos">223</span></a>        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lossFunction</span><span class="p">(</span><span class="n">AllOutputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</span><span id="StackedRNN-224"><a href="#StackedRNN-224"><span class="linenos">224</span></a>        <span class="k">return</span> <span class="n">loss</span>
</span></pre></div>


    

                            <div id="StackedRNN.__init__" class="classattr">
                                        <input id="StackedRNN.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">StackedRNN</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">hiddenStateActivationFunction</span>,</span><span class="param">	<span class="n">outputLayerActivationFunction</span>,</span><span class="param">	<span class="n">lossFunction</span>,</span><span class="param">	<span class="n">numberOfHiddenStates</span>,</span><span class="param">	<span class="n">hiddenSizes</span>,</span><span class="param">	<span class="n">useBatches</span><span class="o">=</span><span class="kc">False</span>,</span><span class="param">	<span class="n">batchSize</span><span class="o">=</span><span class="mi">64</span></span>)</span>

                <label class="view-source-button" for="StackedRNN.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#StackedRNN.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="StackedRNN.__init__-17"><a href="#StackedRNN.__init__-17"><span class="linenos">17</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hiddenStateActivationFunction</span><span class="p">,</span> <span class="n">outputLayerActivationFunction</span><span class="p">,</span> <span class="n">lossFunction</span><span class="p">,</span> <span class="n">numberOfHiddenStates</span><span class="p">,</span> <span class="n">hiddenSizes</span><span class="p">,</span> <span class="n">useBatches</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">batchSize</span> <span class="o">=</span> <span class="mi">64</span><span class="p">):</span>
</span><span id="StackedRNN.__init__-18"><a href="#StackedRNN.__init__-18"><span class="linenos">18</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="StackedRNN.__init__-19"><a href="#StackedRNN.__init__-19"><span class="linenos">19</span></a><span class="sd">        Initializes the RNN model with specified activation functions, loss function,</span>
</span><span id="StackedRNN.__init__-20"><a href="#StackedRNN.__init__-20"><span class="linenos">20</span></a><span class="sd">        hidden layer configuration, and batching options.</span>
</span><span id="StackedRNN.__init__-21"><a href="#StackedRNN.__init__-21"><span class="linenos">21</span></a>
</span><span id="StackedRNN.__init__-22"><a href="#StackedRNN.__init__-22"><span class="linenos">22</span></a><span class="sd">        Args:</span>
</span><span id="StackedRNN.__init__-23"><a href="#StackedRNN.__init__-23"><span class="linenos">23</span></a><span class="sd">            hiddenStateActivationFunction (str): Name of activation function for hidden states (e.g., &quot;relu&quot;, &quot;sigmoid&quot;).</span>
</span><span id="StackedRNN.__init__-24"><a href="#StackedRNN.__init__-24"><span class="linenos">24</span></a><span class="sd">            outputLayerActivationFunction (str): Name of activation function for output layer.</span>
</span><span id="StackedRNN.__init__-25"><a href="#StackedRNN.__init__-25"><span class="linenos">25</span></a><span class="sd">            lossFunction (str): Name of the loss function to use (e.g., &quot;mse&quot;, &quot;mae&quot;, &quot;cross entropy&quot;).</span>
</span><span id="StackedRNN.__init__-26"><a href="#StackedRNN.__init__-26"><span class="linenos">26</span></a><span class="sd">            numberOfHiddenStates (int): Number of hidden layers in the RNN.</span>
</span><span id="StackedRNN.__init__-27"><a href="#StackedRNN.__init__-27"><span class="linenos">27</span></a><span class="sd">            hiddenSizes (list of int): List specifying the size of each hidden layer.</span>
</span><span id="StackedRNN.__init__-28"><a href="#StackedRNN.__init__-28"><span class="linenos">28</span></a><span class="sd">            useBatches (bool, optional): Whether to use batching during training. Default is False.</span>
</span><span id="StackedRNN.__init__-29"><a href="#StackedRNN.__init__-29"><span class="linenos">29</span></a><span class="sd">            batchSize (int, optional): Batch size if batching is enabled. Default is 64.</span>
</span><span id="StackedRNN.__init__-30"><a href="#StackedRNN.__init__-30"><span class="linenos">30</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="StackedRNN.__init__-31"><a href="#StackedRNN.__init__-31"><span class="linenos">31</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">inputWeights</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="StackedRNN.__init__-32"><a href="#StackedRNN.__init__-32"><span class="linenos">32</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hiddenWeights</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="StackedRNN.__init__-33"><a href="#StackedRNN.__init__-33"><span class="linenos">33</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="StackedRNN.__init__-34"><a href="#StackedRNN.__init__-34"><span class="linenos">34</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputWeight</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="StackedRNN.__init__-35"><a href="#StackedRNN.__init__-35"><span class="linenos">35</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputBias</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="StackedRNN.__init__-36"><a href="#StackedRNN.__init__-36"><span class="linenos">36</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hiddenStates</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="StackedRNN.__init__-37"><a href="#StackedRNN.__init__-37"><span class="linenos">37</span></a>        
</span><span id="StackedRNN.__init__-38"><a href="#StackedRNN.__init__-38"><span class="linenos">38</span></a>        <span class="n">funcs</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="StackedRNN.__init__-39"><a href="#StackedRNN.__init__-39"><span class="linenos">39</span></a>            <span class="s2">&quot;relu&quot;</span><span class="p">:</span> <span class="n">relu</span><span class="p">,</span>
</span><span id="StackedRNN.__init__-40"><a href="#StackedRNN.__init__-40"><span class="linenos">40</span></a>            <span class="s2">&quot;sigmoid&quot;</span><span class="p">:</span> <span class="n">sigmoid</span><span class="p">,</span>
</span><span id="StackedRNN.__init__-41"><a href="#StackedRNN.__init__-41"><span class="linenos">41</span></a>            <span class="s2">&quot;linear&quot;</span><span class="p">:</span> <span class="n">linear</span><span class="p">,</span>
</span><span id="StackedRNN.__init__-42"><a href="#StackedRNN.__init__-42"><span class="linenos">42</span></a>            <span class="s2">&quot;tanh&quot;</span><span class="p">:</span> <span class="n">tanH</span><span class="p">,</span>
</span><span id="StackedRNN.__init__-43"><a href="#StackedRNN.__init__-43"><span class="linenos">43</span></a>            <span class="s2">&quot;softmax&quot;</span><span class="p">:</span> <span class="n">softMax</span><span class="p">,</span>
</span><span id="StackedRNN.__init__-44"><a href="#StackedRNN.__init__-44"><span class="linenos">44</span></a>        <span class="p">}</span>
</span><span id="StackedRNN.__init__-45"><a href="#StackedRNN.__init__-45"><span class="linenos">45</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">hiddenStateActivationFunction</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">funcs</span><span class="p">):</span>
</span><span id="StackedRNN.__init__-46"><a href="#StackedRNN.__init__-46"><span class="linenos">46</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Activation function not made: </span><span class="si">{</span><span class="n">hiddenStateActivationFunction</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="StackedRNN.__init__-47"><a href="#StackedRNN.__init__-47"><span class="linenos">47</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">outputLayerActivationFunction</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">funcs</span><span class="p">):</span>
</span><span id="StackedRNN.__init__-48"><a href="#StackedRNN.__init__-48"><span class="linenos">48</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Activation function not made: </span><span class="si">{</span><span class="n">outputLayerActivationFunction</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="StackedRNN.__init__-49"><a href="#StackedRNN.__init__-49"><span class="linenos">49</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hiddenStateActivationFunction</span> <span class="o">=</span> <span class="n">funcs</span><span class="p">[</span><span class="n">hiddenStateActivationFunction</span><span class="o">.</span><span class="n">lower</span><span class="p">()]</span>
</span><span id="StackedRNN.__init__-50"><a href="#StackedRNN.__init__-50"><span class="linenos">50</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputLayerActivationFunction</span> <span class="o">=</span> <span class="n">funcs</span><span class="p">[</span><span class="n">outputLayerActivationFunction</span><span class="o">.</span><span class="n">lower</span><span class="p">()]</span>
</span><span id="StackedRNN.__init__-51"><a href="#StackedRNN.__init__-51"><span class="linenos">51</span></a>
</span><span id="StackedRNN.__init__-52"><a href="#StackedRNN.__init__-52"><span class="linenos">52</span></a>        <span class="n">derivs</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="StackedRNN.__init__-53"><a href="#StackedRNN.__init__-53"><span class="linenos">53</span></a>            <span class="n">relu</span><span class="p">:</span> <span class="n">ReLUDerivative</span><span class="p">,</span>
</span><span id="StackedRNN.__init__-54"><a href="#StackedRNN.__init__-54"><span class="linenos">54</span></a>            <span class="n">sigmoid</span><span class="p">:</span> <span class="n">SigmoidDerivative</span><span class="p">,</span>
</span><span id="StackedRNN.__init__-55"><a href="#StackedRNN.__init__-55"><span class="linenos">55</span></a>            <span class="n">linear</span><span class="p">:</span> <span class="n">LinearDerivative</span><span class="p">,</span>
</span><span id="StackedRNN.__init__-56"><a href="#StackedRNN.__init__-56"><span class="linenos">56</span></a>            <span class="n">tanH</span><span class="p">:</span> <span class="n">TanHDerivative</span><span class="p">,</span>
</span><span id="StackedRNN.__init__-57"><a href="#StackedRNN.__init__-57"><span class="linenos">57</span></a>            <span class="n">softMax</span><span class="p">:</span> <span class="n">SoftMaxDerivative</span><span class="p">,</span>
</span><span id="StackedRNN.__init__-58"><a href="#StackedRNN.__init__-58"><span class="linenos">58</span></a>        <span class="p">}</span>
</span><span id="StackedRNN.__init__-59"><a href="#StackedRNN.__init__-59"><span class="linenos">59</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activationDerivative</span> <span class="o">=</span> <span class="n">derivs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">hiddenStateActivationFunction</span><span class="p">]</span>
</span><span id="StackedRNN.__init__-60"><a href="#StackedRNN.__init__-60"><span class="linenos">60</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputLayerDerivative</span> <span class="o">=</span> <span class="n">derivs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">outputLayerActivationFunction</span><span class="p">]</span>
</span><span id="StackedRNN.__init__-61"><a href="#StackedRNN.__init__-61"><span class="linenos">61</span></a>
</span><span id="StackedRNN.__init__-62"><a href="#StackedRNN.__init__-62"><span class="linenos">62</span></a>        <span class="n">lossFunctionDict</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="StackedRNN.__init__-63"><a href="#StackedRNN.__init__-63"><span class="linenos">63</span></a>            <span class="s2">&quot;mse&quot;</span><span class="p">:</span> <span class="n">MSELossFunction</span><span class="p">,</span>
</span><span id="StackedRNN.__init__-64"><a href="#StackedRNN.__init__-64"><span class="linenos">64</span></a>            <span class="s2">&quot;mae&quot;</span><span class="p">:</span> <span class="n">MAELossFunction</span><span class="p">,</span>
</span><span id="StackedRNN.__init__-65"><a href="#StackedRNN.__init__-65"><span class="linenos">65</span></a>            <span class="s2">&quot;cross entropy&quot;</span><span class="p">:</span> <span class="n">CrossEntropyLossFunction</span><span class="p">,</span><span class="s2">&quot;cross&quot;</span><span class="p">:</span> <span class="n">CrossEntropyLossFunction</span><span class="p">,</span>
</span><span id="StackedRNN.__init__-66"><a href="#StackedRNN.__init__-66"><span class="linenos">66</span></a>        <span class="p">}</span>
</span><span id="StackedRNN.__init__-67"><a href="#StackedRNN.__init__-67"><span class="linenos">67</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">lossFunction</span> <span class="o">=</span> <span class="n">lossFunctionDict</span><span class="p">[</span><span class="n">lossFunction</span><span class="o">.</span><span class="n">lower</span><span class="p">()]</span>
</span><span id="StackedRNN.__init__-68"><a href="#StackedRNN.__init__-68"><span class="linenos">68</span></a>        <span class="n">lossDerivs</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="StackedRNN.__init__-69"><a href="#StackedRNN.__init__-69"><span class="linenos">69</span></a>            <span class="n">MSELossFunction</span><span class="p">:</span> <span class="n">MSEDerivative</span><span class="p">,</span>
</span><span id="StackedRNN.__init__-70"><a href="#StackedRNN.__init__-70"><span class="linenos">70</span></a>            <span class="n">MAELossFunction</span><span class="p">:</span> <span class="n">MAEDerivative</span><span class="p">,</span>
</span><span id="StackedRNN.__init__-71"><a href="#StackedRNN.__init__-71"><span class="linenos">71</span></a>            <span class="n">CrossEntropyLossFunction</span><span class="p">:</span> <span class="n">CrossEntropyLossDerivative</span><span class="p">,</span>
</span><span id="StackedRNN.__init__-72"><a href="#StackedRNN.__init__-72"><span class="linenos">72</span></a>        <span class="p">}</span>
</span><span id="StackedRNN.__init__-73"><a href="#StackedRNN.__init__-73"><span class="linenos">73</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">lossDerivative</span> <span class="o">=</span> <span class="n">lossDerivs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">lossFunction</span><span class="p">]</span>
</span><span id="StackedRNN.__init__-74"><a href="#StackedRNN.__init__-74"><span class="linenos">74</span></a>
</span><span id="StackedRNN.__init__-75"><a href="#StackedRNN.__init__-75"><span class="linenos">75</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">useBatches</span> <span class="o">=</span> <span class="n">useBatches</span>
</span><span id="StackedRNN.__init__-76"><a href="#StackedRNN.__init__-76"><span class="linenos">76</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">useBatches</span> <span class="o">==</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="StackedRNN.__init__-77"><a href="#StackedRNN.__init__-77"><span class="linenos">77</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="StackedRNN.__init__-78"><a href="#StackedRNN.__init__-78"><span class="linenos">78</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="StackedRNN.__init__-79"><a href="#StackedRNN.__init__-79"><span class="linenos">79</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span> <span class="o">=</span> <span class="n">batchSize</span>
</span><span id="StackedRNN.__init__-80"><a href="#StackedRNN.__init__-80"><span class="linenos">80</span></a>
</span><span id="StackedRNN.__init__-81"><a href="#StackedRNN.__init__-81"><span class="linenos">81</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">numberOfHiddenStates</span> <span class="o">=</span> <span class="n">numberOfHiddenStates</span>
</span><span id="StackedRNN.__init__-82"><a href="#StackedRNN.__init__-82"><span class="linenos">82</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hiddenSizes</span> <span class="o">=</span> <span class="n">hiddenSizes</span>
</span><span id="StackedRNN.__init__-83"><a href="#StackedRNN.__init__-83"><span class="linenos">83</span></a>
</span><span id="StackedRNN.__init__-84"><a href="#StackedRNN.__init__-84"><span class="linenos">84</span></a>        <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hiddenSizes</span><span class="p">)</span> <span class="o">==</span> <span class="nb">list</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;hiddenSizes has to be a list&quot;</span>
</span><span id="StackedRNN.__init__-85"><a href="#StackedRNN.__init__-85"><span class="linenos">85</span></a>        <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">hiddenSizes</span><span class="p">:</span>
</span><span id="StackedRNN.__init__-86"><a href="#StackedRNN.__init__-86"><span class="linenos">86</span></a>            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="nb">int</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;hiddenSize has to be a list of integers&quot;</span>
</span><span id="StackedRNN.__init__-87"><a href="#StackedRNN.__init__-87"><span class="linenos">87</span></a>
</span><span id="StackedRNN.__init__-88"><a href="#StackedRNN.__init__-88"><span class="linenos">88</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">adam</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">forwardSequence</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">backwardPropagation</span><span class="p">,</span> <span class="n">giveInputsToBackprop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Initializes the RNN model with specified activation functions, loss function,
hidden layer configuration, and batching options.</p>

<p>Args:
    hiddenStateActivationFunction (str): Name of activation function for hidden states (e.g., "relu", "sigmoid").
    outputLayerActivationFunction (str): Name of activation function for output layer.
    lossFunction (str): Name of the loss function to use (e.g., "mse", "mae", "cross entropy").
    numberOfHiddenStates (int): Number of hidden layers in the RNN.
    hiddenSizes (list of int): List specifying the size of each hidden layer.
    useBatches (bool, optional): Whether to use batching during training. Default is False.
    batchSize (int, optional): Batch size if batching is enabled. Default is 64.</p>
</div>


                            </div>
                            <div id="StackedRNN.inputWeights" class="classattr">
                                <div class="attr variable">
            <span class="name">inputWeights</span>

        
    </div>
    <a class="headerlink" href="#StackedRNN.inputWeights"></a>
    
    

                            </div>
                            <div id="StackedRNN.hiddenWeights" class="classattr">
                                <div class="attr variable">
            <span class="name">hiddenWeights</span>

        
    </div>
    <a class="headerlink" href="#StackedRNN.hiddenWeights"></a>
    
    

                            </div>
                            <div id="StackedRNN.biases" class="classattr">
                                <div class="attr variable">
            <span class="name">biases</span>

        
    </div>
    <a class="headerlink" href="#StackedRNN.biases"></a>
    
    

                            </div>
                            <div id="StackedRNN.outputWeight" class="classattr">
                                <div class="attr variable">
            <span class="name">outputWeight</span>

        
    </div>
    <a class="headerlink" href="#StackedRNN.outputWeight"></a>
    
    

                            </div>
                            <div id="StackedRNN.outputBias" class="classattr">
                                <div class="attr variable">
            <span class="name">outputBias</span>

        
    </div>
    <a class="headerlink" href="#StackedRNN.outputBias"></a>
    
    

                            </div>
                            <div id="StackedRNN.hiddenStates" class="classattr">
                                <div class="attr variable">
            <span class="name">hiddenStates</span>

        
    </div>
    <a class="headerlink" href="#StackedRNN.hiddenStates"></a>
    
    

                            </div>
                            <div id="StackedRNN.hiddenStateActivationFunction" class="classattr">
                                <div class="attr variable">
            <span class="name">hiddenStateActivationFunction</span>

        
    </div>
    <a class="headerlink" href="#StackedRNN.hiddenStateActivationFunction"></a>
    
    

                            </div>
                            <div id="StackedRNN.outputLayerActivationFunction" class="classattr">
                                <div class="attr variable">
            <span class="name">outputLayerActivationFunction</span>

        
    </div>
    <a class="headerlink" href="#StackedRNN.outputLayerActivationFunction"></a>
    
    

                            </div>
                            <div id="StackedRNN.activationDerivative" class="classattr">
                                <div class="attr variable">
            <span class="name">activationDerivative</span>

        
    </div>
    <a class="headerlink" href="#StackedRNN.activationDerivative"></a>
    
    

                            </div>
                            <div id="StackedRNN.outputLayerDerivative" class="classattr">
                                <div class="attr variable">
            <span class="name">outputLayerDerivative</span>

        
    </div>
    <a class="headerlink" href="#StackedRNN.outputLayerDerivative"></a>
    
    

                            </div>
                            <div id="StackedRNN.lossFunction" class="classattr">
                                <div class="attr variable">
            <span class="name">lossFunction</span>

        
    </div>
    <a class="headerlink" href="#StackedRNN.lossFunction"></a>
    
    

                            </div>
                            <div id="StackedRNN.lossDerivative" class="classattr">
                                <div class="attr variable">
            <span class="name">lossDerivative</span>

        
    </div>
    <a class="headerlink" href="#StackedRNN.lossDerivative"></a>
    
    

                            </div>
                            <div id="StackedRNN.useBatches" class="classattr">
                                <div class="attr variable">
            <span class="name">useBatches</span>

        
    </div>
    <a class="headerlink" href="#StackedRNN.useBatches"></a>
    
    

                            </div>
                            <div id="StackedRNN.numberOfHiddenStates" class="classattr">
                                <div class="attr variable">
            <span class="name">numberOfHiddenStates</span>

        
    </div>
    <a class="headerlink" href="#StackedRNN.numberOfHiddenStates"></a>
    
    

                            </div>
                            <div id="StackedRNN.hiddenSizes" class="classattr">
                                <div class="attr variable">
            <span class="name">hiddenSizes</span>

        
    </div>
    <a class="headerlink" href="#StackedRNN.hiddenSizes"></a>
    
    

                            </div>
                            <div id="StackedRNN.adam" class="classattr">
                                <div class="attr variable">
            <span class="name">adam</span>

        
    </div>
    <a class="headerlink" href="#StackedRNN.adam"></a>
    
    

                            </div>
                            <div id="StackedRNN.forwardSequence" class="classattr">
                                        <input id="StackedRNN.forwardSequence-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forwardSequence</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">inputData</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="StackedRNN.forwardSequence-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#StackedRNN.forwardSequence"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="StackedRNN.forwardSequence-90"><a href="#StackedRNN.forwardSequence-90"><span class="linenos"> 90</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forwardSequence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">):</span> <span class="c1"># goes through the whole sequence / time steps</span>
</span><span id="StackedRNN.forwardSequence-91"><a href="#StackedRNN.forwardSequence-91"><span class="linenos"> 91</span></a>        <span class="k">assert</span> <span class="n">inputData</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Input data isnt batched&quot;</span>
</span><span id="StackedRNN.forwardSequence-92"><a href="#StackedRNN.forwardSequence-92"><span class="linenos"> 92</span></a>        <span class="k">assert</span> <span class="n">inputData</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Input data isnt 3D (batchsize, sequenceLength, inputSize)&quot;</span>
</span><span id="StackedRNN.forwardSequence-93"><a href="#StackedRNN.forwardSequence-93"><span class="linenos"> 93</span></a>        <span class="n">preActivations</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="StackedRNN.forwardSequence-94"><a href="#StackedRNN.forwardSequence-94"><span class="linenos"> 94</span></a>        <span class="n">allHiddenStates</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="StackedRNN.forwardSequence-95"><a href="#StackedRNN.forwardSequence-95"><span class="linenos"> 95</span></a>        <span class="n">sequenceLength</span> <span class="o">=</span> <span class="n">inputData</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="StackedRNN.forwardSequence-96"><a href="#StackedRNN.forwardSequence-96"><span class="linenos"> 96</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">sequenceLength</span><span class="p">):</span>
</span><span id="StackedRNN.forwardSequence-97"><a href="#StackedRNN.forwardSequence-97"><span class="linenos"> 97</span></a>            <span class="n">xi</span> <span class="o">=</span> <span class="n">inputData</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:]</span>
</span><span id="StackedRNN.forwardSequence-98"><a href="#StackedRNN.forwardSequence-98"><span class="linenos"> 98</span></a>            <span class="n">allPreAct</span><span class="p">,</span> <span class="n">outputPreAct</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">allHidenStates</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_oneStep</span><span class="p">(</span><span class="n">xi</span><span class="p">)</span>
</span><span id="StackedRNN.forwardSequence-99"><a href="#StackedRNN.forwardSequence-99"><span class="linenos"> 99</span></a>            <span class="n">preActivations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">allPreAct</span><span class="p">)</span>
</span><span id="StackedRNN.forwardSequence-100"><a href="#StackedRNN.forwardSequence-100"><span class="linenos">100</span></a>            <span class="n">allHiddenStates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">allHidenStates</span><span class="p">)</span>
</span><span id="StackedRNN.forwardSequence-101"><a href="#StackedRNN.forwardSequence-101"><span class="linenos">101</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">preActivations</span> <span class="o">=</span> <span class="n">preActivations</span>
</span><span id="StackedRNN.forwardSequence-102"><a href="#StackedRNN.forwardSequence-102"><span class="linenos">102</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">allHiddenStates</span> <span class="o">=</span> <span class="n">allHiddenStates</span>
</span><span id="StackedRNN.forwardSequence-103"><a href="#StackedRNN.forwardSequence-103"><span class="linenos">103</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputPreAct</span> <span class="o">=</span> <span class="n">outputPreAct</span>
</span><span id="StackedRNN.forwardSequence-104"><a href="#StackedRNN.forwardSequence-104"><span class="linenos">104</span></a>        <span class="k">return</span> <span class="n">output</span>
</span></pre></div>


    

                            </div>
                            <div id="StackedRNN.initialiseWeights" class="classattr">
                                        <input id="StackedRNN.initialiseWeights-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">initialiseWeights</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">inputSize</span>, </span><span class="param"><span class="n">outputSize</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="StackedRNN.initialiseWeights-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#StackedRNN.initialiseWeights"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="StackedRNN.initialiseWeights-142"><a href="#StackedRNN.initialiseWeights-142"><span class="linenos">142</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">initialiseWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputSize</span><span class="p">,</span> <span class="n">outputSize</span><span class="p">):</span>
</span><span id="StackedRNN.initialiseWeights-143"><a href="#StackedRNN.initialiseWeights-143"><span class="linenos">143</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="StackedRNN.initialiseWeights-144"><a href="#StackedRNN.initialiseWeights-144"><span class="linenos">144</span></a><span class="sd">        Initializes weights, biases, and hidden states for each hidden layer and the output layer.</span>
</span><span id="StackedRNN.initialiseWeights-145"><a href="#StackedRNN.initialiseWeights-145"><span class="linenos">145</span></a>
</span><span id="StackedRNN.initialiseWeights-146"><a href="#StackedRNN.initialiseWeights-146"><span class="linenos">146</span></a><span class="sd">        Args:</span>
</span><span id="StackedRNN.initialiseWeights-147"><a href="#StackedRNN.initialiseWeights-147"><span class="linenos">147</span></a><span class="sd">            inputSize (int): Size of the input feature vector.</span>
</span><span id="StackedRNN.initialiseWeights-148"><a href="#StackedRNN.initialiseWeights-148"><span class="linenos">148</span></a><span class="sd">            outputSize (int): Size of the output vector.</span>
</span><span id="StackedRNN.initialiseWeights-149"><a href="#StackedRNN.initialiseWeights-149"><span class="linenos">149</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="StackedRNN.initialiseWeights-150"><a href="#StackedRNN.initialiseWeights-150"><span class="linenos">150</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">inputWeights</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="StackedRNN.initialiseWeights-151"><a href="#StackedRNN.initialiseWeights-151"><span class="linenos">151</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hiddenWeights</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="StackedRNN.initialiseWeights-152"><a href="#StackedRNN.initialiseWeights-152"><span class="linenos">152</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="StackedRNN.initialiseWeights-153"><a href="#StackedRNN.initialiseWeights-153"><span class="linenos">153</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hiddenStates</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="StackedRNN.initialiseWeights-154"><a href="#StackedRNN.initialiseWeights-154"><span class="linenos">154</span></a>
</span><span id="StackedRNN.initialiseWeights-155"><a href="#StackedRNN.initialiseWeights-155"><span class="linenos">155</span></a>        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">hiddenSize</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hiddenSizes</span><span class="p">):</span>
</span><span id="StackedRNN.initialiseWeights-156"><a href="#StackedRNN.initialiseWeights-156"><span class="linenos">156</span></a>            <span class="n">inSize</span> <span class="o">=</span> <span class="n">inputSize</span> 
</span><span id="StackedRNN.initialiseWeights-157"><a href="#StackedRNN.initialiseWeights-157"><span class="linenos">157</span></a>            <span class="k">if</span><span class="p">(</span><span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">):</span>
</span><span id="StackedRNN.initialiseWeights-158"><a href="#StackedRNN.initialiseWeights-158"><span class="linenos">158</span></a>                <span class="n">inSize</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hiddenSizes</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
</span><span id="StackedRNN.initialiseWeights-159"><a href="#StackedRNN.initialiseWeights-159"><span class="linenos">159</span></a>
</span><span id="StackedRNN.initialiseWeights-160"><a href="#StackedRNN.initialiseWeights-160"><span class="linenos">160</span></a>            <span class="n">inputW</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialiseWeights</span><span class="p">(</span><span class="n">hiddenSize</span><span class="p">,</span> <span class="n">inSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hiddenStateActivationFunction</span><span class="p">)</span>
</span><span id="StackedRNN.initialiseWeights-161"><a href="#StackedRNN.initialiseWeights-161"><span class="linenos">161</span></a>            <span class="n">hiddenW</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialiseWeights</span><span class="p">(</span><span class="n">hiddenSize</span><span class="p">,</span> <span class="n">hiddenSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hiddenStateActivationFunction</span><span class="p">)</span>
</span><span id="StackedRNN.initialiseWeights-162"><a href="#StackedRNN.initialiseWeights-162"><span class="linenos">162</span></a>            <span class="n">bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">hiddenSize</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span><span id="StackedRNN.initialiseWeights-163"><a href="#StackedRNN.initialiseWeights-163"><span class="linenos">163</span></a>            <span class="n">hiddenState</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span> <span class="n">hiddenSize</span><span class="p">))</span>
</span><span id="StackedRNN.initialiseWeights-164"><a href="#StackedRNN.initialiseWeights-164"><span class="linenos">164</span></a>        
</span><span id="StackedRNN.initialiseWeights-165"><a href="#StackedRNN.initialiseWeights-165"><span class="linenos">165</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">inputWeights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inputW</span><span class="p">)</span>
</span><span id="StackedRNN.initialiseWeights-166"><a href="#StackedRNN.initialiseWeights-166"><span class="linenos">166</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">hiddenWeights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">hiddenW</span><span class="p">)</span>
</span><span id="StackedRNN.initialiseWeights-167"><a href="#StackedRNN.initialiseWeights-167"><span class="linenos">167</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bias</span><span class="p">)</span>
</span><span id="StackedRNN.initialiseWeights-168"><a href="#StackedRNN.initialiseWeights-168"><span class="linenos">168</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">hiddenStates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">hiddenState</span><span class="p">)</span>
</span><span id="StackedRNN.initialiseWeights-169"><a href="#StackedRNN.initialiseWeights-169"><span class="linenos">169</span></a>
</span><span id="StackedRNN.initialiseWeights-170"><a href="#StackedRNN.initialiseWeights-170"><span class="linenos">170</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputWeight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialiseWeights</span><span class="p">(</span><span class="n">outputSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hiddenSizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputLayerActivationFunction</span><span class="p">)</span>
</span><span id="StackedRNN.initialiseWeights-171"><a href="#StackedRNN.initialiseWeights-171"><span class="linenos">171</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputBias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">outputSize</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span><span id="StackedRNN.initialiseWeights-172"><a href="#StackedRNN.initialiseWeights-172"><span class="linenos">172</span></a>    
</span><span id="StackedRNN.initialiseWeights-173"><a href="#StackedRNN.initialiseWeights-173"><span class="linenos">173</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">inputSize</span> <span class="o">=</span> <span class="n">inputSize</span>
</span><span id="StackedRNN.initialiseWeights-174"><a href="#StackedRNN.initialiseWeights-174"><span class="linenos">174</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputSize</span> <span class="o">=</span> <span class="n">outputSize</span>
</span></pre></div>


            <div class="docstring"><p>Initializes weights, biases, and hidden states for each hidden layer and the output layer.</p>

<p>Args:
    inputSize (int): Size of the input feature vector.
    outputSize (int): Size of the output vector.</p>
</div>


                            </div>
                            <div id="StackedRNN.backwardPropagation" class="classattr">
                                        <input id="StackedRNN.backwardPropagation-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">backwardPropagation</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">inputs</span>, </span><span class="param"><span class="n">outputs</span>, </span><span class="param"><span class="n">targets</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="StackedRNN.backwardPropagation-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#StackedRNN.backwardPropagation"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="StackedRNN.backwardPropagation-176"><a href="#StackedRNN.backwardPropagation-176"><span class="linenos">176</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">backwardPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
</span><span id="StackedRNN.backwardPropagation-177"><a href="#StackedRNN.backwardPropagation-177"><span class="linenos">177</span></a>        <span class="n">inputWeightGradients</span><span class="p">,</span> <span class="n">hiddenStateWeightGradients</span><span class="p">,</span> <span class="n">biasGradients</span><span class="p">,</span> <span class="n">outputWeightGradients</span><span class="p">,</span> <span class="n">outputbiasGradients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Stacked_BPTT</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">allHiddenStates</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">preActivations</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputPreAct</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
</span><span id="StackedRNN.backwardPropagation-178"><a href="#StackedRNN.backwardPropagation-178"><span class="linenos">178</span></a>        <span class="n">Parameters</span> <span class="o">=</span>  <span class="p">{</span>
</span><span id="StackedRNN.backwardPropagation-179"><a href="#StackedRNN.backwardPropagation-179"><span class="linenos">179</span></a>            <span class="s2">&quot;I_W&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputWeights</span><span class="p">,</span>
</span><span id="StackedRNN.backwardPropagation-180"><a href="#StackedRNN.backwardPropagation-180"><span class="linenos">180</span></a>            <span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">,</span>
</span><span id="StackedRNN.backwardPropagation-181"><a href="#StackedRNN.backwardPropagation-181"><span class="linenos">181</span></a>            <span class="s2">&quot;H_W&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">hiddenWeights</span><span class="p">,</span>
</span><span id="StackedRNN.backwardPropagation-182"><a href="#StackedRNN.backwardPropagation-182"><span class="linenos">182</span></a>            <span class="s2">&quot;O_W&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputWeight</span><span class="p">,</span>
</span><span id="StackedRNN.backwardPropagation-183"><a href="#StackedRNN.backwardPropagation-183"><span class="linenos">183</span></a>            <span class="s2">&quot;O_b&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputBias</span><span class="p">,</span>
</span><span id="StackedRNN.backwardPropagation-184"><a href="#StackedRNN.backwardPropagation-184"><span class="linenos">184</span></a>        <span class="p">}</span>
</span><span id="StackedRNN.backwardPropagation-185"><a href="#StackedRNN.backwardPropagation-185"><span class="linenos">185</span></a>  
</span><span id="StackedRNN.backwardPropagation-186"><a href="#StackedRNN.backwardPropagation-186"><span class="linenos">186</span></a>        <span class="n">Gradients</span> <span class="o">=</span>  <span class="p">{</span>
</span><span id="StackedRNN.backwardPropagation-187"><a href="#StackedRNN.backwardPropagation-187"><span class="linenos">187</span></a>            <span class="s2">&quot;I_W&quot;</span><span class="p">:</span> <span class="n">inputWeightGradients</span><span class="p">,</span>
</span><span id="StackedRNN.backwardPropagation-188"><a href="#StackedRNN.backwardPropagation-188"><span class="linenos">188</span></a>            <span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="n">biasGradients</span><span class="p">,</span>
</span><span id="StackedRNN.backwardPropagation-189"><a href="#StackedRNN.backwardPropagation-189"><span class="linenos">189</span></a>            <span class="s2">&quot;H_W&quot;</span><span class="p">:</span> <span class="n">hiddenStateWeightGradients</span><span class="p">,</span>
</span><span id="StackedRNN.backwardPropagation-190"><a href="#StackedRNN.backwardPropagation-190"><span class="linenos">190</span></a>            <span class="s2">&quot;O_W&quot;</span><span class="p">:</span> <span class="n">outputWeightGradients</span><span class="p">,</span>
</span><span id="StackedRNN.backwardPropagation-191"><a href="#StackedRNN.backwardPropagation-191"><span class="linenos">191</span></a>            <span class="s2">&quot;O_b&quot;</span><span class="p">:</span> <span class="n">outputbiasGradients</span><span class="p">,</span>
</span><span id="StackedRNN.backwardPropagation-192"><a href="#StackedRNN.backwardPropagation-192"><span class="linenos">192</span></a>        <span class="p">}</span>
</span><span id="StackedRNN.backwardPropagation-193"><a href="#StackedRNN.backwardPropagation-193"><span class="linenos">193</span></a>        <span class="k">return</span> <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span> 
</span></pre></div>


    

                            </div>
                            <div id="StackedRNN.optimiser" class="classattr">
                                        <input id="StackedRNN.optimiser-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">optimiser</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">inputData</span>, </span><span class="param"><span class="n">labels</span>, </span><span class="param"><span class="n">alpha</span>, </span><span class="param"><span class="n">beta1</span>, </span><span class="param"><span class="n">beta2</span>, </span><span class="param"><span class="n">epsilon</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="StackedRNN.optimiser-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#StackedRNN.optimiser"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="StackedRNN.optimiser-195"><a href="#StackedRNN.optimiser-195"><span class="linenos">195</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">optimiser</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">):</span>
</span><span id="StackedRNN.optimiser-196"><a href="#StackedRNN.optimiser-196"><span class="linenos">196</span></a>        <span class="n">AllOutputs</span><span class="p">,</span> <span class="n">Parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adam</span><span class="o">.</span><span class="n">optimiser</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">useBatches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
</span><span id="StackedRNN.optimiser-197"><a href="#StackedRNN.optimiser-197"><span class="linenos">197</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">inputWeights</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;I_W&quot;</span><span class="p">]</span>
</span><span id="StackedRNN.optimiser-198"><a href="#StackedRNN.optimiser-198"><span class="linenos">198</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">]</span>
</span><span id="StackedRNN.optimiser-199"><a href="#StackedRNN.optimiser-199"><span class="linenos">199</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hiddenWeights</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;H_W&quot;</span><span class="p">]</span>
</span><span id="StackedRNN.optimiser-200"><a href="#StackedRNN.optimiser-200"><span class="linenos">200</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputWeight</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;O_W&quot;</span><span class="p">]</span>
</span><span id="StackedRNN.optimiser-201"><a href="#StackedRNN.optimiser-201"><span class="linenos">201</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputBias</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;O_b&quot;</span><span class="p">]</span>
</span><span id="StackedRNN.optimiser-202"><a href="#StackedRNN.optimiser-202"><span class="linenos">202</span></a>        <span class="k">return</span> <span class="n">AllOutputs</span>
</span></pre></div>


    

                            </div>
                            <div id="StackedRNN.train" class="classattr">
                                        <input id="StackedRNN.train-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">train</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">inputData</span>,</span><span class="param">	<span class="n">labels</span>,</span><span class="param">	<span class="n">alpha</span><span class="o">=</span><span class="mf">0.001</span>,</span><span class="param">	<span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span>,</span><span class="param">	<span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span>,</span><span class="param">	<span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-08</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="StackedRNN.train-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#StackedRNN.train"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="StackedRNN.train-204"><a href="#StackedRNN.train-204"><span class="linenos">204</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span> <span class="n">beta1</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span> <span class="n">beta2</span> <span class="o">=</span> <span class="mf">0.999</span><span class="p">,</span> <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">):</span>
</span><span id="StackedRNN.train-205"><a href="#StackedRNN.train-205"><span class="linenos">205</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="StackedRNN.train-206"><a href="#StackedRNN.train-206"><span class="linenos">206</span></a><span class="sd">        Trains the model on the given input data and labels using the Adam optimizer,</span>
</span><span id="StackedRNN.train-207"><a href="#StackedRNN.train-207"><span class="linenos">207</span></a><span class="sd">        then calculates and returns the loss.</span>
</span><span id="StackedRNN.train-208"><a href="#StackedRNN.train-208"><span class="linenos">208</span></a>
</span><span id="StackedRNN.train-209"><a href="#StackedRNN.train-209"><span class="linenos">209</span></a><span class="sd">        Args:</span>
</span><span id="StackedRNN.train-210"><a href="#StackedRNN.train-210"><span class="linenos">210</span></a><span class="sd">            inputData (ndarray): 3D array of input sequences with shape (batchSize, sequenceLength, inputSize).</span>
</span><span id="StackedRNN.train-211"><a href="#StackedRNN.train-211"><span class="linenos">211</span></a><span class="sd">            labels (ndarray): True labels corresponding to the input data.</span>
</span><span id="StackedRNN.train-212"><a href="#StackedRNN.train-212"><span class="linenos">212</span></a><span class="sd">            alpha (float, optional): Learning rate for the optimizer. Default is 0.001.</span>
</span><span id="StackedRNN.train-213"><a href="#StackedRNN.train-213"><span class="linenos">213</span></a><span class="sd">            beta1 (float, optional): Beta1 parameter for Adam optimizer. Default is 0.9.</span>
</span><span id="StackedRNN.train-214"><a href="#StackedRNN.train-214"><span class="linenos">214</span></a><span class="sd">            beta2 (float, optional): Beta2 parameter for Adam optimizer. Default is 0.999.</span>
</span><span id="StackedRNN.train-215"><a href="#StackedRNN.train-215"><span class="linenos">215</span></a><span class="sd">            epsilon (float, optional): Epsilon parameter for Adam optimizer to avoid division by zero. Default is 1e-8.</span>
</span><span id="StackedRNN.train-216"><a href="#StackedRNN.train-216"><span class="linenos">216</span></a>
</span><span id="StackedRNN.train-217"><a href="#StackedRNN.train-217"><span class="linenos">217</span></a><span class="sd">        Returns:</span>
</span><span id="StackedRNN.train-218"><a href="#StackedRNN.train-218"><span class="linenos">218</span></a><span class="sd">            float: Calculated loss between the model output and true labels.</span>
</span><span id="StackedRNN.train-219"><a href="#StackedRNN.train-219"><span class="linenos">219</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="StackedRNN.train-220"><a href="#StackedRNN.train-220"><span class="linenos">220</span></a>        <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputData</span><span class="p">)</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Dimension wrong size, got </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputData</span><span class="p">)</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">, expected 3&quot;</span>
</span><span id="StackedRNN.train-221"><a href="#StackedRNN.train-221"><span class="linenos">221</span></a>        <span class="n">AllOutputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimiser</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
</span><span id="StackedRNN.train-222"><a href="#StackedRNN.train-222"><span class="linenos">222</span></a>        <span class="n">AllOutputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">AllOutputs</span><span class="p">,</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</span><span id="StackedRNN.train-223"><a href="#StackedRNN.train-223"><span class="linenos">223</span></a>        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lossFunction</span><span class="p">(</span><span class="n">AllOutputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</span><span id="StackedRNN.train-224"><a href="#StackedRNN.train-224"><span class="linenos">224</span></a>        <span class="k">return</span> <span class="n">loss</span>
</span></pre></div>


            <div class="docstring"><p>Trains the model on the given input data and labels using the Adam optimizer,
then calculates and returns the loss.</p>

<p>Args:
    inputData (ndarray): 3D array of input sequences with shape (batchSize, sequenceLength, inputSize).
    labels (ndarray): True labels corresponding to the input data.
    alpha (float, optional): Learning rate for the optimizer. Default is 0.001.
    beta1 (float, optional): Beta1 parameter for Adam optimizer. Default is 0.9.
    beta2 (float, optional): Beta2 parameter for Adam optimizer. Default is 0.999.
    epsilon (float, optional): Epsilon parameter for Adam optimizer to avoid division by zero. Default is 1e-8.</p>

<p>Returns:
    float: Calculated loss between the model output and true labels.</p>
</div>


                            </div>
                </section>
                <section id="SingularRNN">
                            <input id="SingularRNN-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">SingularRNN</span><wbr>(<span class="base">quacknet.RNN.Singular.SingularBackPropRNN.RNNBackProp</span>):

                <label class="view-source-button" for="SingularRNN-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#SingularRNN"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="SingularRNN-17"><a href="#SingularRNN-17"><span class="linenos"> 17</span></a><span class="k">class</span><span class="w"> </span><span class="nc">SingularRNN</span><span class="p">(</span><span class="n">RNNBackProp</span><span class="p">):</span> 
</span><span id="SingularRNN-18"><a href="#SingularRNN-18"><span class="linenos"> 18</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hiddenStateActivationFunction</span><span class="p">,</span> <span class="n">outputLayerActivationFunction</span><span class="p">,</span> <span class="n">lossFunction</span><span class="p">,</span> <span class="n">useBatches</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">batchSize</span> <span class="o">=</span> <span class="mi">64</span><span class="p">):</span>
</span><span id="SingularRNN-19"><a href="#SingularRNN-19"><span class="linenos"> 19</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="SingularRNN-20"><a href="#SingularRNN-20"><span class="linenos"> 20</span></a><span class="sd">        Initializes the RNN model by setting activation functions, loss function,</span>
</span><span id="SingularRNN-21"><a href="#SingularRNN-21"><span class="linenos"> 21</span></a><span class="sd">        batching options, and creating the Adam optimizer instance.</span>
</span><span id="SingularRNN-22"><a href="#SingularRNN-22"><span class="linenos"> 22</span></a>
</span><span id="SingularRNN-23"><a href="#SingularRNN-23"><span class="linenos"> 23</span></a><span class="sd">        Args:</span>
</span><span id="SingularRNN-24"><a href="#SingularRNN-24"><span class="linenos"> 24</span></a><span class="sd">            hiddenStateActivationFunction (str): Name of activation function for hidden states (e.g., &quot;relu&quot;, &quot;sigmoid&quot;).</span>
</span><span id="SingularRNN-25"><a href="#SingularRNN-25"><span class="linenos"> 25</span></a><span class="sd">            outputLayerActivationFunction (str): Name of activation function for output layer.</span>
</span><span id="SingularRNN-26"><a href="#SingularRNN-26"><span class="linenos"> 26</span></a><span class="sd">            lossFunction (str): Name of the loss function to use (e.g., &quot;mse&quot;, &quot;mae&quot;, &quot;cross entropy&quot;).</span>
</span><span id="SingularRNN-27"><a href="#SingularRNN-27"><span class="linenos"> 27</span></a><span class="sd">            useBatches (bool, optional): Whether to use batching during training. Default is False.</span>
</span><span id="SingularRNN-28"><a href="#SingularRNN-28"><span class="linenos"> 28</span></a><span class="sd">            batchSize (int, optional): Batch size if batching is enabled. Default is 64. </span>
</span><span id="SingularRNN-29"><a href="#SingularRNN-29"><span class="linenos"> 29</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="SingularRNN-30"><a href="#SingularRNN-30"><span class="linenos"> 30</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">inputWeight</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="SingularRNN-31"><a href="#SingularRNN-31"><span class="linenos"> 31</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hiddenWeight</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="SingularRNN-32"><a href="#SingularRNN-32"><span class="linenos"> 32</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="SingularRNN-33"><a href="#SingularRNN-33"><span class="linenos"> 33</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputWeight</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="SingularRNN-34"><a href="#SingularRNN-34"><span class="linenos"> 34</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputBias</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="SingularRNN-35"><a href="#SingularRNN-35"><span class="linenos"> 35</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hiddenState</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="SingularRNN-36"><a href="#SingularRNN-36"><span class="linenos"> 36</span></a>        
</span><span id="SingularRNN-37"><a href="#SingularRNN-37"><span class="linenos"> 37</span></a>        <span class="n">funcs</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="SingularRNN-38"><a href="#SingularRNN-38"><span class="linenos"> 38</span></a>            <span class="s2">&quot;relu&quot;</span><span class="p">:</span> <span class="n">relu</span><span class="p">,</span>
</span><span id="SingularRNN-39"><a href="#SingularRNN-39"><span class="linenos"> 39</span></a>            <span class="s2">&quot;sigmoid&quot;</span><span class="p">:</span> <span class="n">sigmoid</span><span class="p">,</span>
</span><span id="SingularRNN-40"><a href="#SingularRNN-40"><span class="linenos"> 40</span></a>            <span class="s2">&quot;linear&quot;</span><span class="p">:</span> <span class="n">linear</span><span class="p">,</span>
</span><span id="SingularRNN-41"><a href="#SingularRNN-41"><span class="linenos"> 41</span></a>            <span class="s2">&quot;tanh&quot;</span><span class="p">:</span> <span class="n">tanH</span><span class="p">,</span>
</span><span id="SingularRNN-42"><a href="#SingularRNN-42"><span class="linenos"> 42</span></a>            <span class="s2">&quot;softmax&quot;</span><span class="p">:</span> <span class="n">softMax</span><span class="p">,</span>
</span><span id="SingularRNN-43"><a href="#SingularRNN-43"><span class="linenos"> 43</span></a>        <span class="p">}</span>
</span><span id="SingularRNN-44"><a href="#SingularRNN-44"><span class="linenos"> 44</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">hiddenStateActivationFunction</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">funcs</span><span class="p">):</span>
</span><span id="SingularRNN-45"><a href="#SingularRNN-45"><span class="linenos"> 45</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Activation function not made: </span><span class="si">{</span><span class="n">hiddenStateActivationFunction</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="SingularRNN-46"><a href="#SingularRNN-46"><span class="linenos"> 46</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">outputLayerActivationFunction</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">funcs</span><span class="p">):</span>
</span><span id="SingularRNN-47"><a href="#SingularRNN-47"><span class="linenos"> 47</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Activation function not made: </span><span class="si">{</span><span class="n">outputLayerActivationFunction</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="SingularRNN-48"><a href="#SingularRNN-48"><span class="linenos"> 48</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hiddenStateActivationFunction</span> <span class="o">=</span> <span class="n">funcs</span><span class="p">[</span><span class="n">hiddenStateActivationFunction</span><span class="o">.</span><span class="n">lower</span><span class="p">()]</span>
</span><span id="SingularRNN-49"><a href="#SingularRNN-49"><span class="linenos"> 49</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputLayerActivationFunction</span> <span class="o">=</span> <span class="n">funcs</span><span class="p">[</span><span class="n">outputLayerActivationFunction</span><span class="o">.</span><span class="n">lower</span><span class="p">()]</span>
</span><span id="SingularRNN-50"><a href="#SingularRNN-50"><span class="linenos"> 50</span></a>
</span><span id="SingularRNN-51"><a href="#SingularRNN-51"><span class="linenos"> 51</span></a>        <span class="n">derivs</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="SingularRNN-52"><a href="#SingularRNN-52"><span class="linenos"> 52</span></a>            <span class="n">relu</span><span class="p">:</span> <span class="n">ReLUDerivative</span><span class="p">,</span>
</span><span id="SingularRNN-53"><a href="#SingularRNN-53"><span class="linenos"> 53</span></a>            <span class="n">sigmoid</span><span class="p">:</span> <span class="n">SigmoidDerivative</span><span class="p">,</span>
</span><span id="SingularRNN-54"><a href="#SingularRNN-54"><span class="linenos"> 54</span></a>            <span class="n">linear</span><span class="p">:</span> <span class="n">LinearDerivative</span><span class="p">,</span>
</span><span id="SingularRNN-55"><a href="#SingularRNN-55"><span class="linenos"> 55</span></a>            <span class="n">tanH</span><span class="p">:</span> <span class="n">TanHDerivative</span><span class="p">,</span>
</span><span id="SingularRNN-56"><a href="#SingularRNN-56"><span class="linenos"> 56</span></a>            <span class="n">softMax</span><span class="p">:</span> <span class="n">SoftMaxDerivative</span><span class="p">,</span>
</span><span id="SingularRNN-57"><a href="#SingularRNN-57"><span class="linenos"> 57</span></a>        <span class="p">}</span>
</span><span id="SingularRNN-58"><a href="#SingularRNN-58"><span class="linenos"> 58</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activationDerivative</span> <span class="o">=</span> <span class="n">derivs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">hiddenStateActivationFunction</span><span class="p">]</span>
</span><span id="SingularRNN-59"><a href="#SingularRNN-59"><span class="linenos"> 59</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputLayerDerivative</span> <span class="o">=</span> <span class="n">derivs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">outputLayerActivationFunction</span><span class="p">]</span>
</span><span id="SingularRNN-60"><a href="#SingularRNN-60"><span class="linenos"> 60</span></a>
</span><span id="SingularRNN-61"><a href="#SingularRNN-61"><span class="linenos"> 61</span></a>        <span class="n">lossFunctionDict</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="SingularRNN-62"><a href="#SingularRNN-62"><span class="linenos"> 62</span></a>            <span class="s2">&quot;mse&quot;</span><span class="p">:</span> <span class="n">MSELossFunction</span><span class="p">,</span>
</span><span id="SingularRNN-63"><a href="#SingularRNN-63"><span class="linenos"> 63</span></a>            <span class="s2">&quot;mae&quot;</span><span class="p">:</span> <span class="n">MAELossFunction</span><span class="p">,</span>
</span><span id="SingularRNN-64"><a href="#SingularRNN-64"><span class="linenos"> 64</span></a>            <span class="s2">&quot;cross entropy&quot;</span><span class="p">:</span> <span class="n">CrossEntropyLossFunction</span><span class="p">,</span><span class="s2">&quot;cross&quot;</span><span class="p">:</span> <span class="n">CrossEntropyLossFunction</span><span class="p">,</span>
</span><span id="SingularRNN-65"><a href="#SingularRNN-65"><span class="linenos"> 65</span></a>        <span class="p">}</span>
</span><span id="SingularRNN-66"><a href="#SingularRNN-66"><span class="linenos"> 66</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">lossFunction</span> <span class="o">=</span> <span class="n">lossFunctionDict</span><span class="p">[</span><span class="n">lossFunction</span><span class="o">.</span><span class="n">lower</span><span class="p">()]</span>
</span><span id="SingularRNN-67"><a href="#SingularRNN-67"><span class="linenos"> 67</span></a>        <span class="n">lossDerivs</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="SingularRNN-68"><a href="#SingularRNN-68"><span class="linenos"> 68</span></a>            <span class="n">MSELossFunction</span><span class="p">:</span> <span class="n">MSEDerivative</span><span class="p">,</span>
</span><span id="SingularRNN-69"><a href="#SingularRNN-69"><span class="linenos"> 69</span></a>            <span class="n">MAELossFunction</span><span class="p">:</span> <span class="n">MAEDerivative</span><span class="p">,</span>
</span><span id="SingularRNN-70"><a href="#SingularRNN-70"><span class="linenos"> 70</span></a>            <span class="n">CrossEntropyLossFunction</span><span class="p">:</span> <span class="n">CrossEntropyLossDerivative</span><span class="p">,</span>
</span><span id="SingularRNN-71"><a href="#SingularRNN-71"><span class="linenos"> 71</span></a>        <span class="p">}</span>
</span><span id="SingularRNN-72"><a href="#SingularRNN-72"><span class="linenos"> 72</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">lossDerivative</span> <span class="o">=</span> <span class="n">lossDerivs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">lossFunction</span><span class="p">]</span>
</span><span id="SingularRNN-73"><a href="#SingularRNN-73"><span class="linenos"> 73</span></a>
</span><span id="SingularRNN-74"><a href="#SingularRNN-74"><span class="linenos"> 74</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">useBatches</span> <span class="o">=</span> <span class="n">useBatches</span>
</span><span id="SingularRNN-75"><a href="#SingularRNN-75"><span class="linenos"> 75</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span> <span class="o">=</span> <span class="n">batchSize</span>
</span><span id="SingularRNN-76"><a href="#SingularRNN-76"><span class="linenos"> 76</span></a>
</span><span id="SingularRNN-77"><a href="#SingularRNN-77"><span class="linenos"> 77</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">adam</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">forwardSequence</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">backwardPropagation</span><span class="p">,</span> <span class="n">giveInputsToBackprop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="SingularRNN-78"><a href="#SingularRNN-78"><span class="linenos"> 78</span></a>
</span><span id="SingularRNN-79"><a href="#SingularRNN-79"><span class="linenos"> 79</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forwardSequence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">):</span> <span class="c1"># goes through the whole sequence / time steps</span>
</span><span id="SingularRNN-80"><a href="#SingularRNN-80"><span class="linenos"> 80</span></a>        <span class="n">batchSize</span><span class="p">,</span> <span class="n">sequenceLegth</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">inputData</span><span class="o">.</span><span class="n">shape</span>
</span><span id="SingularRNN-81"><a href="#SingularRNN-81"><span class="linenos"> 81</span></a>        
</span><span id="SingularRNN-82"><a href="#SingularRNN-82"><span class="linenos"> 82</span></a>        <span class="n">preActivations</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="SingularRNN-83"><a href="#SingularRNN-83"><span class="linenos"> 83</span></a>        <span class="n">allHiddenStates</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="SingularRNN-84"><a href="#SingularRNN-84"><span class="linenos"> 84</span></a>
</span><span id="SingularRNN-85"><a href="#SingularRNN-85"><span class="linenos"> 85</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">sequenceLegth</span><span class="p">):</span>
</span><span id="SingularRNN-86"><a href="#SingularRNN-86"><span class="linenos"> 86</span></a>            <span class="n">xi</span> <span class="o">=</span> <span class="n">inputData</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batchSize</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="SingularRNN-87"><a href="#SingularRNN-87"><span class="linenos"> 87</span></a>            <span class="n">preAct</span><span class="p">,</span> <span class="n">outputPreAct</span><span class="p">,</span> <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_oneStep</span><span class="p">(</span><span class="n">xi</span><span class="p">)</span>
</span><span id="SingularRNN-88"><a href="#SingularRNN-88"><span class="linenos"> 88</span></a>            <span class="n">preActivations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">preAct</span><span class="p">)</span>
</span><span id="SingularRNN-89"><a href="#SingularRNN-89"><span class="linenos"> 89</span></a>            <span class="n">allHiddenStates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hiddenState</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
</span><span id="SingularRNN-90"><a href="#SingularRNN-90"><span class="linenos"> 90</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">preActivations</span> <span class="o">=</span> <span class="n">preActivations</span>
</span><span id="SingularRNN-91"><a href="#SingularRNN-91"><span class="linenos"> 91</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">allHiddenStates</span> <span class="o">=</span> <span class="n">allHiddenStates</span>
</span><span id="SingularRNN-92"><a href="#SingularRNN-92"><span class="linenos"> 92</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputPreAct</span> <span class="o">=</span> <span class="n">outputPreAct</span>
</span><span id="SingularRNN-93"><a href="#SingularRNN-93"><span class="linenos"> 93</span></a>        <span class="k">return</span> <span class="n">output</span>
</span><span id="SingularRNN-94"><a href="#SingularRNN-94"><span class="linenos"> 94</span></a>
</span><span id="SingularRNN-95"><a href="#SingularRNN-95"><span class="linenos"> 95</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_oneStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">):</span> <span class="c1"># forward prop on 1 time step</span>
</span><span id="SingularRNN-96"><a href="#SingularRNN-96"><span class="linenos"> 96</span></a>        <span class="n">preActivation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hiddenState</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calculateHiddenLayer</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hiddenState</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputWeight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hiddenWeight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hiddenStateActivationFunction</span><span class="p">)</span>
</span><span id="SingularRNN-97"><a href="#SingularRNN-97"><span class="linenos"> 97</span></a>        <span class="n">preAct</span><span class="p">,</span> <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calculateOutputLayer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hiddenState</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputWeight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputBias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputLayerActivationFunction</span><span class="p">)</span>
</span><span id="SingularRNN-98"><a href="#SingularRNN-98"><span class="linenos"> 98</span></a>        <span class="k">return</span> <span class="n">preActivation</span><span class="p">,</span> <span class="n">preAct</span><span class="p">,</span> <span class="n">output</span>
</span><span id="SingularRNN-99"><a href="#SingularRNN-99"><span class="linenos"> 99</span></a>
</span><span id="SingularRNN-100"><a href="#SingularRNN-100"><span class="linenos">100</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_calculateHiddenLayer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">lastHiddenState</span><span class="p">,</span> <span class="n">inputWeight</span><span class="p">,</span> <span class="n">hiddenWeight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">activationFunction</span><span class="p">):</span> <span class="c1"># a( w_x * x + w_h * h + b )</span>
</span><span id="SingularRNN-101"><a href="#SingularRNN-101"><span class="linenos">101</span></a>        <span class="n">weighttedInp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputWeight</span><span class="p">,</span> <span class="n">inputData</span><span class="p">)</span>
</span><span id="SingularRNN-102"><a href="#SingularRNN-102"><span class="linenos">102</span></a>        <span class="n">weightedHidden</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">hiddenWeight</span><span class="p">,</span> <span class="n">lastHiddenState</span><span class="p">)</span>
</span><span id="SingularRNN-103"><a href="#SingularRNN-103"><span class="linenos">103</span></a>        <span class="n">biasBrodcast</span> <span class="o">=</span> <span class="n">bias</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="SingularRNN-104"><a href="#SingularRNN-104"><span class="linenos">104</span></a>        <span class="n">preActivation</span> <span class="o">=</span> <span class="n">weighttedInp</span> <span class="o">+</span> <span class="n">weightedHidden</span> <span class="o">+</span> <span class="n">biasBrodcast</span>
</span><span id="SingularRNN-105"><a href="#SingularRNN-105"><span class="linenos">105</span></a>        <span class="n">newHiddenState</span> <span class="o">=</span> <span class="n">activationFunction</span><span class="p">(</span><span class="n">preActivation</span><span class="p">)</span>
</span><span id="SingularRNN-106"><a href="#SingularRNN-106"><span class="linenos">106</span></a>        <span class="k">return</span> <span class="n">preActivation</span><span class="p">,</span> <span class="n">newHiddenState</span>
</span><span id="SingularRNN-107"><a href="#SingularRNN-107"><span class="linenos">107</span></a>
</span><span id="SingularRNN-108"><a href="#SingularRNN-108"><span class="linenos">108</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_calculateOutputLayer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">outputWeight</span><span class="p">,</span> <span class="n">outputBias</span><span class="p">,</span> <span class="n">activationFunction</span><span class="p">):</span> <span class="c1"># a( w_o * o + b_o)</span>
</span><span id="SingularRNN-109"><a href="#SingularRNN-109"><span class="linenos">109</span></a>        <span class="n">weigthedOutput</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">outputWeight</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
</span><span id="SingularRNN-110"><a href="#SingularRNN-110"><span class="linenos">110</span></a>        <span class="n">outputBiasBrodcast</span> <span class="o">=</span> <span class="n">outputBias</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="SingularRNN-111"><a href="#SingularRNN-111"><span class="linenos">111</span></a>        <span class="n">preActivation</span> <span class="o">=</span> <span class="n">weigthedOutput</span> <span class="o">+</span> <span class="n">outputBiasBrodcast</span>
</span><span id="SingularRNN-112"><a href="#SingularRNN-112"><span class="linenos">112</span></a>        <span class="n">output</span> <span class="o">=</span> <span class="n">activationFunction</span><span class="p">(</span><span class="n">preActivation</span><span class="p">)</span>
</span><span id="SingularRNN-113"><a href="#SingularRNN-113"><span class="linenos">113</span></a>        <span class="k">return</span> <span class="n">preActivation</span><span class="p">,</span> <span class="n">output</span>
</span><span id="SingularRNN-114"><a href="#SingularRNN-114"><span class="linenos">114</span></a>
</span><span id="SingularRNN-115"><a href="#SingularRNN-115"><span class="linenos">115</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_initialiseWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputSize</span><span class="p">,</span> <span class="n">inputSize</span><span class="p">,</span> <span class="n">activationFunction</span><span class="p">):</span>
</span><span id="SingularRNN-116"><a href="#SingularRNN-116"><span class="linenos">116</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">activationFunction</span> <span class="o">==</span> <span class="n">relu</span><span class="p">):</span>
</span><span id="SingularRNN-117"><a href="#SingularRNN-117"><span class="linenos">117</span></a>            <span class="n">bounds</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">inputSize</span><span class="p">)</span> <span class="c1"># He initialisation</span>
</span><span id="SingularRNN-118"><a href="#SingularRNN-118"><span class="linenos">118</span></a>        <span class="k">elif</span><span class="p">(</span><span class="n">activationFunction</span> <span class="o">==</span> <span class="n">sigmoid</span><span class="p">):</span>
</span><span id="SingularRNN-119"><a href="#SingularRNN-119"><span class="linenos">119</span></a>            <span class="n">bounds</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">6</span> <span class="o">/</span> <span class="p">(</span><span class="n">inputSize</span> <span class="o">+</span> <span class="n">outputSize</span><span class="p">))</span> <span class="c1"># Xavier initialisation</span>
</span><span id="SingularRNN-120"><a href="#SingularRNN-120"><span class="linenos">120</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="SingularRNN-121"><a href="#SingularRNN-121"><span class="linenos">121</span></a>            <span class="n">bounds</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">inputSize</span><span class="p">)</span> <span class="c1"># default</span>
</span><span id="SingularRNN-122"><a href="#SingularRNN-122"><span class="linenos">122</span></a>        <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">bounds</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">outputSize</span><span class="p">,</span> <span class="n">inputSize</span><span class="p">))</span>
</span><span id="SingularRNN-123"><a href="#SingularRNN-123"><span class="linenos">123</span></a>        <span class="k">return</span> <span class="n">w</span>
</span><span id="SingularRNN-124"><a href="#SingularRNN-124"><span class="linenos">124</span></a>    
</span><span id="SingularRNN-125"><a href="#SingularRNN-125"><span class="linenos">125</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">initialiseWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputSize</span><span class="p">,</span> <span class="n">hiddenSize</span><span class="p">,</span> <span class="n">outputSize</span><span class="p">):</span>
</span><span id="SingularRNN-126"><a href="#SingularRNN-126"><span class="linenos">126</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="SingularRNN-127"><a href="#SingularRNN-127"><span class="linenos">127</span></a><span class="sd">        Initializes weights and biases for input, hidden, and output layers,</span>
</span><span id="SingularRNN-128"><a href="#SingularRNN-128"><span class="linenos">128</span></a><span class="sd">        and sets initial hidden state.</span>
</span><span id="SingularRNN-129"><a href="#SingularRNN-129"><span class="linenos">129</span></a>
</span><span id="SingularRNN-130"><a href="#SingularRNN-130"><span class="linenos">130</span></a><span class="sd">        Args:</span>
</span><span id="SingularRNN-131"><a href="#SingularRNN-131"><span class="linenos">131</span></a><span class="sd">            inputSize (int): Size of the input feature vector.</span>
</span><span id="SingularRNN-132"><a href="#SingularRNN-132"><span class="linenos">132</span></a><span class="sd">            hiddenSize (int): Number of units in the hidden layer.</span>
</span><span id="SingularRNN-133"><a href="#SingularRNN-133"><span class="linenos">133</span></a><span class="sd">            outputSize (int): Size of the output vector.</span>
</span><span id="SingularRNN-134"><a href="#SingularRNN-134"><span class="linenos">134</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="SingularRNN-135"><a href="#SingularRNN-135"><span class="linenos">135</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">inputWeight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialiseWeights</span><span class="p">(</span><span class="n">hiddenSize</span><span class="p">,</span> <span class="n">inputSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hiddenStateActivationFunction</span><span class="p">)</span>
</span><span id="SingularRNN-136"><a href="#SingularRNN-136"><span class="linenos">136</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hiddenWeight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialiseWeights</span><span class="p">(</span><span class="n">hiddenSize</span><span class="p">,</span> <span class="n">hiddenSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hiddenStateActivationFunction</span><span class="p">)</span>
</span><span id="SingularRNN-137"><a href="#SingularRNN-137"><span class="linenos">137</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputWeight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialiseWeights</span><span class="p">(</span><span class="n">outputSize</span><span class="p">,</span> <span class="n">hiddenSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputLayerActivationFunction</span><span class="p">)</span>
</span><span id="SingularRNN-138"><a href="#SingularRNN-138"><span class="linenos">138</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">hiddenSize</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span><span id="SingularRNN-139"><a href="#SingularRNN-139"><span class="linenos">139</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputBias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">outputSize</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span><span id="SingularRNN-140"><a href="#SingularRNN-140"><span class="linenos">140</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hiddenState</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">hiddenSize</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span><span id="SingularRNN-141"><a href="#SingularRNN-141"><span class="linenos">141</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">inputSize</span> <span class="o">=</span> <span class="n">inputSize</span>
</span><span id="SingularRNN-142"><a href="#SingularRNN-142"><span class="linenos">142</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hiddenSize</span> <span class="o">=</span> <span class="n">hiddenSize</span>
</span><span id="SingularRNN-143"><a href="#SingularRNN-143"><span class="linenos">143</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputSize</span> <span class="o">=</span> <span class="n">outputSize</span>
</span><span id="SingularRNN-144"><a href="#SingularRNN-144"><span class="linenos">144</span></a>
</span><span id="SingularRNN-145"><a href="#SingularRNN-145"><span class="linenos">145</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">backwardPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
</span><span id="SingularRNN-146"><a href="#SingularRNN-146"><span class="linenos">146</span></a>        <span class="n">inputWeightGradients</span><span class="p">,</span> <span class="n">hiddenStateWeightGradients</span><span class="p">,</span> <span class="n">biasGradients</span><span class="p">,</span> <span class="n">outputWeightGradients</span><span class="p">,</span> <span class="n">outputbiasGradients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Singular_BPTT</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">allHiddenStates</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">preActivations</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputPreAct</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
</span><span id="SingularRNN-147"><a href="#SingularRNN-147"><span class="linenos">147</span></a>        <span class="n">Parameters</span> <span class="o">=</span>  <span class="p">{</span>
</span><span id="SingularRNN-148"><a href="#SingularRNN-148"><span class="linenos">148</span></a>            <span class="s2">&quot;I_W&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputWeight</span><span class="p">,</span>
</span><span id="SingularRNN-149"><a href="#SingularRNN-149"><span class="linenos">149</span></a>            <span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
</span><span id="SingularRNN-150"><a href="#SingularRNN-150"><span class="linenos">150</span></a>            <span class="s2">&quot;H_W&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">hiddenWeight</span><span class="p">,</span>
</span><span id="SingularRNN-151"><a href="#SingularRNN-151"><span class="linenos">151</span></a>            <span class="s2">&quot;O_W&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputWeight</span><span class="p">,</span>
</span><span id="SingularRNN-152"><a href="#SingularRNN-152"><span class="linenos">152</span></a>            <span class="s2">&quot;O_b&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputBias</span><span class="p">,</span>
</span><span id="SingularRNN-153"><a href="#SingularRNN-153"><span class="linenos">153</span></a>        <span class="p">}</span>
</span><span id="SingularRNN-154"><a href="#SingularRNN-154"><span class="linenos">154</span></a>  
</span><span id="SingularRNN-155"><a href="#SingularRNN-155"><span class="linenos">155</span></a>        <span class="n">Gradients</span> <span class="o">=</span>  <span class="p">{</span>
</span><span id="SingularRNN-156"><a href="#SingularRNN-156"><span class="linenos">156</span></a>            <span class="s2">&quot;I_W&quot;</span><span class="p">:</span> <span class="n">inputWeightGradients</span><span class="p">,</span>
</span><span id="SingularRNN-157"><a href="#SingularRNN-157"><span class="linenos">157</span></a>            <span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="n">biasGradients</span><span class="p">,</span>
</span><span id="SingularRNN-158"><a href="#SingularRNN-158"><span class="linenos">158</span></a>            <span class="s2">&quot;H_W&quot;</span><span class="p">:</span> <span class="n">hiddenStateWeightGradients</span><span class="p">,</span>
</span><span id="SingularRNN-159"><a href="#SingularRNN-159"><span class="linenos">159</span></a>            <span class="s2">&quot;O_W&quot;</span><span class="p">:</span> <span class="n">outputWeightGradients</span><span class="p">,</span>
</span><span id="SingularRNN-160"><a href="#SingularRNN-160"><span class="linenos">160</span></a>            <span class="s2">&quot;O_b&quot;</span><span class="p">:</span> <span class="n">outputbiasGradients</span><span class="p">,</span>
</span><span id="SingularRNN-161"><a href="#SingularRNN-161"><span class="linenos">161</span></a>        <span class="p">}</span>
</span><span id="SingularRNN-162"><a href="#SingularRNN-162"><span class="linenos">162</span></a>        <span class="k">return</span> <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span> 
</span><span id="SingularRNN-163"><a href="#SingularRNN-163"><span class="linenos">163</span></a>
</span><span id="SingularRNN-164"><a href="#SingularRNN-164"><span class="linenos">164</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">optimiser</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">):</span>
</span><span id="SingularRNN-165"><a href="#SingularRNN-165"><span class="linenos">165</span></a>        <span class="n">AllOutputs</span><span class="p">,</span> <span class="n">Parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adam</span><span class="o">.</span><span class="n">optimiser</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">useBatches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
</span><span id="SingularRNN-166"><a href="#SingularRNN-166"><span class="linenos">166</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">inputWeights</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;I_W&quot;</span><span class="p">]</span>
</span><span id="SingularRNN-167"><a href="#SingularRNN-167"><span class="linenos">167</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">]</span>
</span><span id="SingularRNN-168"><a href="#SingularRNN-168"><span class="linenos">168</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hiddenWeights</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;H_W&quot;</span><span class="p">]</span>
</span><span id="SingularRNN-169"><a href="#SingularRNN-169"><span class="linenos">169</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputWeight</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;O_W&quot;</span><span class="p">]</span>
</span><span id="SingularRNN-170"><a href="#SingularRNN-170"><span class="linenos">170</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputBias</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;O_b&quot;</span><span class="p">]</span>
</span><span id="SingularRNN-171"><a href="#SingularRNN-171"><span class="linenos">171</span></a>        <span class="k">return</span> <span class="n">AllOutputs</span>
</span><span id="SingularRNN-172"><a href="#SingularRNN-172"><span class="linenos">172</span></a>
</span><span id="SingularRNN-173"><a href="#SingularRNN-173"><span class="linenos">173</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span> <span class="n">beta1</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span> <span class="n">beta2</span> <span class="o">=</span> <span class="mf">0.999</span><span class="p">,</span> <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">):</span>
</span><span id="SingularRNN-174"><a href="#SingularRNN-174"><span class="linenos">174</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="SingularRNN-175"><a href="#SingularRNN-175"><span class="linenos">175</span></a><span class="sd">        Trains the model on the given input sequences and labels using the Adam optimizer,</span>
</span><span id="SingularRNN-176"><a href="#SingularRNN-176"><span class="linenos">176</span></a><span class="sd">        then calculates and returns the loss.</span>
</span><span id="SingularRNN-177"><a href="#SingularRNN-177"><span class="linenos">177</span></a>
</span><span id="SingularRNN-178"><a href="#SingularRNN-178"><span class="linenos">178</span></a><span class="sd">        Args:</span>
</span><span id="SingularRNN-179"><a href="#SingularRNN-179"><span class="linenos">179</span></a><span class="sd">            inputData (ndarray): 3D array of input data with shape (batchSize, sequenceLength, inputSize).</span>
</span><span id="SingularRNN-180"><a href="#SingularRNN-180"><span class="linenos">180</span></a><span class="sd">            labels (ndarray): True labels corresponding to the input data.</span>
</span><span id="SingularRNN-181"><a href="#SingularRNN-181"><span class="linenos">181</span></a><span class="sd">            alpha (float, optional): Learning rate for the optimizer. Default is 0.001.</span>
</span><span id="SingularRNN-182"><a href="#SingularRNN-182"><span class="linenos">182</span></a><span class="sd">            beta1 (float, optional): Beta1 parameter for Adam optimizer. Default is 0.9.</span>
</span><span id="SingularRNN-183"><a href="#SingularRNN-183"><span class="linenos">183</span></a><span class="sd">            beta2 (float, optional): Beta2 parameter for Adam optimizer. Default is 0.999.</span>
</span><span id="SingularRNN-184"><a href="#SingularRNN-184"><span class="linenos">184</span></a><span class="sd">            epsilon (float, optional): Epsilon parameter for Adam optimizer to prevent division by zero. Default is 1e-8.</span>
</span><span id="SingularRNN-185"><a href="#SingularRNN-185"><span class="linenos">185</span></a>
</span><span id="SingularRNN-186"><a href="#SingularRNN-186"><span class="linenos">186</span></a><span class="sd">        Returns:</span>
</span><span id="SingularRNN-187"><a href="#SingularRNN-187"><span class="linenos">187</span></a><span class="sd">            float: Calculated loss between model outputs and true labels.</span>
</span><span id="SingularRNN-188"><a href="#SingularRNN-188"><span class="linenos">188</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="SingularRNN-189"><a href="#SingularRNN-189"><span class="linenos">189</span></a>        <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputData</span><span class="p">)</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Dimension wrong size, got </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputData</span><span class="p">)</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">, expected 3&quot;</span>
</span><span id="SingularRNN-190"><a href="#SingularRNN-190"><span class="linenos">190</span></a>        <span class="n">AllOutputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimiser</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
</span><span id="SingularRNN-191"><a href="#SingularRNN-191"><span class="linenos">191</span></a>        <span class="n">AllOutputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">AllOutputs</span><span class="p">,</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</span><span id="SingularRNN-192"><a href="#SingularRNN-192"><span class="linenos">192</span></a>        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lossFunction</span><span class="p">(</span><span class="n">AllOutputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</span><span id="SingularRNN-193"><a href="#SingularRNN-193"><span class="linenos">193</span></a>        <span class="k">return</span> <span class="n">loss</span>
</span></pre></div>


    

                            <div id="SingularRNN.__init__" class="classattr">
                                        <input id="SingularRNN.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">SingularRNN</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">hiddenStateActivationFunction</span>,</span><span class="param">	<span class="n">outputLayerActivationFunction</span>,</span><span class="param">	<span class="n">lossFunction</span>,</span><span class="param">	<span class="n">useBatches</span><span class="o">=</span><span class="kc">False</span>,</span><span class="param">	<span class="n">batchSize</span><span class="o">=</span><span class="mi">64</span></span>)</span>

                <label class="view-source-button" for="SingularRNN.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#SingularRNN.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="SingularRNN.__init__-18"><a href="#SingularRNN.__init__-18"><span class="linenos">18</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hiddenStateActivationFunction</span><span class="p">,</span> <span class="n">outputLayerActivationFunction</span><span class="p">,</span> <span class="n">lossFunction</span><span class="p">,</span> <span class="n">useBatches</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">batchSize</span> <span class="o">=</span> <span class="mi">64</span><span class="p">):</span>
</span><span id="SingularRNN.__init__-19"><a href="#SingularRNN.__init__-19"><span class="linenos">19</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="SingularRNN.__init__-20"><a href="#SingularRNN.__init__-20"><span class="linenos">20</span></a><span class="sd">        Initializes the RNN model by setting activation functions, loss function,</span>
</span><span id="SingularRNN.__init__-21"><a href="#SingularRNN.__init__-21"><span class="linenos">21</span></a><span class="sd">        batching options, and creating the Adam optimizer instance.</span>
</span><span id="SingularRNN.__init__-22"><a href="#SingularRNN.__init__-22"><span class="linenos">22</span></a>
</span><span id="SingularRNN.__init__-23"><a href="#SingularRNN.__init__-23"><span class="linenos">23</span></a><span class="sd">        Args:</span>
</span><span id="SingularRNN.__init__-24"><a href="#SingularRNN.__init__-24"><span class="linenos">24</span></a><span class="sd">            hiddenStateActivationFunction (str): Name of activation function for hidden states (e.g., &quot;relu&quot;, &quot;sigmoid&quot;).</span>
</span><span id="SingularRNN.__init__-25"><a href="#SingularRNN.__init__-25"><span class="linenos">25</span></a><span class="sd">            outputLayerActivationFunction (str): Name of activation function for output layer.</span>
</span><span id="SingularRNN.__init__-26"><a href="#SingularRNN.__init__-26"><span class="linenos">26</span></a><span class="sd">            lossFunction (str): Name of the loss function to use (e.g., &quot;mse&quot;, &quot;mae&quot;, &quot;cross entropy&quot;).</span>
</span><span id="SingularRNN.__init__-27"><a href="#SingularRNN.__init__-27"><span class="linenos">27</span></a><span class="sd">            useBatches (bool, optional): Whether to use batching during training. Default is False.</span>
</span><span id="SingularRNN.__init__-28"><a href="#SingularRNN.__init__-28"><span class="linenos">28</span></a><span class="sd">            batchSize (int, optional): Batch size if batching is enabled. Default is 64. </span>
</span><span id="SingularRNN.__init__-29"><a href="#SingularRNN.__init__-29"><span class="linenos">29</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="SingularRNN.__init__-30"><a href="#SingularRNN.__init__-30"><span class="linenos">30</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">inputWeight</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="SingularRNN.__init__-31"><a href="#SingularRNN.__init__-31"><span class="linenos">31</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hiddenWeight</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="SingularRNN.__init__-32"><a href="#SingularRNN.__init__-32"><span class="linenos">32</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="SingularRNN.__init__-33"><a href="#SingularRNN.__init__-33"><span class="linenos">33</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputWeight</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="SingularRNN.__init__-34"><a href="#SingularRNN.__init__-34"><span class="linenos">34</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputBias</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="SingularRNN.__init__-35"><a href="#SingularRNN.__init__-35"><span class="linenos">35</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hiddenState</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="SingularRNN.__init__-36"><a href="#SingularRNN.__init__-36"><span class="linenos">36</span></a>        
</span><span id="SingularRNN.__init__-37"><a href="#SingularRNN.__init__-37"><span class="linenos">37</span></a>        <span class="n">funcs</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="SingularRNN.__init__-38"><a href="#SingularRNN.__init__-38"><span class="linenos">38</span></a>            <span class="s2">&quot;relu&quot;</span><span class="p">:</span> <span class="n">relu</span><span class="p">,</span>
</span><span id="SingularRNN.__init__-39"><a href="#SingularRNN.__init__-39"><span class="linenos">39</span></a>            <span class="s2">&quot;sigmoid&quot;</span><span class="p">:</span> <span class="n">sigmoid</span><span class="p">,</span>
</span><span id="SingularRNN.__init__-40"><a href="#SingularRNN.__init__-40"><span class="linenos">40</span></a>            <span class="s2">&quot;linear&quot;</span><span class="p">:</span> <span class="n">linear</span><span class="p">,</span>
</span><span id="SingularRNN.__init__-41"><a href="#SingularRNN.__init__-41"><span class="linenos">41</span></a>            <span class="s2">&quot;tanh&quot;</span><span class="p">:</span> <span class="n">tanH</span><span class="p">,</span>
</span><span id="SingularRNN.__init__-42"><a href="#SingularRNN.__init__-42"><span class="linenos">42</span></a>            <span class="s2">&quot;softmax&quot;</span><span class="p">:</span> <span class="n">softMax</span><span class="p">,</span>
</span><span id="SingularRNN.__init__-43"><a href="#SingularRNN.__init__-43"><span class="linenos">43</span></a>        <span class="p">}</span>
</span><span id="SingularRNN.__init__-44"><a href="#SingularRNN.__init__-44"><span class="linenos">44</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">hiddenStateActivationFunction</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">funcs</span><span class="p">):</span>
</span><span id="SingularRNN.__init__-45"><a href="#SingularRNN.__init__-45"><span class="linenos">45</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Activation function not made: </span><span class="si">{</span><span class="n">hiddenStateActivationFunction</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="SingularRNN.__init__-46"><a href="#SingularRNN.__init__-46"><span class="linenos">46</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">outputLayerActivationFunction</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">funcs</span><span class="p">):</span>
</span><span id="SingularRNN.__init__-47"><a href="#SingularRNN.__init__-47"><span class="linenos">47</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Activation function not made: </span><span class="si">{</span><span class="n">outputLayerActivationFunction</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="SingularRNN.__init__-48"><a href="#SingularRNN.__init__-48"><span class="linenos">48</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hiddenStateActivationFunction</span> <span class="o">=</span> <span class="n">funcs</span><span class="p">[</span><span class="n">hiddenStateActivationFunction</span><span class="o">.</span><span class="n">lower</span><span class="p">()]</span>
</span><span id="SingularRNN.__init__-49"><a href="#SingularRNN.__init__-49"><span class="linenos">49</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputLayerActivationFunction</span> <span class="o">=</span> <span class="n">funcs</span><span class="p">[</span><span class="n">outputLayerActivationFunction</span><span class="o">.</span><span class="n">lower</span><span class="p">()]</span>
</span><span id="SingularRNN.__init__-50"><a href="#SingularRNN.__init__-50"><span class="linenos">50</span></a>
</span><span id="SingularRNN.__init__-51"><a href="#SingularRNN.__init__-51"><span class="linenos">51</span></a>        <span class="n">derivs</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="SingularRNN.__init__-52"><a href="#SingularRNN.__init__-52"><span class="linenos">52</span></a>            <span class="n">relu</span><span class="p">:</span> <span class="n">ReLUDerivative</span><span class="p">,</span>
</span><span id="SingularRNN.__init__-53"><a href="#SingularRNN.__init__-53"><span class="linenos">53</span></a>            <span class="n">sigmoid</span><span class="p">:</span> <span class="n">SigmoidDerivative</span><span class="p">,</span>
</span><span id="SingularRNN.__init__-54"><a href="#SingularRNN.__init__-54"><span class="linenos">54</span></a>            <span class="n">linear</span><span class="p">:</span> <span class="n">LinearDerivative</span><span class="p">,</span>
</span><span id="SingularRNN.__init__-55"><a href="#SingularRNN.__init__-55"><span class="linenos">55</span></a>            <span class="n">tanH</span><span class="p">:</span> <span class="n">TanHDerivative</span><span class="p">,</span>
</span><span id="SingularRNN.__init__-56"><a href="#SingularRNN.__init__-56"><span class="linenos">56</span></a>            <span class="n">softMax</span><span class="p">:</span> <span class="n">SoftMaxDerivative</span><span class="p">,</span>
</span><span id="SingularRNN.__init__-57"><a href="#SingularRNN.__init__-57"><span class="linenos">57</span></a>        <span class="p">}</span>
</span><span id="SingularRNN.__init__-58"><a href="#SingularRNN.__init__-58"><span class="linenos">58</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activationDerivative</span> <span class="o">=</span> <span class="n">derivs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">hiddenStateActivationFunction</span><span class="p">]</span>
</span><span id="SingularRNN.__init__-59"><a href="#SingularRNN.__init__-59"><span class="linenos">59</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputLayerDerivative</span> <span class="o">=</span> <span class="n">derivs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">outputLayerActivationFunction</span><span class="p">]</span>
</span><span id="SingularRNN.__init__-60"><a href="#SingularRNN.__init__-60"><span class="linenos">60</span></a>
</span><span id="SingularRNN.__init__-61"><a href="#SingularRNN.__init__-61"><span class="linenos">61</span></a>        <span class="n">lossFunctionDict</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="SingularRNN.__init__-62"><a href="#SingularRNN.__init__-62"><span class="linenos">62</span></a>            <span class="s2">&quot;mse&quot;</span><span class="p">:</span> <span class="n">MSELossFunction</span><span class="p">,</span>
</span><span id="SingularRNN.__init__-63"><a href="#SingularRNN.__init__-63"><span class="linenos">63</span></a>            <span class="s2">&quot;mae&quot;</span><span class="p">:</span> <span class="n">MAELossFunction</span><span class="p">,</span>
</span><span id="SingularRNN.__init__-64"><a href="#SingularRNN.__init__-64"><span class="linenos">64</span></a>            <span class="s2">&quot;cross entropy&quot;</span><span class="p">:</span> <span class="n">CrossEntropyLossFunction</span><span class="p">,</span><span class="s2">&quot;cross&quot;</span><span class="p">:</span> <span class="n">CrossEntropyLossFunction</span><span class="p">,</span>
</span><span id="SingularRNN.__init__-65"><a href="#SingularRNN.__init__-65"><span class="linenos">65</span></a>        <span class="p">}</span>
</span><span id="SingularRNN.__init__-66"><a href="#SingularRNN.__init__-66"><span class="linenos">66</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">lossFunction</span> <span class="o">=</span> <span class="n">lossFunctionDict</span><span class="p">[</span><span class="n">lossFunction</span><span class="o">.</span><span class="n">lower</span><span class="p">()]</span>
</span><span id="SingularRNN.__init__-67"><a href="#SingularRNN.__init__-67"><span class="linenos">67</span></a>        <span class="n">lossDerivs</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="SingularRNN.__init__-68"><a href="#SingularRNN.__init__-68"><span class="linenos">68</span></a>            <span class="n">MSELossFunction</span><span class="p">:</span> <span class="n">MSEDerivative</span><span class="p">,</span>
</span><span id="SingularRNN.__init__-69"><a href="#SingularRNN.__init__-69"><span class="linenos">69</span></a>            <span class="n">MAELossFunction</span><span class="p">:</span> <span class="n">MAEDerivative</span><span class="p">,</span>
</span><span id="SingularRNN.__init__-70"><a href="#SingularRNN.__init__-70"><span class="linenos">70</span></a>            <span class="n">CrossEntropyLossFunction</span><span class="p">:</span> <span class="n">CrossEntropyLossDerivative</span><span class="p">,</span>
</span><span id="SingularRNN.__init__-71"><a href="#SingularRNN.__init__-71"><span class="linenos">71</span></a>        <span class="p">}</span>
</span><span id="SingularRNN.__init__-72"><a href="#SingularRNN.__init__-72"><span class="linenos">72</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">lossDerivative</span> <span class="o">=</span> <span class="n">lossDerivs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">lossFunction</span><span class="p">]</span>
</span><span id="SingularRNN.__init__-73"><a href="#SingularRNN.__init__-73"><span class="linenos">73</span></a>
</span><span id="SingularRNN.__init__-74"><a href="#SingularRNN.__init__-74"><span class="linenos">74</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">useBatches</span> <span class="o">=</span> <span class="n">useBatches</span>
</span><span id="SingularRNN.__init__-75"><a href="#SingularRNN.__init__-75"><span class="linenos">75</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span> <span class="o">=</span> <span class="n">batchSize</span>
</span><span id="SingularRNN.__init__-76"><a href="#SingularRNN.__init__-76"><span class="linenos">76</span></a>
</span><span id="SingularRNN.__init__-77"><a href="#SingularRNN.__init__-77"><span class="linenos">77</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">adam</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">forwardSequence</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">backwardPropagation</span><span class="p">,</span> <span class="n">giveInputsToBackprop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Initializes the RNN model by setting activation functions, loss function,
batching options, and creating the Adam optimizer instance.</p>

<p>Args:
    hiddenStateActivationFunction (str): Name of activation function for hidden states (e.g., "relu", "sigmoid").
    outputLayerActivationFunction (str): Name of activation function for output layer.
    lossFunction (str): Name of the loss function to use (e.g., "mse", "mae", "cross entropy").
    useBatches (bool, optional): Whether to use batching during training. Default is False.
    batchSize (int, optional): Batch size if batching is enabled. Default is 64.</p>
</div>


                            </div>
                            <div id="SingularRNN.inputWeight" class="classattr">
                                <div class="attr variable">
            <span class="name">inputWeight</span>

        
    </div>
    <a class="headerlink" href="#SingularRNN.inputWeight"></a>
    
    

                            </div>
                            <div id="SingularRNN.hiddenWeight" class="classattr">
                                <div class="attr variable">
            <span class="name">hiddenWeight</span>

        
    </div>
    <a class="headerlink" href="#SingularRNN.hiddenWeight"></a>
    
    

                            </div>
                            <div id="SingularRNN.bias" class="classattr">
                                <div class="attr variable">
            <span class="name">bias</span>

        
    </div>
    <a class="headerlink" href="#SingularRNN.bias"></a>
    
    

                            </div>
                            <div id="SingularRNN.outputWeight" class="classattr">
                                <div class="attr variable">
            <span class="name">outputWeight</span>

        
    </div>
    <a class="headerlink" href="#SingularRNN.outputWeight"></a>
    
    

                            </div>
                            <div id="SingularRNN.outputBias" class="classattr">
                                <div class="attr variable">
            <span class="name">outputBias</span>

        
    </div>
    <a class="headerlink" href="#SingularRNN.outputBias"></a>
    
    

                            </div>
                            <div id="SingularRNN.hiddenState" class="classattr">
                                <div class="attr variable">
            <span class="name">hiddenState</span>

        
    </div>
    <a class="headerlink" href="#SingularRNN.hiddenState"></a>
    
    

                            </div>
                            <div id="SingularRNN.hiddenStateActivationFunction" class="classattr">
                                <div class="attr variable">
            <span class="name">hiddenStateActivationFunction</span>

        
    </div>
    <a class="headerlink" href="#SingularRNN.hiddenStateActivationFunction"></a>
    
    

                            </div>
                            <div id="SingularRNN.outputLayerActivationFunction" class="classattr">
                                <div class="attr variable">
            <span class="name">outputLayerActivationFunction</span>

        
    </div>
    <a class="headerlink" href="#SingularRNN.outputLayerActivationFunction"></a>
    
    

                            </div>
                            <div id="SingularRNN.activationDerivative" class="classattr">
                                <div class="attr variable">
            <span class="name">activationDerivative</span>

        
    </div>
    <a class="headerlink" href="#SingularRNN.activationDerivative"></a>
    
    

                            </div>
                            <div id="SingularRNN.outputLayerDerivative" class="classattr">
                                <div class="attr variable">
            <span class="name">outputLayerDerivative</span>

        
    </div>
    <a class="headerlink" href="#SingularRNN.outputLayerDerivative"></a>
    
    

                            </div>
                            <div id="SingularRNN.lossFunction" class="classattr">
                                <div class="attr variable">
            <span class="name">lossFunction</span>

        
    </div>
    <a class="headerlink" href="#SingularRNN.lossFunction"></a>
    
    

                            </div>
                            <div id="SingularRNN.lossDerivative" class="classattr">
                                <div class="attr variable">
            <span class="name">lossDerivative</span>

        
    </div>
    <a class="headerlink" href="#SingularRNN.lossDerivative"></a>
    
    

                            </div>
                            <div id="SingularRNN.useBatches" class="classattr">
                                <div class="attr variable">
            <span class="name">useBatches</span>

        
    </div>
    <a class="headerlink" href="#SingularRNN.useBatches"></a>
    
    

                            </div>
                            <div id="SingularRNN.batchSize" class="classattr">
                                <div class="attr variable">
            <span class="name">batchSize</span>

        
    </div>
    <a class="headerlink" href="#SingularRNN.batchSize"></a>
    
    

                            </div>
                            <div id="SingularRNN.adam" class="classattr">
                                <div class="attr variable">
            <span class="name">adam</span>

        
    </div>
    <a class="headerlink" href="#SingularRNN.adam"></a>
    
    

                            </div>
                            <div id="SingularRNN.forwardSequence" class="classattr">
                                        <input id="SingularRNN.forwardSequence-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forwardSequence</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">inputData</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="SingularRNN.forwardSequence-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#SingularRNN.forwardSequence"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="SingularRNN.forwardSequence-79"><a href="#SingularRNN.forwardSequence-79"><span class="linenos">79</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forwardSequence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">):</span> <span class="c1"># goes through the whole sequence / time steps</span>
</span><span id="SingularRNN.forwardSequence-80"><a href="#SingularRNN.forwardSequence-80"><span class="linenos">80</span></a>        <span class="n">batchSize</span><span class="p">,</span> <span class="n">sequenceLegth</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">inputData</span><span class="o">.</span><span class="n">shape</span>
</span><span id="SingularRNN.forwardSequence-81"><a href="#SingularRNN.forwardSequence-81"><span class="linenos">81</span></a>        
</span><span id="SingularRNN.forwardSequence-82"><a href="#SingularRNN.forwardSequence-82"><span class="linenos">82</span></a>        <span class="n">preActivations</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="SingularRNN.forwardSequence-83"><a href="#SingularRNN.forwardSequence-83"><span class="linenos">83</span></a>        <span class="n">allHiddenStates</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="SingularRNN.forwardSequence-84"><a href="#SingularRNN.forwardSequence-84"><span class="linenos">84</span></a>
</span><span id="SingularRNN.forwardSequence-85"><a href="#SingularRNN.forwardSequence-85"><span class="linenos">85</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">sequenceLegth</span><span class="p">):</span>
</span><span id="SingularRNN.forwardSequence-86"><a href="#SingularRNN.forwardSequence-86"><span class="linenos">86</span></a>            <span class="n">xi</span> <span class="o">=</span> <span class="n">inputData</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batchSize</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span><span id="SingularRNN.forwardSequence-87"><a href="#SingularRNN.forwardSequence-87"><span class="linenos">87</span></a>            <span class="n">preAct</span><span class="p">,</span> <span class="n">outputPreAct</span><span class="p">,</span> <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_oneStep</span><span class="p">(</span><span class="n">xi</span><span class="p">)</span>
</span><span id="SingularRNN.forwardSequence-88"><a href="#SingularRNN.forwardSequence-88"><span class="linenos">88</span></a>            <span class="n">preActivations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">preAct</span><span class="p">)</span>
</span><span id="SingularRNN.forwardSequence-89"><a href="#SingularRNN.forwardSequence-89"><span class="linenos">89</span></a>            <span class="n">allHiddenStates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hiddenState</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
</span><span id="SingularRNN.forwardSequence-90"><a href="#SingularRNN.forwardSequence-90"><span class="linenos">90</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">preActivations</span> <span class="o">=</span> <span class="n">preActivations</span>
</span><span id="SingularRNN.forwardSequence-91"><a href="#SingularRNN.forwardSequence-91"><span class="linenos">91</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">allHiddenStates</span> <span class="o">=</span> <span class="n">allHiddenStates</span>
</span><span id="SingularRNN.forwardSequence-92"><a href="#SingularRNN.forwardSequence-92"><span class="linenos">92</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputPreAct</span> <span class="o">=</span> <span class="n">outputPreAct</span>
</span><span id="SingularRNN.forwardSequence-93"><a href="#SingularRNN.forwardSequence-93"><span class="linenos">93</span></a>        <span class="k">return</span> <span class="n">output</span>
</span></pre></div>


    

                            </div>
                            <div id="SingularRNN.initialiseWeights" class="classattr">
                                        <input id="SingularRNN.initialiseWeights-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">initialiseWeights</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">inputSize</span>, </span><span class="param"><span class="n">hiddenSize</span>, </span><span class="param"><span class="n">outputSize</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="SingularRNN.initialiseWeights-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#SingularRNN.initialiseWeights"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="SingularRNN.initialiseWeights-125"><a href="#SingularRNN.initialiseWeights-125"><span class="linenos">125</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">initialiseWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputSize</span><span class="p">,</span> <span class="n">hiddenSize</span><span class="p">,</span> <span class="n">outputSize</span><span class="p">):</span>
</span><span id="SingularRNN.initialiseWeights-126"><a href="#SingularRNN.initialiseWeights-126"><span class="linenos">126</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="SingularRNN.initialiseWeights-127"><a href="#SingularRNN.initialiseWeights-127"><span class="linenos">127</span></a><span class="sd">        Initializes weights and biases for input, hidden, and output layers,</span>
</span><span id="SingularRNN.initialiseWeights-128"><a href="#SingularRNN.initialiseWeights-128"><span class="linenos">128</span></a><span class="sd">        and sets initial hidden state.</span>
</span><span id="SingularRNN.initialiseWeights-129"><a href="#SingularRNN.initialiseWeights-129"><span class="linenos">129</span></a>
</span><span id="SingularRNN.initialiseWeights-130"><a href="#SingularRNN.initialiseWeights-130"><span class="linenos">130</span></a><span class="sd">        Args:</span>
</span><span id="SingularRNN.initialiseWeights-131"><a href="#SingularRNN.initialiseWeights-131"><span class="linenos">131</span></a><span class="sd">            inputSize (int): Size of the input feature vector.</span>
</span><span id="SingularRNN.initialiseWeights-132"><a href="#SingularRNN.initialiseWeights-132"><span class="linenos">132</span></a><span class="sd">            hiddenSize (int): Number of units in the hidden layer.</span>
</span><span id="SingularRNN.initialiseWeights-133"><a href="#SingularRNN.initialiseWeights-133"><span class="linenos">133</span></a><span class="sd">            outputSize (int): Size of the output vector.</span>
</span><span id="SingularRNN.initialiseWeights-134"><a href="#SingularRNN.initialiseWeights-134"><span class="linenos">134</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="SingularRNN.initialiseWeights-135"><a href="#SingularRNN.initialiseWeights-135"><span class="linenos">135</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">inputWeight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialiseWeights</span><span class="p">(</span><span class="n">hiddenSize</span><span class="p">,</span> <span class="n">inputSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hiddenStateActivationFunction</span><span class="p">)</span>
</span><span id="SingularRNN.initialiseWeights-136"><a href="#SingularRNN.initialiseWeights-136"><span class="linenos">136</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hiddenWeight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialiseWeights</span><span class="p">(</span><span class="n">hiddenSize</span><span class="p">,</span> <span class="n">hiddenSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hiddenStateActivationFunction</span><span class="p">)</span>
</span><span id="SingularRNN.initialiseWeights-137"><a href="#SingularRNN.initialiseWeights-137"><span class="linenos">137</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputWeight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialiseWeights</span><span class="p">(</span><span class="n">outputSize</span><span class="p">,</span> <span class="n">hiddenSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputLayerActivationFunction</span><span class="p">)</span>
</span><span id="SingularRNN.initialiseWeights-138"><a href="#SingularRNN.initialiseWeights-138"><span class="linenos">138</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">hiddenSize</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span><span id="SingularRNN.initialiseWeights-139"><a href="#SingularRNN.initialiseWeights-139"><span class="linenos">139</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputBias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">outputSize</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span><span id="SingularRNN.initialiseWeights-140"><a href="#SingularRNN.initialiseWeights-140"><span class="linenos">140</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hiddenState</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">hiddenSize</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span><span id="SingularRNN.initialiseWeights-141"><a href="#SingularRNN.initialiseWeights-141"><span class="linenos">141</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">inputSize</span> <span class="o">=</span> <span class="n">inputSize</span>
</span><span id="SingularRNN.initialiseWeights-142"><a href="#SingularRNN.initialiseWeights-142"><span class="linenos">142</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hiddenSize</span> <span class="o">=</span> <span class="n">hiddenSize</span>
</span><span id="SingularRNN.initialiseWeights-143"><a href="#SingularRNN.initialiseWeights-143"><span class="linenos">143</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputSize</span> <span class="o">=</span> <span class="n">outputSize</span>
</span></pre></div>


            <div class="docstring"><p>Initializes weights and biases for input, hidden, and output layers,
and sets initial hidden state.</p>

<p>Args:
    inputSize (int): Size of the input feature vector.
    hiddenSize (int): Number of units in the hidden layer.
    outputSize (int): Size of the output vector.</p>
</div>


                            </div>
                            <div id="SingularRNN.backwardPropagation" class="classattr">
                                        <input id="SingularRNN.backwardPropagation-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">backwardPropagation</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">inputs</span>, </span><span class="param"><span class="n">outputs</span>, </span><span class="param"><span class="n">targets</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="SingularRNN.backwardPropagation-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#SingularRNN.backwardPropagation"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="SingularRNN.backwardPropagation-145"><a href="#SingularRNN.backwardPropagation-145"><span class="linenos">145</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">backwardPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
</span><span id="SingularRNN.backwardPropagation-146"><a href="#SingularRNN.backwardPropagation-146"><span class="linenos">146</span></a>        <span class="n">inputWeightGradients</span><span class="p">,</span> <span class="n">hiddenStateWeightGradients</span><span class="p">,</span> <span class="n">biasGradients</span><span class="p">,</span> <span class="n">outputWeightGradients</span><span class="p">,</span> <span class="n">outputbiasGradients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Singular_BPTT</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">allHiddenStates</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">preActivations</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputPreAct</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
</span><span id="SingularRNN.backwardPropagation-147"><a href="#SingularRNN.backwardPropagation-147"><span class="linenos">147</span></a>        <span class="n">Parameters</span> <span class="o">=</span>  <span class="p">{</span>
</span><span id="SingularRNN.backwardPropagation-148"><a href="#SingularRNN.backwardPropagation-148"><span class="linenos">148</span></a>            <span class="s2">&quot;I_W&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputWeight</span><span class="p">,</span>
</span><span id="SingularRNN.backwardPropagation-149"><a href="#SingularRNN.backwardPropagation-149"><span class="linenos">149</span></a>            <span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
</span><span id="SingularRNN.backwardPropagation-150"><a href="#SingularRNN.backwardPropagation-150"><span class="linenos">150</span></a>            <span class="s2">&quot;H_W&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">hiddenWeight</span><span class="p">,</span>
</span><span id="SingularRNN.backwardPropagation-151"><a href="#SingularRNN.backwardPropagation-151"><span class="linenos">151</span></a>            <span class="s2">&quot;O_W&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputWeight</span><span class="p">,</span>
</span><span id="SingularRNN.backwardPropagation-152"><a href="#SingularRNN.backwardPropagation-152"><span class="linenos">152</span></a>            <span class="s2">&quot;O_b&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputBias</span><span class="p">,</span>
</span><span id="SingularRNN.backwardPropagation-153"><a href="#SingularRNN.backwardPropagation-153"><span class="linenos">153</span></a>        <span class="p">}</span>
</span><span id="SingularRNN.backwardPropagation-154"><a href="#SingularRNN.backwardPropagation-154"><span class="linenos">154</span></a>  
</span><span id="SingularRNN.backwardPropagation-155"><a href="#SingularRNN.backwardPropagation-155"><span class="linenos">155</span></a>        <span class="n">Gradients</span> <span class="o">=</span>  <span class="p">{</span>
</span><span id="SingularRNN.backwardPropagation-156"><a href="#SingularRNN.backwardPropagation-156"><span class="linenos">156</span></a>            <span class="s2">&quot;I_W&quot;</span><span class="p">:</span> <span class="n">inputWeightGradients</span><span class="p">,</span>
</span><span id="SingularRNN.backwardPropagation-157"><a href="#SingularRNN.backwardPropagation-157"><span class="linenos">157</span></a>            <span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="n">biasGradients</span><span class="p">,</span>
</span><span id="SingularRNN.backwardPropagation-158"><a href="#SingularRNN.backwardPropagation-158"><span class="linenos">158</span></a>            <span class="s2">&quot;H_W&quot;</span><span class="p">:</span> <span class="n">hiddenStateWeightGradients</span><span class="p">,</span>
</span><span id="SingularRNN.backwardPropagation-159"><a href="#SingularRNN.backwardPropagation-159"><span class="linenos">159</span></a>            <span class="s2">&quot;O_W&quot;</span><span class="p">:</span> <span class="n">outputWeightGradients</span><span class="p">,</span>
</span><span id="SingularRNN.backwardPropagation-160"><a href="#SingularRNN.backwardPropagation-160"><span class="linenos">160</span></a>            <span class="s2">&quot;O_b&quot;</span><span class="p">:</span> <span class="n">outputbiasGradients</span><span class="p">,</span>
</span><span id="SingularRNN.backwardPropagation-161"><a href="#SingularRNN.backwardPropagation-161"><span class="linenos">161</span></a>        <span class="p">}</span>
</span><span id="SingularRNN.backwardPropagation-162"><a href="#SingularRNN.backwardPropagation-162"><span class="linenos">162</span></a>        <span class="k">return</span> <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span> 
</span></pre></div>


    

                            </div>
                            <div id="SingularRNN.optimiser" class="classattr">
                                        <input id="SingularRNN.optimiser-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">optimiser</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">inputData</span>, </span><span class="param"><span class="n">labels</span>, </span><span class="param"><span class="n">alpha</span>, </span><span class="param"><span class="n">beta1</span>, </span><span class="param"><span class="n">beta2</span>, </span><span class="param"><span class="n">epsilon</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="SingularRNN.optimiser-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#SingularRNN.optimiser"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="SingularRNN.optimiser-164"><a href="#SingularRNN.optimiser-164"><span class="linenos">164</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">optimiser</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">):</span>
</span><span id="SingularRNN.optimiser-165"><a href="#SingularRNN.optimiser-165"><span class="linenos">165</span></a>        <span class="n">AllOutputs</span><span class="p">,</span> <span class="n">Parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adam</span><span class="o">.</span><span class="n">optimiser</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">useBatches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
</span><span id="SingularRNN.optimiser-166"><a href="#SingularRNN.optimiser-166"><span class="linenos">166</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">inputWeights</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;I_W&quot;</span><span class="p">]</span>
</span><span id="SingularRNN.optimiser-167"><a href="#SingularRNN.optimiser-167"><span class="linenos">167</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">]</span>
</span><span id="SingularRNN.optimiser-168"><a href="#SingularRNN.optimiser-168"><span class="linenos">168</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hiddenWeights</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;H_W&quot;</span><span class="p">]</span>
</span><span id="SingularRNN.optimiser-169"><a href="#SingularRNN.optimiser-169"><span class="linenos">169</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputWeight</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;O_W&quot;</span><span class="p">]</span>
</span><span id="SingularRNN.optimiser-170"><a href="#SingularRNN.optimiser-170"><span class="linenos">170</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputBias</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;O_b&quot;</span><span class="p">]</span>
</span><span id="SingularRNN.optimiser-171"><a href="#SingularRNN.optimiser-171"><span class="linenos">171</span></a>        <span class="k">return</span> <span class="n">AllOutputs</span>
</span></pre></div>


    

                            </div>
                            <div id="SingularRNN.train" class="classattr">
                                        <input id="SingularRNN.train-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">train</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">inputData</span>,</span><span class="param">	<span class="n">labels</span>,</span><span class="param">	<span class="n">alpha</span><span class="o">=</span><span class="mf">0.001</span>,</span><span class="param">	<span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span>,</span><span class="param">	<span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span>,</span><span class="param">	<span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-08</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="SingularRNN.train-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#SingularRNN.train"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="SingularRNN.train-173"><a href="#SingularRNN.train-173"><span class="linenos">173</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span> <span class="n">beta1</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span> <span class="n">beta2</span> <span class="o">=</span> <span class="mf">0.999</span><span class="p">,</span> <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">):</span>
</span><span id="SingularRNN.train-174"><a href="#SingularRNN.train-174"><span class="linenos">174</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="SingularRNN.train-175"><a href="#SingularRNN.train-175"><span class="linenos">175</span></a><span class="sd">        Trains the model on the given input sequences and labels using the Adam optimizer,</span>
</span><span id="SingularRNN.train-176"><a href="#SingularRNN.train-176"><span class="linenos">176</span></a><span class="sd">        then calculates and returns the loss.</span>
</span><span id="SingularRNN.train-177"><a href="#SingularRNN.train-177"><span class="linenos">177</span></a>
</span><span id="SingularRNN.train-178"><a href="#SingularRNN.train-178"><span class="linenos">178</span></a><span class="sd">        Args:</span>
</span><span id="SingularRNN.train-179"><a href="#SingularRNN.train-179"><span class="linenos">179</span></a><span class="sd">            inputData (ndarray): 3D array of input data with shape (batchSize, sequenceLength, inputSize).</span>
</span><span id="SingularRNN.train-180"><a href="#SingularRNN.train-180"><span class="linenos">180</span></a><span class="sd">            labels (ndarray): True labels corresponding to the input data.</span>
</span><span id="SingularRNN.train-181"><a href="#SingularRNN.train-181"><span class="linenos">181</span></a><span class="sd">            alpha (float, optional): Learning rate for the optimizer. Default is 0.001.</span>
</span><span id="SingularRNN.train-182"><a href="#SingularRNN.train-182"><span class="linenos">182</span></a><span class="sd">            beta1 (float, optional): Beta1 parameter for Adam optimizer. Default is 0.9.</span>
</span><span id="SingularRNN.train-183"><a href="#SingularRNN.train-183"><span class="linenos">183</span></a><span class="sd">            beta2 (float, optional): Beta2 parameter for Adam optimizer. Default is 0.999.</span>
</span><span id="SingularRNN.train-184"><a href="#SingularRNN.train-184"><span class="linenos">184</span></a><span class="sd">            epsilon (float, optional): Epsilon parameter for Adam optimizer to prevent division by zero. Default is 1e-8.</span>
</span><span id="SingularRNN.train-185"><a href="#SingularRNN.train-185"><span class="linenos">185</span></a>
</span><span id="SingularRNN.train-186"><a href="#SingularRNN.train-186"><span class="linenos">186</span></a><span class="sd">        Returns:</span>
</span><span id="SingularRNN.train-187"><a href="#SingularRNN.train-187"><span class="linenos">187</span></a><span class="sd">            float: Calculated loss between model outputs and true labels.</span>
</span><span id="SingularRNN.train-188"><a href="#SingularRNN.train-188"><span class="linenos">188</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="SingularRNN.train-189"><a href="#SingularRNN.train-189"><span class="linenos">189</span></a>        <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputData</span><span class="p">)</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Dimension wrong size, got </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputData</span><span class="p">)</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">, expected 3&quot;</span>
</span><span id="SingularRNN.train-190"><a href="#SingularRNN.train-190"><span class="linenos">190</span></a>        <span class="n">AllOutputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimiser</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
</span><span id="SingularRNN.train-191"><a href="#SingularRNN.train-191"><span class="linenos">191</span></a>        <span class="n">AllOutputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">AllOutputs</span><span class="p">,</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</span><span id="SingularRNN.train-192"><a href="#SingularRNN.train-192"><span class="linenos">192</span></a>        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lossFunction</span><span class="p">(</span><span class="n">AllOutputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</span><span id="SingularRNN.train-193"><a href="#SingularRNN.train-193"><span class="linenos">193</span></a>        <span class="k">return</span> <span class="n">loss</span>
</span></pre></div>


            <div class="docstring"><p>Trains the model on the given input sequences and labels using the Adam optimizer,
then calculates and returns the loss.</p>

<p>Args:
    inputData (ndarray): 3D array of input data with shape (batchSize, sequenceLength, inputSize).
    labels (ndarray): True labels corresponding to the input data.
    alpha (float, optional): Learning rate for the optimizer. Default is 0.001.
    beta1 (float, optional): Beta1 parameter for Adam optimizer. Default is 0.9.
    beta2 (float, optional): Beta2 parameter for Adam optimizer. Default is 0.999.
    epsilon (float, optional): Epsilon parameter for Adam optimizer to prevent division by zero. Default is 1e-8.</p>

<p>Returns:
    float: Calculated loss between model outputs and true labels.</p>
</div>


                            </div>
                </section>
                <section id="Transformer">
                            <input id="Transformer-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">Transformer</span>:

                <label class="view-source-button" for="Transformer-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Transformer"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Transformer-10"><a href="#Transformer-10"><span class="linenos"> 10</span></a><span class="k">class</span><span class="w"> </span><span class="nc">Transformer</span><span class="p">:</span>
</span><span id="Transformer-11"><a href="#Transformer-11"><span class="linenos"> 11</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">sequenceLength</span><span class="p">,</span> <span class="n">embedDimension</span><span class="p">,</span> <span class="n">vocabSize</span><span class="p">,</span> <span class="n">hasDecoderBlock</span><span class="p">):</span>
</span><span id="Transformer-12"><a href="#Transformer-12"><span class="linenos"> 12</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="Transformer-13"><a href="#Transformer-13"><span class="linenos"> 13</span></a><span class="sd">        Initializes the model with an Adam optimizer instance,</span>
</span><span id="Transformer-14"><a href="#Transformer-14"><span class="linenos"> 14</span></a><span class="sd">        an empty dictionary to store model blocks, and</span>
</span><span id="Transformer-15"><a href="#Transformer-15"><span class="linenos"> 15</span></a><span class="sd">        the output linear layer for decoder.        </span>
</span><span id="Transformer-16"><a href="#Transformer-16"><span class="linenos"> 16</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="Transformer-17"><a href="#Transformer-17"><span class="linenos"> 17</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">adam</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">forwardPropagation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">backwardPropagation</span><span class="p">)</span>
</span><span id="Transformer-18"><a href="#Transformer-18"><span class="linenos"> 18</span></a>        
</span><span id="Transformer-19"><a href="#Transformer-19"><span class="linenos"> 19</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="Transformer-20"><a href="#Transformer-20"><span class="linenos"> 20</span></a>
</span><span id="Transformer-21"><a href="#Transformer-21"><span class="linenos"> 21</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hasDecoderBlock</span> <span class="o">=</span> <span class="n">hasDecoderBlock</span>
</span><span id="Transformer-22"><a href="#Transformer-22"><span class="linenos"> 22</span></a>
</span><span id="Transformer-23"><a href="#Transformer-23"><span class="linenos"> 23</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hasDecoderBlock</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="Transformer-24"><a href="#Transformer-24"><span class="linenos"> 24</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">linearLayer</span> <span class="o">=</span> <span class="n">LinearLayer</span><span class="p">(</span>
</span><span id="Transformer-25"><a href="#Transformer-25"><span class="linenos"> 25</span></a>                <span class="n">batchSize</span><span class="o">=</span><span class="n">batchSize</span><span class="p">,</span>
</span><span id="Transformer-26"><a href="#Transformer-26"><span class="linenos"> 26</span></a>                <span class="n">sequenceLength</span><span class="o">=</span><span class="n">sequenceLength</span><span class="p">,</span>
</span><span id="Transformer-27"><a href="#Transformer-27"><span class="linenos"> 27</span></a>                <span class="n">inFeatures</span><span class="o">=</span><span class="n">embedDimension</span><span class="p">,</span>
</span><span id="Transformer-28"><a href="#Transformer-28"><span class="linenos"> 28</span></a>                <span class="n">outFeatures</span><span class="o">=</span><span class="n">vocabSize</span><span class="p">,</span>
</span><span id="Transformer-29"><a href="#Transformer-29"><span class="linenos"> 29</span></a>            <span class="p">)</span>
</span><span id="Transformer-30"><a href="#Transformer-30"><span class="linenos"> 30</span></a>
</span><span id="Transformer-31"><a href="#Transformer-31"><span class="linenos"> 31</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">addBlock</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">block</span><span class="p">):</span>
</span><span id="Transformer-32"><a href="#Transformer-32"><span class="linenos"> 32</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="Transformer-33"><a href="#Transformer-33"><span class="linenos"> 33</span></a><span class="sd">        Adds a new block/module to the model&#39;s blocks dictionary.</span>
</span><span id="Transformer-34"><a href="#Transformer-34"><span class="linenos"> 34</span></a>
</span><span id="Transformer-35"><a href="#Transformer-35"><span class="linenos"> 35</span></a><span class="sd">        Args:</span>
</span><span id="Transformer-36"><a href="#Transformer-36"><span class="linenos"> 36</span></a><span class="sd">            block (object): A model block or layer to be added.</span>
</span><span id="Transformer-37"><a href="#Transformer-37"><span class="linenos"> 37</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="Transformer-38"><a href="#Transformer-38"><span class="linenos"> 38</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">):</span> <span class="n">block</span><span class="p">})</span>
</span><span id="Transformer-39"><a href="#Transformer-39"><span class="linenos"> 39</span></a>    
</span><span id="Transformer-40"><a href="#Transformer-40"><span class="linenos"> 40</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forwardPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
</span><span id="Transformer-41"><a href="#Transformer-41"><span class="linenos"> 41</span></a>        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">:</span>
</span><span id="Transformer-42"><a href="#Transformer-42"><span class="linenos"> 42</span></a>            <span class="k">if</span><span class="p">(</span><span class="n">key</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
</span><span id="Transformer-43"><a href="#Transformer-43"><span class="linenos"> 43</span></a>                <span class="nb">input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">forwardPropagation</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
</span><span id="Transformer-44"><a href="#Transformer-44"><span class="linenos"> 44</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="Transformer-45"><a href="#Transformer-45"><span class="linenos"> 45</span></a>                <span class="nb">input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">forwardPropagation</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
</span><span id="Transformer-46"><a href="#Transformer-46"><span class="linenos"> 46</span></a>
</span><span id="Transformer-47"><a href="#Transformer-47"><span class="linenos"> 47</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hasDecoderBlock</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="Transformer-48"><a href="#Transformer-48"><span class="linenos"> 48</span></a>            <span class="nb">input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linearLayer</span><span class="o">.</span><span class="n">forwardPropagation</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="Transformer-49"><a href="#Transformer-49"><span class="linenos"> 49</span></a>            <span class="nb">input</span> <span class="o">=</span> <span class="n">softMax</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="Transformer-50"><a href="#Transformer-50"><span class="linenos"> 50</span></a>        
</span><span id="Transformer-51"><a href="#Transformer-51"><span class="linenos"> 51</span></a>        <span class="k">return</span> <span class="nb">input</span>
</span><span id="Transformer-52"><a href="#Transformer-52"><span class="linenos"> 52</span></a>
</span><span id="Transformer-53"><a href="#Transformer-53"><span class="linenos"> 53</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">backwardPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
</span><span id="Transformer-54"><a href="#Transformer-54"><span class="linenos"> 54</span></a>        <span class="n">Parameters</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="Transformer-55"><a href="#Transformer-55"><span class="linenos"> 55</span></a>        <span class="n">Gradients</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="Transformer-56"><a href="#Transformer-56"><span class="linenos"> 56</span></a>
</span><span id="Transformer-57"><a href="#Transformer-57"><span class="linenos"> 57</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hasDecoderBlock</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="Transformer-58"><a href="#Transformer-58"><span class="linenos"> 58</span></a>            <span class="n">inpDeriv</span> <span class="o">=</span> <span class="n">SoftMaxDerivative</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
</span><span id="Transformer-59"><a href="#Transformer-59"><span class="linenos"> 59</span></a>            <span class="n">Param</span><span class="p">,</span> <span class="n">Grad</span><span class="p">,</span> <span class="n">inpDeriv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linearLayer</span><span class="o">.</span><span class="n">backwardPropagation</span><span class="p">(</span><span class="n">inpDeriv</span><span class="p">)</span>
</span><span id="Transformer-60"><a href="#Transformer-60"><span class="linenos"> 60</span></a>            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">Param</span><span class="p">:</span>
</span><span id="Transformer-61"><a href="#Transformer-61"><span class="linenos"> 61</span></a>                <span class="n">Parameters</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="sa">f</span><span class="s2">&quot;Linear.</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">Param</span><span class="p">[</span><span class="n">key</span><span class="p">]})</span>
</span><span id="Transformer-62"><a href="#Transformer-62"><span class="linenos"> 62</span></a>                <span class="n">Gradients</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="sa">f</span><span class="s2">&quot;Linear.</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">Grad</span><span class="p">[</span><span class="n">key</span><span class="p">]})</span>   
</span><span id="Transformer-63"><a href="#Transformer-63"><span class="linenos"> 63</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="Transformer-64"><a href="#Transformer-64"><span class="linenos"> 64</span></a>            <span class="n">inpDeriv</span> <span class="o">=</span> <span class="n">MSEDerivative</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span><span id="Transformer-65"><a href="#Transformer-65"><span class="linenos"> 65</span></a>
</span><span id="Transformer-66"><a href="#Transformer-66"><span class="linenos"> 66</span></a>        <span class="k">for</span> <span class="n">blockKey</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">):</span>
</span><span id="Transformer-67"><a href="#Transformer-67"><span class="linenos"> 67</span></a>            <span class="n">Param</span><span class="p">,</span> <span class="n">Grad</span><span class="p">,</span> <span class="n">inpDeriv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">blockKey</span><span class="p">]</span><span class="o">.</span><span class="n">blockBackPropagation</span><span class="p">(</span><span class="n">inpDeriv</span><span class="p">)</span>
</span><span id="Transformer-68"><a href="#Transformer-68"><span class="linenos"> 68</span></a>
</span><span id="Transformer-69"><a href="#Transformer-69"><span class="linenos"> 69</span></a>            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">Param</span><span class="p">:</span>
</span><span id="Transformer-70"><a href="#Transformer-70"><span class="linenos"> 70</span></a>                <span class="n">Parameters</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">blockKey</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">Param</span><span class="p">[</span><span class="n">key</span><span class="p">]})</span> <span class="c1"># block key: 2, key: ATT_WO</span>
</span><span id="Transformer-71"><a href="#Transformer-71"><span class="linenos"> 71</span></a>                <span class="n">Gradients</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">blockKey</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">Grad</span><span class="p">[</span><span class="n">key</span><span class="p">]})</span>   <span class="c1"># will become 2.ATT_WO</span>
</span><span id="Transformer-72"><a href="#Transformer-72"><span class="linenos"> 72</span></a>
</span><span id="Transformer-73"><a href="#Transformer-73"><span class="linenos"> 73</span></a>        <span class="k">return</span> <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span>
</span><span id="Transformer-74"><a href="#Transformer-74"><span class="linenos"> 74</span></a>
</span><span id="Transformer-75"><a href="#Transformer-75"><span class="linenos"> 75</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">optimiser</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">useBatches</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">):</span>
</span><span id="Transformer-76"><a href="#Transformer-76"><span class="linenos"> 76</span></a>        <span class="n">AllOutputs</span><span class="p">,</span> <span class="n">Parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adam</span><span class="o">.</span><span class="n">optimiser</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">useBatches</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>       
</span><span id="Transformer-77"><a href="#Transformer-77"><span class="linenos"> 77</span></a>        <span class="c1"># Parameters will be like [b1.w1, b1.w2 ... b2.w1, b2.w2 ... bn.w1, b2.w2]</span>
</span><span id="Transformer-78"><a href="#Transformer-78"><span class="linenos"> 78</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">)):</span>
</span><span id="Transformer-79"><a href="#Transformer-79"><span class="linenos"> 79</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.Norm1_gamma&quot;</span><span class="p">]</span>
</span><span id="Transformer-80"><a href="#Transformer-80"><span class="linenos"> 80</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.Norm1_beta&quot;</span><span class="p">]</span> 
</span><span id="Transformer-81"><a href="#Transformer-81"><span class="linenos"> 81</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.Norm2_gamma&quot;</span><span class="p">]</span> 
</span><span id="Transformer-82"><a href="#Transformer-82"><span class="linenos"> 82</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.Norm2_beta&quot;</span><span class="p">]</span>
</span><span id="Transformer-83"><a href="#Transformer-83"><span class="linenos"> 83</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">W1</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.FFN_W1&quot;</span><span class="p">]</span> 
</span><span id="Transformer-84"><a href="#Transformer-84"><span class="linenos"> 84</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">b1</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.FFN_b1&quot;</span><span class="p">]</span> 
</span><span id="Transformer-85"><a href="#Transformer-85"><span class="linenos"> 85</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.FFN_W2&quot;</span><span class="p">]</span> 
</span><span id="Transformer-86"><a href="#Transformer-86"><span class="linenos"> 86</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">b2</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.FFN_b2&quot;</span><span class="p">]</span> 
</span><span id="Transformer-87"><a href="#Transformer-87"><span class="linenos"> 87</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">outputWeight</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.ATT_WO&quot;</span><span class="p">]</span> 
</span><span id="Transformer-88"><a href="#Transformer-88"><span class="linenos"> 88</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">outputBias</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.ATT_BO&quot;</span><span class="p">]</span> 
</span><span id="Transformer-89"><a href="#Transformer-89"><span class="linenos"> 89</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">QueryWeights</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.ATT_WQ&quot;</span><span class="p">]</span> 
</span><span id="Transformer-90"><a href="#Transformer-90"><span class="linenos"> 90</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">KeyWeights</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.ATT_WK&quot;</span><span class="p">]</span> 
</span><span id="Transformer-91"><a href="#Transformer-91"><span class="linenos"> 91</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">ValueWeights</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.ATT_WV&quot;</span><span class="p">]</span>
</span><span id="Transformer-92"><a href="#Transformer-92"><span class="linenos"> 92</span></a>            <span class="k">if</span><span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
</span><span id="Transformer-93"><a href="#Transformer-93"><span class="linenos"> 93</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.Embed_W&quot;</span><span class="p">]</span>
</span><span id="Transformer-94"><a href="#Transformer-94"><span class="linenos"> 94</span></a>
</span><span id="Transformer-95"><a href="#Transformer-95"><span class="linenos"> 95</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hasDecoderBlock</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="Transformer-96"><a href="#Transformer-96"><span class="linenos"> 96</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">linearLayer</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;Linear.LO_W&quot;</span><span class="p">]</span>
</span><span id="Transformer-97"><a href="#Transformer-97"><span class="linenos"> 97</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">linearLayer</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;Linear.LO_b&quot;</span><span class="p">]</span> 
</span><span id="Transformer-98"><a href="#Transformer-98"><span class="linenos"> 98</span></a>
</span><span id="Transformer-99"><a href="#Transformer-99"><span class="linenos"> 99</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">Parameters</span>
</span><span id="Transformer-100"><a href="#Transformer-100"><span class="linenos">100</span></a>        <span class="k">return</span> <span class="n">AllOutputs</span><span class="p">,</span> <span class="n">Parameters</span>
</span><span id="Transformer-101"><a href="#Transformer-101"><span class="linenos">101</span></a>
</span><span id="Transformer-102"><a href="#Transformer-102"><span class="linenos">102</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">saveWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fileName</span> <span class="o">=</span> <span class="s2">&quot;TransformerParameters.npz&quot;</span><span class="p">):</span>
</span><span id="Transformer-103"><a href="#Transformer-103"><span class="linenos">103</span></a>        <span class="n">np</span><span class="o">.</span><span class="n">savez</span><span class="p">(</span><span class="n">fileName</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
</span><span id="Transformer-104"><a href="#Transformer-104"><span class="linenos">104</span></a>
</span><span id="Transformer-105"><a href="#Transformer-105"><span class="linenos">105</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">loadWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fileName</span> <span class="o">=</span> <span class="s2">&quot;TransformerParameters.npz&quot;</span><span class="p">):</span>
</span><span id="Transformer-106"><a href="#Transformer-106"><span class="linenos">106</span></a>        <span class="n">loaded</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">fileName</span><span class="p">)</span>
</span><span id="Transformer-107"><a href="#Transformer-107"><span class="linenos">107</span></a>
</span><span id="Transformer-108"><a href="#Transformer-108"><span class="linenos">108</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">)):</span>
</span><span id="Transformer-109"><a href="#Transformer-109"><span class="linenos">109</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.Norm1_gamma&quot;</span><span class="p">]</span>
</span><span id="Transformer-110"><a href="#Transformer-110"><span class="linenos">110</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.Norm1_beta&quot;</span><span class="p">]</span> 
</span><span id="Transformer-111"><a href="#Transformer-111"><span class="linenos">111</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.Norm2_gamma&quot;</span><span class="p">]</span> 
</span><span id="Transformer-112"><a href="#Transformer-112"><span class="linenos">112</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.Norm2_beta&quot;</span><span class="p">]</span>
</span><span id="Transformer-113"><a href="#Transformer-113"><span class="linenos">113</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">W1</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.FFN_W1&quot;</span><span class="p">]</span> 
</span><span id="Transformer-114"><a href="#Transformer-114"><span class="linenos">114</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">b1</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.FFN_b1&quot;</span><span class="p">]</span> 
</span><span id="Transformer-115"><a href="#Transformer-115"><span class="linenos">115</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.FFN_W2&quot;</span><span class="p">]</span> 
</span><span id="Transformer-116"><a href="#Transformer-116"><span class="linenos">116</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">b2</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.FFN_b2&quot;</span><span class="p">]</span> 
</span><span id="Transformer-117"><a href="#Transformer-117"><span class="linenos">117</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">outputWeight</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.ATT_WO&quot;</span><span class="p">]</span> 
</span><span id="Transformer-118"><a href="#Transformer-118"><span class="linenos">118</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">outputBias</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.ATT_BO&quot;</span><span class="p">]</span> 
</span><span id="Transformer-119"><a href="#Transformer-119"><span class="linenos">119</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">QueryWeights</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.ATT_WQ&quot;</span><span class="p">]</span> 
</span><span id="Transformer-120"><a href="#Transformer-120"><span class="linenos">120</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">KeyWeights</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.ATT_WK&quot;</span><span class="p">]</span> 
</span><span id="Transformer-121"><a href="#Transformer-121"><span class="linenos">121</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">ValueWeights</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.ATT_WV&quot;</span><span class="p">]</span>
</span><span id="Transformer-122"><a href="#Transformer-122"><span class="linenos">122</span></a>            <span class="k">if</span><span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
</span><span id="Transformer-123"><a href="#Transformer-123"><span class="linenos">123</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.Embed_W&quot;</span><span class="p">]</span>
</span><span id="Transformer-124"><a href="#Transformer-124"><span class="linenos">124</span></a>
</span><span id="Transformer-125"><a href="#Transformer-125"><span class="linenos">125</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hasDecoderBlock</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="Transformer-126"><a href="#Transformer-126"><span class="linenos">126</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">linearLayer</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;Linear.LO_W&quot;</span><span class="p">]</span>
</span><span id="Transformer-127"><a href="#Transformer-127"><span class="linenos">127</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">linearLayer</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;Linear.LO_b&quot;</span><span class="p">]</span> 
</span><span id="Transformer-128"><a href="#Transformer-128"><span class="linenos">128</span></a>
</span><span id="Transformer-129"><a href="#Transformer-129"><span class="linenos">129</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">useBatches</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">batchSize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span> <span class="n">beta1</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span> <span class="n">beta2</span> <span class="o">=</span> <span class="mf">0.999</span><span class="p">,</span> <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">):</span>
</span><span id="Transformer-130"><a href="#Transformer-130"><span class="linenos">130</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="Transformer-131"><a href="#Transformer-131"><span class="linenos">131</span></a><span class="sd">        Trains the model on the input data and labels using the Adam optimizer,</span>
</span><span id="Transformer-132"><a href="#Transformer-132"><span class="linenos">132</span></a><span class="sd">        optionally with batching, and returns the mean squared error loss.</span>
</span><span id="Transformer-133"><a href="#Transformer-133"><span class="linenos">133</span></a>
</span><span id="Transformer-134"><a href="#Transformer-134"><span class="linenos">134</span></a><span class="sd">        Args:</span>
</span><span id="Transformer-135"><a href="#Transformer-135"><span class="linenos">135</span></a><span class="sd">            inputData (ndarray): Input data for training.</span>
</span><span id="Transformer-136"><a href="#Transformer-136"><span class="linenos">136</span></a><span class="sd">            labels (ndarray): True labels corresponding to the input data.</span>
</span><span id="Transformer-137"><a href="#Transformer-137"><span class="linenos">137</span></a><span class="sd">            useBatches (bool, optional): Whether to use mini-batching. Default is False.</span>
</span><span id="Transformer-138"><a href="#Transformer-138"><span class="linenos">138</span></a><span class="sd">            batchSize (int, optional): Size of each batch if batching is used. Default is 16.</span>
</span><span id="Transformer-139"><a href="#Transformer-139"><span class="linenos">139</span></a><span class="sd">            alpha (float, optional): Learning rate for the Adam optimizer. Default is 0.001.</span>
</span><span id="Transformer-140"><a href="#Transformer-140"><span class="linenos">140</span></a><span class="sd">            beta1 (float, optional): Beta1 parameter for Adam optimizer. Default is 0.9.</span>
</span><span id="Transformer-141"><a href="#Transformer-141"><span class="linenos">141</span></a><span class="sd">            beta2 (float, optional): Beta2 parameter for Adam optimizer. Default is 0.999.</span>
</span><span id="Transformer-142"><a href="#Transformer-142"><span class="linenos">142</span></a><span class="sd">            epsilon (float, optional): Epsilon parameter to prevent division by zero in Adam optimizer. Default is 1e-8.</span>
</span><span id="Transformer-143"><a href="#Transformer-143"><span class="linenos">143</span></a>
</span><span id="Transformer-144"><a href="#Transformer-144"><span class="linenos">144</span></a><span class="sd">        Returns:</span>
</span><span id="Transformer-145"><a href="#Transformer-145"><span class="linenos">145</span></a><span class="sd">            float: Mean squared error loss between predicted outputs and true labels.</span>
</span><span id="Transformer-146"><a href="#Transformer-146"><span class="linenos">146</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="Transformer-147"><a href="#Transformer-147"><span class="linenos">147</span></a>        <span class="n">AllOutputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimiser</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">useBatches</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
</span><span id="Transformer-148"><a href="#Transformer-148"><span class="linenos">148</span></a>        <span class="n">loss</span> <span class="o">=</span> <span class="n">MSELossFunction</span><span class="p">(</span><span class="n">AllOutputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</span><span id="Transformer-149"><a href="#Transformer-149"><span class="linenos">149</span></a>        <span class="k">return</span> <span class="n">loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputData</span><span class="p">)</span>
</span></pre></div>


    

                            <div id="Transformer.__init__" class="classattr">
                                        <input id="Transformer.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">Transformer</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">batchSize</span>,</span><span class="param">	<span class="n">sequenceLength</span>,</span><span class="param">	<span class="n">embedDimension</span>,</span><span class="param">	<span class="n">vocabSize</span>,</span><span class="param">	<span class="n">hasDecoderBlock</span></span>)</span>

                <label class="view-source-button" for="Transformer.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Transformer.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Transformer.__init__-11"><a href="#Transformer.__init__-11"><span class="linenos">11</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">sequenceLength</span><span class="p">,</span> <span class="n">embedDimension</span><span class="p">,</span> <span class="n">vocabSize</span><span class="p">,</span> <span class="n">hasDecoderBlock</span><span class="p">):</span>
</span><span id="Transformer.__init__-12"><a href="#Transformer.__init__-12"><span class="linenos">12</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="Transformer.__init__-13"><a href="#Transformer.__init__-13"><span class="linenos">13</span></a><span class="sd">        Initializes the model with an Adam optimizer instance,</span>
</span><span id="Transformer.__init__-14"><a href="#Transformer.__init__-14"><span class="linenos">14</span></a><span class="sd">        an empty dictionary to store model blocks, and</span>
</span><span id="Transformer.__init__-15"><a href="#Transformer.__init__-15"><span class="linenos">15</span></a><span class="sd">        the output linear layer for decoder.        </span>
</span><span id="Transformer.__init__-16"><a href="#Transformer.__init__-16"><span class="linenos">16</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="Transformer.__init__-17"><a href="#Transformer.__init__-17"><span class="linenos">17</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">adam</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">forwardPropagation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">backwardPropagation</span><span class="p">)</span>
</span><span id="Transformer.__init__-18"><a href="#Transformer.__init__-18"><span class="linenos">18</span></a>        
</span><span id="Transformer.__init__-19"><a href="#Transformer.__init__-19"><span class="linenos">19</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="Transformer.__init__-20"><a href="#Transformer.__init__-20"><span class="linenos">20</span></a>
</span><span id="Transformer.__init__-21"><a href="#Transformer.__init__-21"><span class="linenos">21</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hasDecoderBlock</span> <span class="o">=</span> <span class="n">hasDecoderBlock</span>
</span><span id="Transformer.__init__-22"><a href="#Transformer.__init__-22"><span class="linenos">22</span></a>
</span><span id="Transformer.__init__-23"><a href="#Transformer.__init__-23"><span class="linenos">23</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hasDecoderBlock</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="Transformer.__init__-24"><a href="#Transformer.__init__-24"><span class="linenos">24</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">linearLayer</span> <span class="o">=</span> <span class="n">LinearLayer</span><span class="p">(</span>
</span><span id="Transformer.__init__-25"><a href="#Transformer.__init__-25"><span class="linenos">25</span></a>                <span class="n">batchSize</span><span class="o">=</span><span class="n">batchSize</span><span class="p">,</span>
</span><span id="Transformer.__init__-26"><a href="#Transformer.__init__-26"><span class="linenos">26</span></a>                <span class="n">sequenceLength</span><span class="o">=</span><span class="n">sequenceLength</span><span class="p">,</span>
</span><span id="Transformer.__init__-27"><a href="#Transformer.__init__-27"><span class="linenos">27</span></a>                <span class="n">inFeatures</span><span class="o">=</span><span class="n">embedDimension</span><span class="p">,</span>
</span><span id="Transformer.__init__-28"><a href="#Transformer.__init__-28"><span class="linenos">28</span></a>                <span class="n">outFeatures</span><span class="o">=</span><span class="n">vocabSize</span><span class="p">,</span>
</span><span id="Transformer.__init__-29"><a href="#Transformer.__init__-29"><span class="linenos">29</span></a>            <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Initializes the model with an Adam optimizer instance,
an empty dictionary to store model blocks, and
the output linear layer for decoder.</p>
</div>


                            </div>
                            <div id="Transformer.adam" class="classattr">
                                <div class="attr variable">
            <span class="name">adam</span>

        
    </div>
    <a class="headerlink" href="#Transformer.adam"></a>
    
    

                            </div>
                            <div id="Transformer.blocks" class="classattr">
                                <div class="attr variable">
            <span class="name">blocks</span>

        
    </div>
    <a class="headerlink" href="#Transformer.blocks"></a>
    
    

                            </div>
                            <div id="Transformer.hasDecoderBlock" class="classattr">
                                <div class="attr variable">
            <span class="name">hasDecoderBlock</span>

        
    </div>
    <a class="headerlink" href="#Transformer.hasDecoderBlock"></a>
    
    

                            </div>
                            <div id="Transformer.addBlock" class="classattr">
                                        <input id="Transformer.addBlock-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">addBlock</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">block</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="Transformer.addBlock-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Transformer.addBlock"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Transformer.addBlock-31"><a href="#Transformer.addBlock-31"><span class="linenos">31</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">addBlock</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">block</span><span class="p">):</span>
</span><span id="Transformer.addBlock-32"><a href="#Transformer.addBlock-32"><span class="linenos">32</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="Transformer.addBlock-33"><a href="#Transformer.addBlock-33"><span class="linenos">33</span></a><span class="sd">        Adds a new block/module to the model&#39;s blocks dictionary.</span>
</span><span id="Transformer.addBlock-34"><a href="#Transformer.addBlock-34"><span class="linenos">34</span></a>
</span><span id="Transformer.addBlock-35"><a href="#Transformer.addBlock-35"><span class="linenos">35</span></a><span class="sd">        Args:</span>
</span><span id="Transformer.addBlock-36"><a href="#Transformer.addBlock-36"><span class="linenos">36</span></a><span class="sd">            block (object): A model block or layer to be added.</span>
</span><span id="Transformer.addBlock-37"><a href="#Transformer.addBlock-37"><span class="linenos">37</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="Transformer.addBlock-38"><a href="#Transformer.addBlock-38"><span class="linenos">38</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">):</span> <span class="n">block</span><span class="p">})</span>
</span></pre></div>


            <div class="docstring"><p>Adds a new block/module to the model's blocks dictionary.</p>

<p>Args:
    block (object): A model block or layer to be added.</p>
</div>


                            </div>
                            <div id="Transformer.forwardPropagation" class="classattr">
                                        <input id="Transformer.forwardPropagation-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forwardPropagation</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="nb">input</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="Transformer.forwardPropagation-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Transformer.forwardPropagation"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Transformer.forwardPropagation-40"><a href="#Transformer.forwardPropagation-40"><span class="linenos">40</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forwardPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
</span><span id="Transformer.forwardPropagation-41"><a href="#Transformer.forwardPropagation-41"><span class="linenos">41</span></a>        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">:</span>
</span><span id="Transformer.forwardPropagation-42"><a href="#Transformer.forwardPropagation-42"><span class="linenos">42</span></a>            <span class="k">if</span><span class="p">(</span><span class="n">key</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
</span><span id="Transformer.forwardPropagation-43"><a href="#Transformer.forwardPropagation-43"><span class="linenos">43</span></a>                <span class="nb">input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">forwardPropagation</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
</span><span id="Transformer.forwardPropagation-44"><a href="#Transformer.forwardPropagation-44"><span class="linenos">44</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="Transformer.forwardPropagation-45"><a href="#Transformer.forwardPropagation-45"><span class="linenos">45</span></a>                <span class="nb">input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">forwardPropagation</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
</span><span id="Transformer.forwardPropagation-46"><a href="#Transformer.forwardPropagation-46"><span class="linenos">46</span></a>
</span><span id="Transformer.forwardPropagation-47"><a href="#Transformer.forwardPropagation-47"><span class="linenos">47</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hasDecoderBlock</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="Transformer.forwardPropagation-48"><a href="#Transformer.forwardPropagation-48"><span class="linenos">48</span></a>            <span class="nb">input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linearLayer</span><span class="o">.</span><span class="n">forwardPropagation</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="Transformer.forwardPropagation-49"><a href="#Transformer.forwardPropagation-49"><span class="linenos">49</span></a>            <span class="nb">input</span> <span class="o">=</span> <span class="n">softMax</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="Transformer.forwardPropagation-50"><a href="#Transformer.forwardPropagation-50"><span class="linenos">50</span></a>        
</span><span id="Transformer.forwardPropagation-51"><a href="#Transformer.forwardPropagation-51"><span class="linenos">51</span></a>        <span class="k">return</span> <span class="nb">input</span>
</span></pre></div>


    

                            </div>
                            <div id="Transformer.backwardPropagation" class="classattr">
                                        <input id="Transformer.backwardPropagation-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">backwardPropagation</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">output</span>, </span><span class="param"><span class="n">labels</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="Transformer.backwardPropagation-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Transformer.backwardPropagation"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Transformer.backwardPropagation-53"><a href="#Transformer.backwardPropagation-53"><span class="linenos">53</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">backwardPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
</span><span id="Transformer.backwardPropagation-54"><a href="#Transformer.backwardPropagation-54"><span class="linenos">54</span></a>        <span class="n">Parameters</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="Transformer.backwardPropagation-55"><a href="#Transformer.backwardPropagation-55"><span class="linenos">55</span></a>        <span class="n">Gradients</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="Transformer.backwardPropagation-56"><a href="#Transformer.backwardPropagation-56"><span class="linenos">56</span></a>
</span><span id="Transformer.backwardPropagation-57"><a href="#Transformer.backwardPropagation-57"><span class="linenos">57</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hasDecoderBlock</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="Transformer.backwardPropagation-58"><a href="#Transformer.backwardPropagation-58"><span class="linenos">58</span></a>            <span class="n">inpDeriv</span> <span class="o">=</span> <span class="n">SoftMaxDerivative</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
</span><span id="Transformer.backwardPropagation-59"><a href="#Transformer.backwardPropagation-59"><span class="linenos">59</span></a>            <span class="n">Param</span><span class="p">,</span> <span class="n">Grad</span><span class="p">,</span> <span class="n">inpDeriv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linearLayer</span><span class="o">.</span><span class="n">backwardPropagation</span><span class="p">(</span><span class="n">inpDeriv</span><span class="p">)</span>
</span><span id="Transformer.backwardPropagation-60"><a href="#Transformer.backwardPropagation-60"><span class="linenos">60</span></a>            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">Param</span><span class="p">:</span>
</span><span id="Transformer.backwardPropagation-61"><a href="#Transformer.backwardPropagation-61"><span class="linenos">61</span></a>                <span class="n">Parameters</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="sa">f</span><span class="s2">&quot;Linear.</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">Param</span><span class="p">[</span><span class="n">key</span><span class="p">]})</span>
</span><span id="Transformer.backwardPropagation-62"><a href="#Transformer.backwardPropagation-62"><span class="linenos">62</span></a>                <span class="n">Gradients</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="sa">f</span><span class="s2">&quot;Linear.</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">Grad</span><span class="p">[</span><span class="n">key</span><span class="p">]})</span>   
</span><span id="Transformer.backwardPropagation-63"><a href="#Transformer.backwardPropagation-63"><span class="linenos">63</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="Transformer.backwardPropagation-64"><a href="#Transformer.backwardPropagation-64"><span class="linenos">64</span></a>            <span class="n">inpDeriv</span> <span class="o">=</span> <span class="n">MSEDerivative</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span><span id="Transformer.backwardPropagation-65"><a href="#Transformer.backwardPropagation-65"><span class="linenos">65</span></a>
</span><span id="Transformer.backwardPropagation-66"><a href="#Transformer.backwardPropagation-66"><span class="linenos">66</span></a>        <span class="k">for</span> <span class="n">blockKey</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">):</span>
</span><span id="Transformer.backwardPropagation-67"><a href="#Transformer.backwardPropagation-67"><span class="linenos">67</span></a>            <span class="n">Param</span><span class="p">,</span> <span class="n">Grad</span><span class="p">,</span> <span class="n">inpDeriv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">blockKey</span><span class="p">]</span><span class="o">.</span><span class="n">blockBackPropagation</span><span class="p">(</span><span class="n">inpDeriv</span><span class="p">)</span>
</span><span id="Transformer.backwardPropagation-68"><a href="#Transformer.backwardPropagation-68"><span class="linenos">68</span></a>
</span><span id="Transformer.backwardPropagation-69"><a href="#Transformer.backwardPropagation-69"><span class="linenos">69</span></a>            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">Param</span><span class="p">:</span>
</span><span id="Transformer.backwardPropagation-70"><a href="#Transformer.backwardPropagation-70"><span class="linenos">70</span></a>                <span class="n">Parameters</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">blockKey</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">Param</span><span class="p">[</span><span class="n">key</span><span class="p">]})</span> <span class="c1"># block key: 2, key: ATT_WO</span>
</span><span id="Transformer.backwardPropagation-71"><a href="#Transformer.backwardPropagation-71"><span class="linenos">71</span></a>                <span class="n">Gradients</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">blockKey</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">Grad</span><span class="p">[</span><span class="n">key</span><span class="p">]})</span>   <span class="c1"># will become 2.ATT_WO</span>
</span><span id="Transformer.backwardPropagation-72"><a href="#Transformer.backwardPropagation-72"><span class="linenos">72</span></a>
</span><span id="Transformer.backwardPropagation-73"><a href="#Transformer.backwardPropagation-73"><span class="linenos">73</span></a>        <span class="k">return</span> <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span>
</span></pre></div>


    

                            </div>
                            <div id="Transformer.optimiser" class="classattr">
                                        <input id="Transformer.optimiser-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">optimiser</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">inputData</span>,</span><span class="param">	<span class="n">labels</span>,</span><span class="param">	<span class="n">useBatches</span>,</span><span class="param">	<span class="n">batchSize</span>,</span><span class="param">	<span class="n">alpha</span>,</span><span class="param">	<span class="n">beta1</span>,</span><span class="param">	<span class="n">beta2</span>,</span><span class="param">	<span class="n">epsilon</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="Transformer.optimiser-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Transformer.optimiser"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Transformer.optimiser-75"><a href="#Transformer.optimiser-75"><span class="linenos"> 75</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">optimiser</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">useBatches</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">):</span>
</span><span id="Transformer.optimiser-76"><a href="#Transformer.optimiser-76"><span class="linenos"> 76</span></a>        <span class="n">AllOutputs</span><span class="p">,</span> <span class="n">Parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adam</span><span class="o">.</span><span class="n">optimiser</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">useBatches</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>       
</span><span id="Transformer.optimiser-77"><a href="#Transformer.optimiser-77"><span class="linenos"> 77</span></a>        <span class="c1"># Parameters will be like [b1.w1, b1.w2 ... b2.w1, b2.w2 ... bn.w1, b2.w2]</span>
</span><span id="Transformer.optimiser-78"><a href="#Transformer.optimiser-78"><span class="linenos"> 78</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">)):</span>
</span><span id="Transformer.optimiser-79"><a href="#Transformer.optimiser-79"><span class="linenos"> 79</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.Norm1_gamma&quot;</span><span class="p">]</span>
</span><span id="Transformer.optimiser-80"><a href="#Transformer.optimiser-80"><span class="linenos"> 80</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.Norm1_beta&quot;</span><span class="p">]</span> 
</span><span id="Transformer.optimiser-81"><a href="#Transformer.optimiser-81"><span class="linenos"> 81</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.Norm2_gamma&quot;</span><span class="p">]</span> 
</span><span id="Transformer.optimiser-82"><a href="#Transformer.optimiser-82"><span class="linenos"> 82</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.Norm2_beta&quot;</span><span class="p">]</span>
</span><span id="Transformer.optimiser-83"><a href="#Transformer.optimiser-83"><span class="linenos"> 83</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">W1</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.FFN_W1&quot;</span><span class="p">]</span> 
</span><span id="Transformer.optimiser-84"><a href="#Transformer.optimiser-84"><span class="linenos"> 84</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">b1</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.FFN_b1&quot;</span><span class="p">]</span> 
</span><span id="Transformer.optimiser-85"><a href="#Transformer.optimiser-85"><span class="linenos"> 85</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.FFN_W2&quot;</span><span class="p">]</span> 
</span><span id="Transformer.optimiser-86"><a href="#Transformer.optimiser-86"><span class="linenos"> 86</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">b2</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.FFN_b2&quot;</span><span class="p">]</span> 
</span><span id="Transformer.optimiser-87"><a href="#Transformer.optimiser-87"><span class="linenos"> 87</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">outputWeight</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.ATT_WO&quot;</span><span class="p">]</span> 
</span><span id="Transformer.optimiser-88"><a href="#Transformer.optimiser-88"><span class="linenos"> 88</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">outputBias</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.ATT_BO&quot;</span><span class="p">]</span> 
</span><span id="Transformer.optimiser-89"><a href="#Transformer.optimiser-89"><span class="linenos"> 89</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">QueryWeights</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.ATT_WQ&quot;</span><span class="p">]</span> 
</span><span id="Transformer.optimiser-90"><a href="#Transformer.optimiser-90"><span class="linenos"> 90</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">KeyWeights</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.ATT_WK&quot;</span><span class="p">]</span> 
</span><span id="Transformer.optimiser-91"><a href="#Transformer.optimiser-91"><span class="linenos"> 91</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">ValueWeights</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.ATT_WV&quot;</span><span class="p">]</span>
</span><span id="Transformer.optimiser-92"><a href="#Transformer.optimiser-92"><span class="linenos"> 92</span></a>            <span class="k">if</span><span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
</span><span id="Transformer.optimiser-93"><a href="#Transformer.optimiser-93"><span class="linenos"> 93</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.Embed_W&quot;</span><span class="p">]</span>
</span><span id="Transformer.optimiser-94"><a href="#Transformer.optimiser-94"><span class="linenos"> 94</span></a>
</span><span id="Transformer.optimiser-95"><a href="#Transformer.optimiser-95"><span class="linenos"> 95</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hasDecoderBlock</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="Transformer.optimiser-96"><a href="#Transformer.optimiser-96"><span class="linenos"> 96</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">linearLayer</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;Linear.LO_W&quot;</span><span class="p">]</span>
</span><span id="Transformer.optimiser-97"><a href="#Transformer.optimiser-97"><span class="linenos"> 97</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">linearLayer</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;Linear.LO_b&quot;</span><span class="p">]</span> 
</span><span id="Transformer.optimiser-98"><a href="#Transformer.optimiser-98"><span class="linenos"> 98</span></a>
</span><span id="Transformer.optimiser-99"><a href="#Transformer.optimiser-99"><span class="linenos"> 99</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">Parameters</span>
</span><span id="Transformer.optimiser-100"><a href="#Transformer.optimiser-100"><span class="linenos">100</span></a>        <span class="k">return</span> <span class="n">AllOutputs</span><span class="p">,</span> <span class="n">Parameters</span>
</span></pre></div>


    

                            </div>
                            <div id="Transformer.saveWeights" class="classattr">
                                        <input id="Transformer.saveWeights-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">saveWeights</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">fileName</span><span class="o">=</span><span class="s1">&#39;TransformerParameters.npz&#39;</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="Transformer.saveWeights-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Transformer.saveWeights"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Transformer.saveWeights-102"><a href="#Transformer.saveWeights-102"><span class="linenos">102</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">saveWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fileName</span> <span class="o">=</span> <span class="s2">&quot;TransformerParameters.npz&quot;</span><span class="p">):</span>
</span><span id="Transformer.saveWeights-103"><a href="#Transformer.saveWeights-103"><span class="linenos">103</span></a>        <span class="n">np</span><span class="o">.</span><span class="n">savez</span><span class="p">(</span><span class="n">fileName</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
</span></pre></div>


    

                            </div>
                            <div id="Transformer.loadWeights" class="classattr">
                                        <input id="Transformer.loadWeights-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">loadWeights</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">fileName</span><span class="o">=</span><span class="s1">&#39;TransformerParameters.npz&#39;</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="Transformer.loadWeights-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Transformer.loadWeights"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Transformer.loadWeights-105"><a href="#Transformer.loadWeights-105"><span class="linenos">105</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">loadWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fileName</span> <span class="o">=</span> <span class="s2">&quot;TransformerParameters.npz&quot;</span><span class="p">):</span>
</span><span id="Transformer.loadWeights-106"><a href="#Transformer.loadWeights-106"><span class="linenos">106</span></a>        <span class="n">loaded</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">fileName</span><span class="p">)</span>
</span><span id="Transformer.loadWeights-107"><a href="#Transformer.loadWeights-107"><span class="linenos">107</span></a>
</span><span id="Transformer.loadWeights-108"><a href="#Transformer.loadWeights-108"><span class="linenos">108</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">)):</span>
</span><span id="Transformer.loadWeights-109"><a href="#Transformer.loadWeights-109"><span class="linenos">109</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.Norm1_gamma&quot;</span><span class="p">]</span>
</span><span id="Transformer.loadWeights-110"><a href="#Transformer.loadWeights-110"><span class="linenos">110</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.Norm1_beta&quot;</span><span class="p">]</span> 
</span><span id="Transformer.loadWeights-111"><a href="#Transformer.loadWeights-111"><span class="linenos">111</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.Norm2_gamma&quot;</span><span class="p">]</span> 
</span><span id="Transformer.loadWeights-112"><a href="#Transformer.loadWeights-112"><span class="linenos">112</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.Norm2_beta&quot;</span><span class="p">]</span>
</span><span id="Transformer.loadWeights-113"><a href="#Transformer.loadWeights-113"><span class="linenos">113</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">W1</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.FFN_W1&quot;</span><span class="p">]</span> 
</span><span id="Transformer.loadWeights-114"><a href="#Transformer.loadWeights-114"><span class="linenos">114</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">b1</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.FFN_b1&quot;</span><span class="p">]</span> 
</span><span id="Transformer.loadWeights-115"><a href="#Transformer.loadWeights-115"><span class="linenos">115</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.FFN_W2&quot;</span><span class="p">]</span> 
</span><span id="Transformer.loadWeights-116"><a href="#Transformer.loadWeights-116"><span class="linenos">116</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">b2</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.FFN_b2&quot;</span><span class="p">]</span> 
</span><span id="Transformer.loadWeights-117"><a href="#Transformer.loadWeights-117"><span class="linenos">117</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">outputWeight</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.ATT_WO&quot;</span><span class="p">]</span> 
</span><span id="Transformer.loadWeights-118"><a href="#Transformer.loadWeights-118"><span class="linenos">118</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">outputBias</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.ATT_BO&quot;</span><span class="p">]</span> 
</span><span id="Transformer.loadWeights-119"><a href="#Transformer.loadWeights-119"><span class="linenos">119</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">QueryWeights</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.ATT_WQ&quot;</span><span class="p">]</span> 
</span><span id="Transformer.loadWeights-120"><a href="#Transformer.loadWeights-120"><span class="linenos">120</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">KeyWeights</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.ATT_WK&quot;</span><span class="p">]</span> 
</span><span id="Transformer.loadWeights-121"><a href="#Transformer.loadWeights-121"><span class="linenos">121</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">ValueWeights</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.ATT_WV&quot;</span><span class="p">]</span>
</span><span id="Transformer.loadWeights-122"><a href="#Transformer.loadWeights-122"><span class="linenos">122</span></a>            <span class="k">if</span><span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
</span><span id="Transformer.loadWeights-123"><a href="#Transformer.loadWeights-123"><span class="linenos">123</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.Embed_W&quot;</span><span class="p">]</span>
</span><span id="Transformer.loadWeights-124"><a href="#Transformer.loadWeights-124"><span class="linenos">124</span></a>
</span><span id="Transformer.loadWeights-125"><a href="#Transformer.loadWeights-125"><span class="linenos">125</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hasDecoderBlock</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="Transformer.loadWeights-126"><a href="#Transformer.loadWeights-126"><span class="linenos">126</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">linearLayer</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;Linear.LO_W&quot;</span><span class="p">]</span>
</span><span id="Transformer.loadWeights-127"><a href="#Transformer.loadWeights-127"><span class="linenos">127</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">linearLayer</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;Linear.LO_b&quot;</span><span class="p">]</span> 
</span></pre></div>


    

                            </div>
                            <div id="Transformer.train" class="classattr">
                                        <input id="Transformer.train-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">train</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">inputData</span>,</span><span class="param">	<span class="n">labels</span>,</span><span class="param">	<span class="n">useBatches</span><span class="o">=</span><span class="kc">False</span>,</span><span class="param">	<span class="n">batchSize</span><span class="o">=</span><span class="mi">16</span>,</span><span class="param">	<span class="n">alpha</span><span class="o">=</span><span class="mf">0.001</span>,</span><span class="param">	<span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span>,</span><span class="param">	<span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span>,</span><span class="param">	<span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-08</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="Transformer.train-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Transformer.train"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Transformer.train-129"><a href="#Transformer.train-129"><span class="linenos">129</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">useBatches</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">batchSize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span> <span class="n">beta1</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span> <span class="n">beta2</span> <span class="o">=</span> <span class="mf">0.999</span><span class="p">,</span> <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">):</span>
</span><span id="Transformer.train-130"><a href="#Transformer.train-130"><span class="linenos">130</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="Transformer.train-131"><a href="#Transformer.train-131"><span class="linenos">131</span></a><span class="sd">        Trains the model on the input data and labels using the Adam optimizer,</span>
</span><span id="Transformer.train-132"><a href="#Transformer.train-132"><span class="linenos">132</span></a><span class="sd">        optionally with batching, and returns the mean squared error loss.</span>
</span><span id="Transformer.train-133"><a href="#Transformer.train-133"><span class="linenos">133</span></a>
</span><span id="Transformer.train-134"><a href="#Transformer.train-134"><span class="linenos">134</span></a><span class="sd">        Args:</span>
</span><span id="Transformer.train-135"><a href="#Transformer.train-135"><span class="linenos">135</span></a><span class="sd">            inputData (ndarray): Input data for training.</span>
</span><span id="Transformer.train-136"><a href="#Transformer.train-136"><span class="linenos">136</span></a><span class="sd">            labels (ndarray): True labels corresponding to the input data.</span>
</span><span id="Transformer.train-137"><a href="#Transformer.train-137"><span class="linenos">137</span></a><span class="sd">            useBatches (bool, optional): Whether to use mini-batching. Default is False.</span>
</span><span id="Transformer.train-138"><a href="#Transformer.train-138"><span class="linenos">138</span></a><span class="sd">            batchSize (int, optional): Size of each batch if batching is used. Default is 16.</span>
</span><span id="Transformer.train-139"><a href="#Transformer.train-139"><span class="linenos">139</span></a><span class="sd">            alpha (float, optional): Learning rate for the Adam optimizer. Default is 0.001.</span>
</span><span id="Transformer.train-140"><a href="#Transformer.train-140"><span class="linenos">140</span></a><span class="sd">            beta1 (float, optional): Beta1 parameter for Adam optimizer. Default is 0.9.</span>
</span><span id="Transformer.train-141"><a href="#Transformer.train-141"><span class="linenos">141</span></a><span class="sd">            beta2 (float, optional): Beta2 parameter for Adam optimizer. Default is 0.999.</span>
</span><span id="Transformer.train-142"><a href="#Transformer.train-142"><span class="linenos">142</span></a><span class="sd">            epsilon (float, optional): Epsilon parameter to prevent division by zero in Adam optimizer. Default is 1e-8.</span>
</span><span id="Transformer.train-143"><a href="#Transformer.train-143"><span class="linenos">143</span></a>
</span><span id="Transformer.train-144"><a href="#Transformer.train-144"><span class="linenos">144</span></a><span class="sd">        Returns:</span>
</span><span id="Transformer.train-145"><a href="#Transformer.train-145"><span class="linenos">145</span></a><span class="sd">            float: Mean squared error loss between predicted outputs and true labels.</span>
</span><span id="Transformer.train-146"><a href="#Transformer.train-146"><span class="linenos">146</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="Transformer.train-147"><a href="#Transformer.train-147"><span class="linenos">147</span></a>        <span class="n">AllOutputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimiser</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">useBatches</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
</span><span id="Transformer.train-148"><a href="#Transformer.train-148"><span class="linenos">148</span></a>        <span class="n">loss</span> <span class="o">=</span> <span class="n">MSELossFunction</span><span class="p">(</span><span class="n">AllOutputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</span><span id="Transformer.train-149"><a href="#Transformer.train-149"><span class="linenos">149</span></a>        <span class="k">return</span> <span class="n">loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputData</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Trains the model on the input data and labels using the Adam optimizer,
optionally with batching, and returns the mean squared error loss.</p>

<p>Args:
    inputData (ndarray): Input data for training.
    labels (ndarray): True labels corresponding to the input data.
    useBatches (bool, optional): Whether to use mini-batching. Default is False.
    batchSize (int, optional): Size of each batch if batching is used. Default is 16.
    alpha (float, optional): Learning rate for the Adam optimizer. Default is 0.001.
    beta1 (float, optional): Beta1 parameter for Adam optimizer. Default is 0.9.
    beta2 (float, optional): Beta2 parameter for Adam optimizer. Default is 0.999.
    epsilon (float, optional): Epsilon parameter to prevent division by zero in Adam optimizer. Default is 1e-8.</p>

<p>Returns:
    float: Mean squared error loss between predicted outputs and true labels.</p>
</div>


                            </div>
                </section>
                <section id="TransformerBlock">
                            <input id="TransformerBlock-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">TransformerBlock</span>:

                <label class="view-source-button" for="TransformerBlock-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#TransformerBlock"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="TransformerBlock-60"><a href="#TransformerBlock-60"><span class="linenos"> 60</span></a><span class="k">class</span><span class="w"> </span><span class="nc">TransformerBlock</span><span class="p">:</span>
</span><span id="TransformerBlock-61"><a href="#TransformerBlock-61"><span class="linenos"> 61</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">sequenceLength</span><span class="p">,</span> <span class="n">vocabSize</span><span class="p">,</span> <span class="n">embedDimension</span><span class="p">,</span> <span class="n">positionalEmbddingDimension</span><span class="p">,</span> <span class="n">numberHeads</span><span class="p">,</span> <span class="n">hiddenDimensionFFN</span><span class="p">,</span> <span class="n">blockType</span> <span class="o">=</span> <span class="s2">&quot;encoder&quot;</span><span class="p">,</span> <span class="n">useResidual</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">useNorm</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">usePaddingMask</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">paddingMask</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
</span><span id="TransformerBlock-62"><a href="#TransformerBlock-62"><span class="linenos"> 62</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span> <span class="o">=</span> <span class="n">batchSize</span>
</span><span id="TransformerBlock-63"><a href="#TransformerBlock-63"><span class="linenos"> 63</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span> <span class="o">=</span> <span class="n">sequenceLength</span>
</span><span id="TransformerBlock-64"><a href="#TransformerBlock-64"><span class="linenos"> 64</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">vocabSize</span> <span class="o">=</span> <span class="n">vocabSize</span>
</span><span id="TransformerBlock-65"><a href="#TransformerBlock-65"><span class="linenos"> 65</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span> <span class="o">=</span> <span class="n">embedDimension</span>
</span><span id="TransformerBlock-66"><a href="#TransformerBlock-66"><span class="linenos"> 66</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">numberHeads</span> <span class="o">=</span> <span class="n">numberHeads</span>
</span><span id="TransformerBlock-67"><a href="#TransformerBlock-67"><span class="linenos"> 67</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hiddenDimensionFFN</span> <span class="o">=</span> <span class="n">hiddenDimensionFFN</span>
</span><span id="TransformerBlock-68"><a href="#TransformerBlock-68"><span class="linenos"> 68</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">useResidual</span> <span class="o">=</span> <span class="n">useResidual</span>
</span><span id="TransformerBlock-69"><a href="#TransformerBlock-69"><span class="linenos"> 69</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">useNorm</span> <span class="o">=</span> <span class="n">useNorm</span>
</span><span id="TransformerBlock-70"><a href="#TransformerBlock-70"><span class="linenos"> 70</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">positionalEmbddingDimension</span> <span class="o">=</span> <span class="n">positionalEmbddingDimension</span>
</span><span id="TransformerBlock-71"><a href="#TransformerBlock-71"><span class="linenos"> 71</span></a>
</span><span id="TransformerBlock-72"><a href="#TransformerBlock-72"><span class="linenos"> 72</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">isDecoder</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="TransformerBlock-73"><a href="#TransformerBlock-73"><span class="linenos"> 73</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">blockType</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;decoder&quot;</span><span class="p">):</span>
</span><span id="TransformerBlock-74"><a href="#TransformerBlock-74"><span class="linenos"> 74</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">isDecoder</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="TransformerBlock-75"><a href="#TransformerBlock-75"><span class="linenos"> 75</span></a>
</span><span id="TransformerBlock-76"><a href="#TransformerBlock-76"><span class="linenos"> 76</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">EmbeddingLayer</span><span class="p">(</span>
</span><span id="TransformerBlock-77"><a href="#TransformerBlock-77"><span class="linenos"> 77</span></a>            <span class="n">vocabSize</span><span class="o">=</span><span class="n">vocabSize</span><span class="p">,</span>
</span><span id="TransformerBlock-78"><a href="#TransformerBlock-78"><span class="linenos"> 78</span></a>            <span class="n">embedDimension</span><span class="o">=</span><span class="n">embedDimension</span><span class="p">,</span>
</span><span id="TransformerBlock-79"><a href="#TransformerBlock-79"><span class="linenos"> 79</span></a>        <span class="p">)</span>
</span><span id="TransformerBlock-80"><a href="#TransformerBlock-80"><span class="linenos"> 80</span></a>
</span><span id="TransformerBlock-81"><a href="#TransformerBlock-81"><span class="linenos"> 81</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">positionalEncoding</span> <span class="o">=</span> <span class="n">PositionalEncoding</span><span class="p">(</span>
</span><span id="TransformerBlock-82"><a href="#TransformerBlock-82"><span class="linenos"> 82</span></a>            <span class="n">maxDimension</span><span class="o">=</span><span class="n">positionalEmbddingDimension</span><span class="p">,</span>
</span><span id="TransformerBlock-83"><a href="#TransformerBlock-83"><span class="linenos"> 83</span></a>            <span class="n">embeddingSize</span><span class="o">=</span><span class="n">embedDimension</span>
</span><span id="TransformerBlock-84"><a href="#TransformerBlock-84"><span class="linenos"> 84</span></a>        <span class="p">)</span>
</span><span id="TransformerBlock-85"><a href="#TransformerBlock-85"><span class="linenos"> 85</span></a>
</span><span id="TransformerBlock-86"><a href="#TransformerBlock-86"><span class="linenos"> 86</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">MultiAttentionHeadLayer</span><span class="p">(</span>
</span><span id="TransformerBlock-87"><a href="#TransformerBlock-87"><span class="linenos"> 87</span></a>            <span class="n">batchSize</span><span class="o">=</span><span class="n">batchSize</span><span class="p">,</span>
</span><span id="TransformerBlock-88"><a href="#TransformerBlock-88"><span class="linenos"> 88</span></a>            <span class="n">sequenceLength</span><span class="o">=</span><span class="n">sequenceLength</span><span class="p">,</span>
</span><span id="TransformerBlock-89"><a href="#TransformerBlock-89"><span class="linenos"> 89</span></a>            <span class="n">embedDimension</span><span class="o">=</span><span class="n">embedDimension</span><span class="p">,</span>
</span><span id="TransformerBlock-90"><a href="#TransformerBlock-90"><span class="linenos"> 90</span></a>            <span class="n">numberOfHeads</span><span class="o">=</span><span class="n">numberHeads</span><span class="p">,</span>
</span><span id="TransformerBlock-91"><a href="#TransformerBlock-91"><span class="linenos"> 91</span></a>            <span class="n">QueryWeights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="TransformerBlock-92"><a href="#TransformerBlock-92"><span class="linenos"> 92</span></a>            <span class="n">KeyWeights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
</span><span id="TransformerBlock-93"><a href="#TransformerBlock-93"><span class="linenos"> 93</span></a>            <span class="n">ValueWeights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="TransformerBlock-94"><a href="#TransformerBlock-94"><span class="linenos"> 94</span></a>            <span class="n">outputWeight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="TransformerBlock-95"><a href="#TransformerBlock-95"><span class="linenos"> 95</span></a>            <span class="n">outputBias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="TransformerBlock-96"><a href="#TransformerBlock-96"><span class="linenos"> 96</span></a>            <span class="n">useCasualMasking</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">isDecoder</span><span class="p">,</span> <span class="c1"># if it is decoder it uses casual masking to mask future tokens</span>
</span><span id="TransformerBlock-97"><a href="#TransformerBlock-97"><span class="linenos"> 97</span></a>            <span class="n">usePaddingMask</span> <span class="o">=</span> <span class="n">usePaddingMask</span><span class="p">,</span> 
</span><span id="TransformerBlock-98"><a href="#TransformerBlock-98"><span class="linenos"> 98</span></a>            <span class="n">paddingMask</span> <span class="o">=</span> <span class="n">paddingMask</span><span class="p">,</span>
</span><span id="TransformerBlock-99"><a href="#TransformerBlock-99"><span class="linenos"> 99</span></a>        <span class="p">)</span>
</span><span id="TransformerBlock-100"><a href="#TransformerBlock-100"><span class="linenos">100</span></a>
</span><span id="TransformerBlock-101"><a href="#TransformerBlock-101"><span class="linenos">101</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">FFN</span> <span class="o">=</span> <span class="n">FeedForwardNetwork</span><span class="p">(</span>
</span><span id="TransformerBlock-102"><a href="#TransformerBlock-102"><span class="linenos">102</span></a>            <span class="n">inputDimension</span><span class="o">=</span><span class="n">embedDimension</span><span class="p">,</span>
</span><span id="TransformerBlock-103"><a href="#TransformerBlock-103"><span class="linenos">103</span></a>            <span class="n">hiddenDimension</span><span class="o">=</span><span class="n">hiddenDimensionFFN</span><span class="p">,</span>
</span><span id="TransformerBlock-104"><a href="#TransformerBlock-104"><span class="linenos">104</span></a>            <span class="n">W1</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="TransformerBlock-105"><a href="#TransformerBlock-105"><span class="linenos">105</span></a>            <span class="n">b1</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="TransformerBlock-106"><a href="#TransformerBlock-106"><span class="linenos">106</span></a>            <span class="n">W2</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="TransformerBlock-107"><a href="#TransformerBlock-107"><span class="linenos">107</span></a>            <span class="n">b2</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="TransformerBlock-108"><a href="#TransformerBlock-108"><span class="linenos">108</span></a>        <span class="p">)</span>
</span><span id="TransformerBlock-109"><a href="#TransformerBlock-109"><span class="linenos">109</span></a>
</span><span id="TransformerBlock-110"><a href="#TransformerBlock-110"><span class="linenos">110</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">useNorm</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="TransformerBlock-111"><a href="#TransformerBlock-111"><span class="linenos">111</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">NormLayer</span><span class="p">(</span><span class="n">embedDimension</span><span class="p">)</span>
</span><span id="TransformerBlock-112"><a href="#TransformerBlock-112"><span class="linenos">112</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">NormLayer</span><span class="p">(</span><span class="n">embedDimension</span><span class="p">)</span>
</span><span id="TransformerBlock-113"><a href="#TransformerBlock-113"><span class="linenos">113</span></a>
</span><span id="TransformerBlock-114"><a href="#TransformerBlock-114"><span class="linenos">114</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">adam</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">forwardPropagation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">backwardPropagation</span><span class="p">)</span>
</span><span id="TransformerBlock-115"><a href="#TransformerBlock-115"><span class="linenos">115</span></a>
</span><span id="TransformerBlock-116"><a href="#TransformerBlock-116"><span class="linenos">116</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forwardPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">firstBlock</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="TransformerBlock-117"><a href="#TransformerBlock-117"><span class="linenos">117</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">firstBlock</span> <span class="o">=</span> <span class="n">firstBlock</span>
</span><span id="TransformerBlock-118"><a href="#TransformerBlock-118"><span class="linenos">118</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">firstBlock</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="TransformerBlock-119"><a href="#TransformerBlock-119"><span class="linenos">119</span></a>            <span class="nb">input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">forwardPropagation</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="TransformerBlock-120"><a href="#TransformerBlock-120"><span class="linenos">120</span></a>
</span><span id="TransformerBlock-121"><a href="#TransformerBlock-121"><span class="linenos">121</span></a>            <span class="nb">input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">positionalEncoding</span><span class="o">.</span><span class="n">forwardPropagation</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="TransformerBlock-122"><a href="#TransformerBlock-122"><span class="linenos">122</span></a>
</span><span id="TransformerBlock-123"><a href="#TransformerBlock-123"><span class="linenos">123</span></a>        <span class="n">attentionOutput</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">forwardPropagation</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="TransformerBlock-124"><a href="#TransformerBlock-124"><span class="linenos">124</span></a>
</span><span id="TransformerBlock-125"><a href="#TransformerBlock-125"><span class="linenos">125</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">useResidual</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="TransformerBlock-126"><a href="#TransformerBlock-126"><span class="linenos">126</span></a>            <span class="n">attentionOutput</span> <span class="o">=</span> <span class="n">ResidualConnection</span><span class="o">.</span><span class="n">forwardPropagation</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">attentionOutput</span><span class="p">)</span>
</span><span id="TransformerBlock-127"><a href="#TransformerBlock-127"><span class="linenos">127</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">useNorm</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="TransformerBlock-128"><a href="#TransformerBlock-128"><span class="linenos">128</span></a>            <span class="n">attentionOutput</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">forwardPropagation</span><span class="p">(</span><span class="n">attentionOutput</span><span class="p">)</span>
</span><span id="TransformerBlock-129"><a href="#TransformerBlock-129"><span class="linenos">129</span></a>
</span><span id="TransformerBlock-130"><a href="#TransformerBlock-130"><span class="linenos">130</span></a>        <span class="n">FNNOutput</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">forwardPropagation</span><span class="p">(</span><span class="n">attentionOutput</span><span class="p">)</span>
</span><span id="TransformerBlock-131"><a href="#TransformerBlock-131"><span class="linenos">131</span></a>
</span><span id="TransformerBlock-132"><a href="#TransformerBlock-132"><span class="linenos">132</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">useResidual</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="TransformerBlock-133"><a href="#TransformerBlock-133"><span class="linenos">133</span></a>            <span class="n">FNNOutput</span> <span class="o">=</span> <span class="n">ResidualConnection</span><span class="o">.</span><span class="n">forwardPropagation</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">FNNOutput</span><span class="p">,</span> <span class="n">attentionOutput</span><span class="p">)</span>
</span><span id="TransformerBlock-134"><a href="#TransformerBlock-134"><span class="linenos">134</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">useNorm</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="TransformerBlock-135"><a href="#TransformerBlock-135"><span class="linenos">135</span></a>            <span class="n">FNNOutput</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">forwardPropagation</span><span class="p">(</span><span class="n">FNNOutput</span><span class="p">)</span>
</span><span id="TransformerBlock-136"><a href="#TransformerBlock-136"><span class="linenos">136</span></a>
</span><span id="TransformerBlock-137"><a href="#TransformerBlock-137"><span class="linenos">137</span></a>        <span class="k">return</span> <span class="n">FNNOutput</span>
</span><span id="TransformerBlock-138"><a href="#TransformerBlock-138"><span class="linenos">138</span></a>    
</span><span id="TransformerBlock-139"><a href="#TransformerBlock-139"><span class="linenos">139</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">backwardPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
</span><span id="TransformerBlock-140"><a href="#TransformerBlock-140"><span class="linenos">140</span></a>        <span class="k">assert</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;MSE Derivative expected derviative </span><span class="si">{</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">)</span><span class="si">}</span><span class="s2">, got </span><span class="si">{</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="TransformerBlock-141"><a href="#TransformerBlock-141"><span class="linenos">141</span></a>
</span><span id="TransformerBlock-142"><a href="#TransformerBlock-142"><span class="linenos">142</span></a>        <span class="n">InputDerivative</span> <span class="o">=</span> <span class="n">MSEDerivative</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span><span id="TransformerBlock-143"><a href="#TransformerBlock-143"><span class="linenos">143</span></a>        <span class="k">assert</span> <span class="n">InputDerivative</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;MSE Derivative expected derviative </span><span class="si">{</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">)</span><span class="si">}</span><span class="s2">, got </span><span class="si">{</span><span class="n">InputDerivative</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="TransformerBlock-144"><a href="#TransformerBlock-144"><span class="linenos">144</span></a>        
</span><span id="TransformerBlock-145"><a href="#TransformerBlock-145"><span class="linenos">145</span></a>        <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span><span class="p">,</span> <span class="n">InputDerivative</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">blockBackPropagation</span><span class="p">(</span><span class="n">InputDerivative</span><span class="p">)</span>
</span><span id="TransformerBlock-146"><a href="#TransformerBlock-146"><span class="linenos">146</span></a>
</span><span id="TransformerBlock-147"><a href="#TransformerBlock-147"><span class="linenos">147</span></a>        <span class="k">return</span> <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span>
</span><span id="TransformerBlock-148"><a href="#TransformerBlock-148"><span class="linenos">148</span></a>
</span><span id="TransformerBlock-149"><a href="#TransformerBlock-149"><span class="linenos">149</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">blockBackPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">InputDerivative</span><span class="p">):</span> <span class="c1">#, output, labels): </span>
</span><span id="TransformerBlock-150"><a href="#TransformerBlock-150"><span class="linenos">150</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="TransformerBlock-151"><a href="#TransformerBlock-151"><span class="linenos">151</span></a><span class="sd">        assert output.shape == (self.batchSize, self.sequenceLength, self.embedDimension), f&quot;MSE Derivative expected derviative {(self.batchSize, self.sequenceLength, self.embedDimension)}, got {output.shape}&quot;</span>
</span><span id="TransformerBlock-152"><a href="#TransformerBlock-152"><span class="linenos">152</span></a>
</span><span id="TransformerBlock-153"><a href="#TransformerBlock-153"><span class="linenos">153</span></a><span class="sd">        InputDerivative = MSEDerivative(output, labels, output.shape[-1])</span>
</span><span id="TransformerBlock-154"><a href="#TransformerBlock-154"><span class="linenos">154</span></a><span class="sd">        assert InputDerivative.shape == (self.batchSize, self.sequenceLength, self.embedDimension), f&quot;MSE Derivative expected derviative {(self.batchSize, self.sequenceLength, self.embedDimension)}, got {InputDerivative.shape}&quot;</span>
</span><span id="TransformerBlock-155"><a href="#TransformerBlock-155"><span class="linenos">155</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="TransformerBlock-156"><a href="#TransformerBlock-156"><span class="linenos">156</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">useNorm</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="TransformerBlock-157"><a href="#TransformerBlock-157"><span class="linenos">157</span></a>            <span class="n">InputDerivative</span><span class="p">,</span> <span class="n">norm2GammaDerivative</span><span class="p">,</span> <span class="n">norm2BetaDerivative</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">backwardPropagation</span><span class="p">(</span><span class="n">InputDerivative</span><span class="p">)</span>
</span><span id="TransformerBlock-158"><a href="#TransformerBlock-158"><span class="linenos">158</span></a>            <span class="k">assert</span> <span class="n">InputDerivative</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Norm2 expected derviative </span><span class="si">{</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">)</span><span class="si">}</span><span class="s2">, got </span><span class="si">{</span><span class="n">InputDerivative</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="TransformerBlock-159"><a href="#TransformerBlock-159"><span class="linenos">159</span></a>
</span><span id="TransformerBlock-160"><a href="#TransformerBlock-160"><span class="linenos">160</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">useResidual</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="TransformerBlock-161"><a href="#TransformerBlock-161"><span class="linenos">161</span></a>            <span class="n">residual2</span> <span class="o">=</span> <span class="n">InputDerivative</span>
</span><span id="TransformerBlock-162"><a href="#TransformerBlock-162"><span class="linenos">162</span></a>        
</span><span id="TransformerBlock-163"><a href="#TransformerBlock-163"><span class="linenos">163</span></a>        <span class="n">InputDerivative</span><span class="p">,</span> <span class="n">FFNWeightGradient1</span><span class="p">,</span> <span class="n">FFNBiasGradient1</span><span class="p">,</span> <span class="n">FFNWeightGradient2</span><span class="p">,</span> <span class="n">FFNBiasGradient2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">backwardPropagation</span><span class="p">(</span><span class="n">InputDerivative</span><span class="p">)</span>
</span><span id="TransformerBlock-164"><a href="#TransformerBlock-164"><span class="linenos">164</span></a>        <span class="k">assert</span> <span class="n">InputDerivative</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;FFN expected derviative </span><span class="si">{</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">)</span><span class="si">}</span><span class="s2">, got </span><span class="si">{</span><span class="n">InputDerivative</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="TransformerBlock-165"><a href="#TransformerBlock-165"><span class="linenos">165</span></a>
</span><span id="TransformerBlock-166"><a href="#TransformerBlock-166"><span class="linenos">166</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">useResidual</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="TransformerBlock-167"><a href="#TransformerBlock-167"><span class="linenos">167</span></a>            <span class="n">InputDerivative</span> <span class="o">+=</span> <span class="n">residual2</span>
</span><span id="TransformerBlock-168"><a href="#TransformerBlock-168"><span class="linenos">168</span></a>            <span class="k">assert</span> <span class="n">InputDerivative</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Residual1 expected derviative </span><span class="si">{</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">)</span><span class="si">}</span><span class="s2">, got </span><span class="si">{</span><span class="n">InputDerivative</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="TransformerBlock-169"><a href="#TransformerBlock-169"><span class="linenos">169</span></a>
</span><span id="TransformerBlock-170"><a href="#TransformerBlock-170"><span class="linenos">170</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">useNorm</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="TransformerBlock-171"><a href="#TransformerBlock-171"><span class="linenos">171</span></a>            <span class="n">InputDerivative</span><span class="p">,</span> <span class="n">norm1GammaDerivative</span><span class="p">,</span> <span class="n">norm1BetaDerivative</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">backwardPropagation</span><span class="p">(</span><span class="n">InputDerivative</span><span class="p">)</span>
</span><span id="TransformerBlock-172"><a href="#TransformerBlock-172"><span class="linenos">172</span></a>            <span class="k">assert</span> <span class="n">InputDerivative</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Norm1 expected derviative </span><span class="si">{</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">)</span><span class="si">}</span><span class="s2">, got </span><span class="si">{</span><span class="n">InputDerivative</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="TransformerBlock-173"><a href="#TransformerBlock-173"><span class="linenos">173</span></a>
</span><span id="TransformerBlock-174"><a href="#TransformerBlock-174"><span class="linenos">174</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">useResidual</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="TransformerBlock-175"><a href="#TransformerBlock-175"><span class="linenos">175</span></a>            <span class="n">residual1</span> <span class="o">=</span> <span class="n">InputDerivative</span>
</span><span id="TransformerBlock-176"><a href="#TransformerBlock-176"><span class="linenos">176</span></a>            
</span><span id="TransformerBlock-177"><a href="#TransformerBlock-177"><span class="linenos">177</span></a>        <span class="n">AttentionOutputWeightGradient</span><span class="p">,</span> <span class="n">AttentionOutputBiasGradient</span><span class="p">,</span> <span class="n">InputDerivative</span><span class="p">,</span> <span class="n">AttentionQueryWeightDerivative</span><span class="p">,</span> <span class="n">AttentionKeyWeightDerivative</span><span class="p">,</span> <span class="n">AttentionValueWeightDerivative</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">backwardPropagation</span><span class="p">(</span><span class="n">InputDerivative</span><span class="p">)</span>
</span><span id="TransformerBlock-178"><a href="#TransformerBlock-178"><span class="linenos">178</span></a>        <span class="k">assert</span> <span class="n">InputDerivative</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Attention expected derviative </span><span class="si">{</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">)</span><span class="si">}</span><span class="s2">, got </span><span class="si">{</span><span class="n">InputDerivative</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="TransformerBlock-179"><a href="#TransformerBlock-179"><span class="linenos">179</span></a>
</span><span id="TransformerBlock-180"><a href="#TransformerBlock-180"><span class="linenos">180</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">useResidual</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="TransformerBlock-181"><a href="#TransformerBlock-181"><span class="linenos">181</span></a>            <span class="n">InputDerivative</span> <span class="o">+=</span> <span class="n">residual1</span>
</span><span id="TransformerBlock-182"><a href="#TransformerBlock-182"><span class="linenos">182</span></a>            <span class="k">assert</span> <span class="n">InputDerivative</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Resiudal1 Derivative expected derviative </span><span class="si">{</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">)</span><span class="si">}</span><span class="s2">, got </span><span class="si">{</span><span class="n">InputDerivative</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="TransformerBlock-183"><a href="#TransformerBlock-183"><span class="linenos">183</span></a>
</span><span id="TransformerBlock-184"><a href="#TransformerBlock-184"><span class="linenos">184</span></a>        <span class="n">embeddingGradient</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="TransformerBlock-185"><a href="#TransformerBlock-185"><span class="linenos">185</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">firstBlock</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="TransformerBlock-186"><a href="#TransformerBlock-186"><span class="linenos">186</span></a>            <span class="n">embeddingGradient</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">backwardPropagation</span><span class="p">(</span><span class="n">InputDerivative</span><span class="p">)</span>
</span><span id="TransformerBlock-187"><a href="#TransformerBlock-187"><span class="linenos">187</span></a>            <span class="n">InputDerivative</span> <span class="o">=</span> <span class="n">embeddingGradient</span>
</span><span id="TransformerBlock-188"><a href="#TransformerBlock-188"><span class="linenos">188</span></a>
</span><span id="TransformerBlock-189"><a href="#TransformerBlock-189"><span class="linenos">189</span></a>        <span class="n">Parameters</span> <span class="o">=</span>  <span class="p">{</span>
</span><span id="TransformerBlock-190"><a href="#TransformerBlock-190"><span class="linenos">190</span></a>            <span class="s2">&quot;Norm1_gamma&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">gamma</span><span class="p">,</span> 
</span><span id="TransformerBlock-191"><a href="#TransformerBlock-191"><span class="linenos">191</span></a>            <span class="s2">&quot;Norm1_beta&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">beta</span><span class="p">,</span> 
</span><span id="TransformerBlock-192"><a href="#TransformerBlock-192"><span class="linenos">192</span></a>            <span class="s2">&quot;Norm2_gamma&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">gamma</span><span class="p">,</span> 
</span><span id="TransformerBlock-193"><a href="#TransformerBlock-193"><span class="linenos">193</span></a>            <span class="s2">&quot;Norm2_beta&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">beta</span><span class="p">,</span> 
</span><span id="TransformerBlock-194"><a href="#TransformerBlock-194"><span class="linenos">194</span></a>            <span class="s2">&quot;FFN_W1&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">W1</span><span class="p">,</span> 
</span><span id="TransformerBlock-195"><a href="#TransformerBlock-195"><span class="linenos">195</span></a>            <span class="s2">&quot;FFN_b1&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">b1</span><span class="p">,</span> 
</span><span id="TransformerBlock-196"><a href="#TransformerBlock-196"><span class="linenos">196</span></a>            <span class="s2">&quot;FFN_W2&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">W2</span><span class="p">,</span> 
</span><span id="TransformerBlock-197"><a href="#TransformerBlock-197"><span class="linenos">197</span></a>            <span class="s2">&quot;FFN_b2&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">b2</span><span class="p">,</span> 
</span><span id="TransformerBlock-198"><a href="#TransformerBlock-198"><span class="linenos">198</span></a>            <span class="s2">&quot;ATT_WO&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">outputWeight</span><span class="p">,</span> 
</span><span id="TransformerBlock-199"><a href="#TransformerBlock-199"><span class="linenos">199</span></a>            <span class="s2">&quot;ATT_BO&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">outputBias</span><span class="p">,</span> 
</span><span id="TransformerBlock-200"><a href="#TransformerBlock-200"><span class="linenos">200</span></a>            <span class="s2">&quot;ATT_WQ&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">QueryWeights</span><span class="p">,</span> 
</span><span id="TransformerBlock-201"><a href="#TransformerBlock-201"><span class="linenos">201</span></a>            <span class="s2">&quot;ATT_WK&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">KeyWeights</span><span class="p">,</span> 
</span><span id="TransformerBlock-202"><a href="#TransformerBlock-202"><span class="linenos">202</span></a>            <span class="s2">&quot;ATT_WV&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">ValueWeights</span><span class="p">,</span>
</span><span id="TransformerBlock-203"><a href="#TransformerBlock-203"><span class="linenos">203</span></a>            <span class="s2">&quot;Embed_W&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span>
</span><span id="TransformerBlock-204"><a href="#TransformerBlock-204"><span class="linenos">204</span></a>        <span class="p">}</span>
</span><span id="TransformerBlock-205"><a href="#TransformerBlock-205"><span class="linenos">205</span></a>  
</span><span id="TransformerBlock-206"><a href="#TransformerBlock-206"><span class="linenos">206</span></a>        <span class="n">Gradients</span> <span class="o">=</span>  <span class="p">{</span>
</span><span id="TransformerBlock-207"><a href="#TransformerBlock-207"><span class="linenos">207</span></a>            <span class="s2">&quot;Norm1_gamma&quot;</span><span class="p">:</span> <span class="n">norm1GammaDerivative</span><span class="p">,</span> 
</span><span id="TransformerBlock-208"><a href="#TransformerBlock-208"><span class="linenos">208</span></a>            <span class="s2">&quot;Norm1_beta&quot;</span><span class="p">:</span> <span class="n">norm1BetaDerivative</span><span class="p">,</span> 
</span><span id="TransformerBlock-209"><a href="#TransformerBlock-209"><span class="linenos">209</span></a>            <span class="s2">&quot;Norm2_gamma&quot;</span><span class="p">:</span> <span class="n">norm2GammaDerivative</span><span class="p">,</span> 
</span><span id="TransformerBlock-210"><a href="#TransformerBlock-210"><span class="linenos">210</span></a>            <span class="s2">&quot;Norm2_beta&quot;</span><span class="p">:</span> <span class="n">norm2BetaDerivative</span><span class="p">,</span> 
</span><span id="TransformerBlock-211"><a href="#TransformerBlock-211"><span class="linenos">211</span></a>            <span class="s2">&quot;FFN_W1&quot;</span><span class="p">:</span> <span class="n">FFNWeightGradient1</span><span class="p">,</span> 
</span><span id="TransformerBlock-212"><a href="#TransformerBlock-212"><span class="linenos">212</span></a>            <span class="s2">&quot;FFN_b1&quot;</span><span class="p">:</span> <span class="n">FFNBiasGradient1</span><span class="p">,</span> 
</span><span id="TransformerBlock-213"><a href="#TransformerBlock-213"><span class="linenos">213</span></a>            <span class="s2">&quot;FFN_W2&quot;</span><span class="p">:</span> <span class="n">FFNWeightGradient2</span><span class="p">,</span> 
</span><span id="TransformerBlock-214"><a href="#TransformerBlock-214"><span class="linenos">214</span></a>            <span class="s2">&quot;FFN_b2&quot;</span><span class="p">:</span> <span class="n">FFNBiasGradient2</span><span class="p">,</span> 
</span><span id="TransformerBlock-215"><a href="#TransformerBlock-215"><span class="linenos">215</span></a>            <span class="s2">&quot;ATT_WO&quot;</span><span class="p">:</span> <span class="n">AttentionOutputWeightGradient</span><span class="p">,</span> 
</span><span id="TransformerBlock-216"><a href="#TransformerBlock-216"><span class="linenos">216</span></a>            <span class="s2">&quot;ATT_BO&quot;</span><span class="p">:</span> <span class="n">AttentionOutputBiasGradient</span><span class="p">,</span> 
</span><span id="TransformerBlock-217"><a href="#TransformerBlock-217"><span class="linenos">217</span></a>            <span class="s2">&quot;ATT_WQ&quot;</span><span class="p">:</span> <span class="n">AttentionQueryWeightDerivative</span><span class="p">,</span> 
</span><span id="TransformerBlock-218"><a href="#TransformerBlock-218"><span class="linenos">218</span></a>            <span class="s2">&quot;ATT_WK&quot;</span><span class="p">:</span> <span class="n">AttentionKeyWeightDerivative</span><span class="p">,</span> 
</span><span id="TransformerBlock-219"><a href="#TransformerBlock-219"><span class="linenos">219</span></a>            <span class="s2">&quot;ATT_WV&quot;</span><span class="p">:</span> <span class="n">AttentionValueWeightDerivative</span><span class="p">,</span>
</span><span id="TransformerBlock-220"><a href="#TransformerBlock-220"><span class="linenos">220</span></a>            <span class="s2">&quot;Embed_W&quot;</span><span class="p">:</span> <span class="n">embeddingGradient</span><span class="p">,</span>
</span><span id="TransformerBlock-221"><a href="#TransformerBlock-221"><span class="linenos">221</span></a>        <span class="p">}</span>
</span><span id="TransformerBlock-222"><a href="#TransformerBlock-222"><span class="linenos">222</span></a>        
</span><span id="TransformerBlock-223"><a href="#TransformerBlock-223"><span class="linenos">223</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="s2">&quot;Embed_W&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
</span><span id="TransformerBlock-224"><a href="#TransformerBlock-224"><span class="linenos">224</span></a>            <span class="n">Parameters</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;Embed_W&quot;</span><span class="p">)</span>
</span><span id="TransformerBlock-225"><a href="#TransformerBlock-225"><span class="linenos">225</span></a>            <span class="n">Gradients</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;Embed_W&quot;</span><span class="p">)</span>
</span><span id="TransformerBlock-226"><a href="#TransformerBlock-226"><span class="linenos">226</span></a>
</span><span id="TransformerBlock-227"><a href="#TransformerBlock-227"><span class="linenos">227</span></a>        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">Gradients</span><span class="p">:</span>    
</span><span id="TransformerBlock-228"><a href="#TransformerBlock-228"><span class="linenos">228</span></a>            <span class="k">if</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="n">Parameters</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">ndim</span> <span class="o">+</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="TransformerBlock-229"><a href="#TransformerBlock-229"><span class="linenos">229</span></a>                <span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># without batches some parameters are (1, x, y) which becomes (x, y) so this turns them back into (x, y)</span>
</span><span id="TransformerBlock-230"><a href="#TransformerBlock-230"><span class="linenos">230</span></a>
</span><span id="TransformerBlock-231"><a href="#TransformerBlock-231"><span class="linenos">231</span></a>        <span class="k">return</span> <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span><span class="p">,</span> <span class="n">InputDerivative</span>
</span><span id="TransformerBlock-232"><a href="#TransformerBlock-232"><span class="linenos">232</span></a>    
</span><span id="TransformerBlock-233"><a href="#TransformerBlock-233"><span class="linenos">233</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">optimiser</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">useBatches</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">):</span>
</span><span id="TransformerBlock-234"><a href="#TransformerBlock-234"><span class="linenos">234</span></a>        <span class="n">AllOutputs</span><span class="p">,</span> <span class="n">Parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adam</span><span class="o">.</span><span class="n">optimiser</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">useBatches</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>       
</span><span id="TransformerBlock-235"><a href="#TransformerBlock-235"><span class="linenos">235</span></a>
</span><span id="TransformerBlock-236"><a href="#TransformerBlock-236"><span class="linenos">236</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;Norm1_gamma&quot;</span><span class="p">]</span>
</span><span id="TransformerBlock-237"><a href="#TransformerBlock-237"><span class="linenos">237</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;Norm1_beta&quot;</span><span class="p">]</span> 
</span><span id="TransformerBlock-238"><a href="#TransformerBlock-238"><span class="linenos">238</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;Norm2_gamma&quot;</span><span class="p">]</span> 
</span><span id="TransformerBlock-239"><a href="#TransformerBlock-239"><span class="linenos">239</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;Norm2_beta&quot;</span><span class="p">]</span>
</span><span id="TransformerBlock-240"><a href="#TransformerBlock-240"><span class="linenos">240</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">W1</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;FFN_W1&quot;</span><span class="p">]</span> 
</span><span id="TransformerBlock-241"><a href="#TransformerBlock-241"><span class="linenos">241</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">b1</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;FFN_b1&quot;</span><span class="p">]</span> 
</span><span id="TransformerBlock-242"><a href="#TransformerBlock-242"><span class="linenos">242</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;FFN_W2&quot;</span><span class="p">]</span> 
</span><span id="TransformerBlock-243"><a href="#TransformerBlock-243"><span class="linenos">243</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">b2</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;FFN_b2&quot;</span><span class="p">]</span> 
</span><span id="TransformerBlock-244"><a href="#TransformerBlock-244"><span class="linenos">244</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">outputWeight</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;ATT_WO&quot;</span><span class="p">]</span> 
</span><span id="TransformerBlock-245"><a href="#TransformerBlock-245"><span class="linenos">245</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">outputBias</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;ATT_BO&quot;</span><span class="p">]</span> 
</span><span id="TransformerBlock-246"><a href="#TransformerBlock-246"><span class="linenos">246</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">QueryWeights</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;ATT_WQ&quot;</span><span class="p">]</span> 
</span><span id="TransformerBlock-247"><a href="#TransformerBlock-247"><span class="linenos">247</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">KeyWeights</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;ATT_WK&quot;</span><span class="p">]</span> 
</span><span id="TransformerBlock-248"><a href="#TransformerBlock-248"><span class="linenos">248</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">ValueWeights</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;ATT_WV&quot;</span><span class="p">]</span>
</span><span id="TransformerBlock-249"><a href="#TransformerBlock-249"><span class="linenos">249</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;Embed_W&quot;</span><span class="p">]</span>
</span><span id="TransformerBlock-250"><a href="#TransformerBlock-250"><span class="linenos">250</span></a>
</span><span id="TransformerBlock-251"><a href="#TransformerBlock-251"><span class="linenos">251</span></a>        <span class="k">return</span> <span class="n">AllOutputs</span><span class="p">,</span> <span class="n">Parameters</span>
</span><span id="TransformerBlock-252"><a href="#TransformerBlock-252"><span class="linenos">252</span></a>
</span><span id="TransformerBlock-253"><a href="#TransformerBlock-253"><span class="linenos">253</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">useBatches</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">batchSize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span> <span class="n">beta1</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span> <span class="n">beta2</span> <span class="o">=</span> <span class="mf">0.999</span><span class="p">,</span> <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">):</span>
</span><span id="TransformerBlock-254"><a href="#TransformerBlock-254"><span class="linenos">254</span></a>        <span class="n">AllOutputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimiser</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">useBatches</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
</span><span id="TransformerBlock-255"><a href="#TransformerBlock-255"><span class="linenos">255</span></a>        <span class="n">loss</span> <span class="o">=</span> <span class="n">MSELossFunction</span><span class="p">(</span><span class="n">AllOutputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</span><span id="TransformerBlock-256"><a href="#TransformerBlock-256"><span class="linenos">256</span></a>        <span class="k">return</span> <span class="n">loss</span>
</span><span id="TransformerBlock-257"><a href="#TransformerBlock-257"><span class="linenos">257</span></a>
</span><span id="TransformerBlock-258"><a href="#TransformerBlock-258"><span class="linenos">258</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">saveWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fileName</span> <span class="o">=</span> <span class="s2">&quot;TransformerParameters.npz&quot;</span><span class="p">):</span>
</span><span id="TransformerBlock-259"><a href="#TransformerBlock-259"><span class="linenos">259</span></a>        <span class="n">np</span><span class="o">.</span><span class="n">savez</span><span class="p">(</span><span class="n">fileName</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">Parameters</span><span class="p">)</span>
</span><span id="TransformerBlock-260"><a href="#TransformerBlock-260"><span class="linenos">260</span></a>
</span><span id="TransformerBlock-261"><a href="#TransformerBlock-261"><span class="linenos">261</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">loadWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fileName</span> <span class="o">=</span> <span class="s2">&quot;TransformerParameters.npz&quot;</span><span class="p">):</span>
</span><span id="TransformerBlock-262"><a href="#TransformerBlock-262"><span class="linenos">262</span></a>        <span class="n">loaded</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">fileName</span><span class="p">)</span>
</span><span id="TransformerBlock-263"><a href="#TransformerBlock-263"><span class="linenos">263</span></a>
</span><span id="TransformerBlock-264"><a href="#TransformerBlock-264"><span class="linenos">264</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">gamma</span>            <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="s2">&quot;Norm1_gamma&quot;</span><span class="p">]</span>
</span><span id="TransformerBlock-265"><a href="#TransformerBlock-265"><span class="linenos">265</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">beta</span>             <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="s2">&quot;Norm1_beta&quot;</span><span class="p">]</span>
</span><span id="TransformerBlock-266"><a href="#TransformerBlock-266"><span class="linenos">266</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">gamma</span>            <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="s2">&quot;Norm2_gamma&quot;</span><span class="p">]</span>
</span><span id="TransformerBlock-267"><a href="#TransformerBlock-267"><span class="linenos">267</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">beta</span>             <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="s2">&quot;Norm2_beta&quot;</span><span class="p">]</span>
</span><span id="TransformerBlock-268"><a href="#TransformerBlock-268"><span class="linenos">268</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">W1</span>                 <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="s2">&quot;FFN_W1&quot;</span><span class="p">]</span>
</span><span id="TransformerBlock-269"><a href="#TransformerBlock-269"><span class="linenos">269</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">b1</span>                 <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="s2">&quot;FFN_b1&quot;</span><span class="p">]</span>
</span><span id="TransformerBlock-270"><a href="#TransformerBlock-270"><span class="linenos">270</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">W2</span>                 <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="s2">&quot;FFN_W2&quot;</span><span class="p">]</span>
</span><span id="TransformerBlock-271"><a href="#TransformerBlock-271"><span class="linenos">271</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">b2</span>                 <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="s2">&quot;FFN_b2&quot;</span><span class="p">]</span>
</span><span id="TransformerBlock-272"><a href="#TransformerBlock-272"><span class="linenos">272</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">outputWeight</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="s2">&quot;ATT_WO&quot;</span><span class="p">]</span>
</span><span id="TransformerBlock-273"><a href="#TransformerBlock-273"><span class="linenos">273</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">outputBias</span>   <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="s2">&quot;ATT_BO&quot;</span><span class="p">]</span>
</span><span id="TransformerBlock-274"><a href="#TransformerBlock-274"><span class="linenos">274</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">QueryWeights</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="s2">&quot;ATT_WQ&quot;</span><span class="p">]</span>
</span><span id="TransformerBlock-275"><a href="#TransformerBlock-275"><span class="linenos">275</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">KeyWeights</span>   <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="s2">&quot;ATT_WK&quot;</span><span class="p">]</span>
</span><span id="TransformerBlock-276"><a href="#TransformerBlock-276"><span class="linenos">276</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">ValueWeights</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="s2">&quot;ATT_WV&quot;</span><span class="p">]</span>
</span><span id="TransformerBlock-277"><a href="#TransformerBlock-277"><span class="linenos">277</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">weights</span>      <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="s2">&quot;Embed_W&quot;</span><span class="p">]</span>
</span></pre></div>


    

                            <div id="TransformerBlock.__init__" class="classattr">
                                        <input id="TransformerBlock.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">TransformerBlock</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">batchSize</span>,</span><span class="param">	<span class="n">sequenceLength</span>,</span><span class="param">	<span class="n">vocabSize</span>,</span><span class="param">	<span class="n">embedDimension</span>,</span><span class="param">	<span class="n">positionalEmbddingDimension</span>,</span><span class="param">	<span class="n">numberHeads</span>,</span><span class="param">	<span class="n">hiddenDimensionFFN</span>,</span><span class="param">	<span class="n">blockType</span><span class="o">=</span><span class="s1">&#39;encoder&#39;</span>,</span><span class="param">	<span class="n">useResidual</span><span class="o">=</span><span class="kc">True</span>,</span><span class="param">	<span class="n">useNorm</span><span class="o">=</span><span class="kc">True</span>,</span><span class="param">	<span class="n">usePaddingMask</span><span class="o">=</span><span class="kc">False</span>,</span><span class="param">	<span class="n">paddingMask</span><span class="o">=</span><span class="kc">None</span></span>)</span>

                <label class="view-source-button" for="TransformerBlock.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#TransformerBlock.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="TransformerBlock.__init__-61"><a href="#TransformerBlock.__init__-61"><span class="linenos"> 61</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">sequenceLength</span><span class="p">,</span> <span class="n">vocabSize</span><span class="p">,</span> <span class="n">embedDimension</span><span class="p">,</span> <span class="n">positionalEmbddingDimension</span><span class="p">,</span> <span class="n">numberHeads</span><span class="p">,</span> <span class="n">hiddenDimensionFFN</span><span class="p">,</span> <span class="n">blockType</span> <span class="o">=</span> <span class="s2">&quot;encoder&quot;</span><span class="p">,</span> <span class="n">useResidual</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">useNorm</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">usePaddingMask</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">paddingMask</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
</span><span id="TransformerBlock.__init__-62"><a href="#TransformerBlock.__init__-62"><span class="linenos"> 62</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span> <span class="o">=</span> <span class="n">batchSize</span>
</span><span id="TransformerBlock.__init__-63"><a href="#TransformerBlock.__init__-63"><span class="linenos"> 63</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span> <span class="o">=</span> <span class="n">sequenceLength</span>
</span><span id="TransformerBlock.__init__-64"><a href="#TransformerBlock.__init__-64"><span class="linenos"> 64</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">vocabSize</span> <span class="o">=</span> <span class="n">vocabSize</span>
</span><span id="TransformerBlock.__init__-65"><a href="#TransformerBlock.__init__-65"><span class="linenos"> 65</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span> <span class="o">=</span> <span class="n">embedDimension</span>
</span><span id="TransformerBlock.__init__-66"><a href="#TransformerBlock.__init__-66"><span class="linenos"> 66</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">numberHeads</span> <span class="o">=</span> <span class="n">numberHeads</span>
</span><span id="TransformerBlock.__init__-67"><a href="#TransformerBlock.__init__-67"><span class="linenos"> 67</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hiddenDimensionFFN</span> <span class="o">=</span> <span class="n">hiddenDimensionFFN</span>
</span><span id="TransformerBlock.__init__-68"><a href="#TransformerBlock.__init__-68"><span class="linenos"> 68</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">useResidual</span> <span class="o">=</span> <span class="n">useResidual</span>
</span><span id="TransformerBlock.__init__-69"><a href="#TransformerBlock.__init__-69"><span class="linenos"> 69</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">useNorm</span> <span class="o">=</span> <span class="n">useNorm</span>
</span><span id="TransformerBlock.__init__-70"><a href="#TransformerBlock.__init__-70"><span class="linenos"> 70</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">positionalEmbddingDimension</span> <span class="o">=</span> <span class="n">positionalEmbddingDimension</span>
</span><span id="TransformerBlock.__init__-71"><a href="#TransformerBlock.__init__-71"><span class="linenos"> 71</span></a>
</span><span id="TransformerBlock.__init__-72"><a href="#TransformerBlock.__init__-72"><span class="linenos"> 72</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">isDecoder</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="TransformerBlock.__init__-73"><a href="#TransformerBlock.__init__-73"><span class="linenos"> 73</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">blockType</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;decoder&quot;</span><span class="p">):</span>
</span><span id="TransformerBlock.__init__-74"><a href="#TransformerBlock.__init__-74"><span class="linenos"> 74</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">isDecoder</span> <span class="o">=</span> <span class="kc">True</span>
</span><span id="TransformerBlock.__init__-75"><a href="#TransformerBlock.__init__-75"><span class="linenos"> 75</span></a>
</span><span id="TransformerBlock.__init__-76"><a href="#TransformerBlock.__init__-76"><span class="linenos"> 76</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">EmbeddingLayer</span><span class="p">(</span>
</span><span id="TransformerBlock.__init__-77"><a href="#TransformerBlock.__init__-77"><span class="linenos"> 77</span></a>            <span class="n">vocabSize</span><span class="o">=</span><span class="n">vocabSize</span><span class="p">,</span>
</span><span id="TransformerBlock.__init__-78"><a href="#TransformerBlock.__init__-78"><span class="linenos"> 78</span></a>            <span class="n">embedDimension</span><span class="o">=</span><span class="n">embedDimension</span><span class="p">,</span>
</span><span id="TransformerBlock.__init__-79"><a href="#TransformerBlock.__init__-79"><span class="linenos"> 79</span></a>        <span class="p">)</span>
</span><span id="TransformerBlock.__init__-80"><a href="#TransformerBlock.__init__-80"><span class="linenos"> 80</span></a>
</span><span id="TransformerBlock.__init__-81"><a href="#TransformerBlock.__init__-81"><span class="linenos"> 81</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">positionalEncoding</span> <span class="o">=</span> <span class="n">PositionalEncoding</span><span class="p">(</span>
</span><span id="TransformerBlock.__init__-82"><a href="#TransformerBlock.__init__-82"><span class="linenos"> 82</span></a>            <span class="n">maxDimension</span><span class="o">=</span><span class="n">positionalEmbddingDimension</span><span class="p">,</span>
</span><span id="TransformerBlock.__init__-83"><a href="#TransformerBlock.__init__-83"><span class="linenos"> 83</span></a>            <span class="n">embeddingSize</span><span class="o">=</span><span class="n">embedDimension</span>
</span><span id="TransformerBlock.__init__-84"><a href="#TransformerBlock.__init__-84"><span class="linenos"> 84</span></a>        <span class="p">)</span>
</span><span id="TransformerBlock.__init__-85"><a href="#TransformerBlock.__init__-85"><span class="linenos"> 85</span></a>
</span><span id="TransformerBlock.__init__-86"><a href="#TransformerBlock.__init__-86"><span class="linenos"> 86</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">MultiAttentionHeadLayer</span><span class="p">(</span>
</span><span id="TransformerBlock.__init__-87"><a href="#TransformerBlock.__init__-87"><span class="linenos"> 87</span></a>            <span class="n">batchSize</span><span class="o">=</span><span class="n">batchSize</span><span class="p">,</span>
</span><span id="TransformerBlock.__init__-88"><a href="#TransformerBlock.__init__-88"><span class="linenos"> 88</span></a>            <span class="n">sequenceLength</span><span class="o">=</span><span class="n">sequenceLength</span><span class="p">,</span>
</span><span id="TransformerBlock.__init__-89"><a href="#TransformerBlock.__init__-89"><span class="linenos"> 89</span></a>            <span class="n">embedDimension</span><span class="o">=</span><span class="n">embedDimension</span><span class="p">,</span>
</span><span id="TransformerBlock.__init__-90"><a href="#TransformerBlock.__init__-90"><span class="linenos"> 90</span></a>            <span class="n">numberOfHeads</span><span class="o">=</span><span class="n">numberHeads</span><span class="p">,</span>
</span><span id="TransformerBlock.__init__-91"><a href="#TransformerBlock.__init__-91"><span class="linenos"> 91</span></a>            <span class="n">QueryWeights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="TransformerBlock.__init__-92"><a href="#TransformerBlock.__init__-92"><span class="linenos"> 92</span></a>            <span class="n">KeyWeights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
</span><span id="TransformerBlock.__init__-93"><a href="#TransformerBlock.__init__-93"><span class="linenos"> 93</span></a>            <span class="n">ValueWeights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="TransformerBlock.__init__-94"><a href="#TransformerBlock.__init__-94"><span class="linenos"> 94</span></a>            <span class="n">outputWeight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="TransformerBlock.__init__-95"><a href="#TransformerBlock.__init__-95"><span class="linenos"> 95</span></a>            <span class="n">outputBias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="TransformerBlock.__init__-96"><a href="#TransformerBlock.__init__-96"><span class="linenos"> 96</span></a>            <span class="n">useCasualMasking</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">isDecoder</span><span class="p">,</span> <span class="c1"># if it is decoder it uses casual masking to mask future tokens</span>
</span><span id="TransformerBlock.__init__-97"><a href="#TransformerBlock.__init__-97"><span class="linenos"> 97</span></a>            <span class="n">usePaddingMask</span> <span class="o">=</span> <span class="n">usePaddingMask</span><span class="p">,</span> 
</span><span id="TransformerBlock.__init__-98"><a href="#TransformerBlock.__init__-98"><span class="linenos"> 98</span></a>            <span class="n">paddingMask</span> <span class="o">=</span> <span class="n">paddingMask</span><span class="p">,</span>
</span><span id="TransformerBlock.__init__-99"><a href="#TransformerBlock.__init__-99"><span class="linenos"> 99</span></a>        <span class="p">)</span>
</span><span id="TransformerBlock.__init__-100"><a href="#TransformerBlock.__init__-100"><span class="linenos">100</span></a>
</span><span id="TransformerBlock.__init__-101"><a href="#TransformerBlock.__init__-101"><span class="linenos">101</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">FFN</span> <span class="o">=</span> <span class="n">FeedForwardNetwork</span><span class="p">(</span>
</span><span id="TransformerBlock.__init__-102"><a href="#TransformerBlock.__init__-102"><span class="linenos">102</span></a>            <span class="n">inputDimension</span><span class="o">=</span><span class="n">embedDimension</span><span class="p">,</span>
</span><span id="TransformerBlock.__init__-103"><a href="#TransformerBlock.__init__-103"><span class="linenos">103</span></a>            <span class="n">hiddenDimension</span><span class="o">=</span><span class="n">hiddenDimensionFFN</span><span class="p">,</span>
</span><span id="TransformerBlock.__init__-104"><a href="#TransformerBlock.__init__-104"><span class="linenos">104</span></a>            <span class="n">W1</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="TransformerBlock.__init__-105"><a href="#TransformerBlock.__init__-105"><span class="linenos">105</span></a>            <span class="n">b1</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="TransformerBlock.__init__-106"><a href="#TransformerBlock.__init__-106"><span class="linenos">106</span></a>            <span class="n">W2</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="TransformerBlock.__init__-107"><a href="#TransformerBlock.__init__-107"><span class="linenos">107</span></a>            <span class="n">b2</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="TransformerBlock.__init__-108"><a href="#TransformerBlock.__init__-108"><span class="linenos">108</span></a>        <span class="p">)</span>
</span><span id="TransformerBlock.__init__-109"><a href="#TransformerBlock.__init__-109"><span class="linenos">109</span></a>
</span><span id="TransformerBlock.__init__-110"><a href="#TransformerBlock.__init__-110"><span class="linenos">110</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">useNorm</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="TransformerBlock.__init__-111"><a href="#TransformerBlock.__init__-111"><span class="linenos">111</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">NormLayer</span><span class="p">(</span><span class="n">embedDimension</span><span class="p">)</span>
</span><span id="TransformerBlock.__init__-112"><a href="#TransformerBlock.__init__-112"><span class="linenos">112</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">NormLayer</span><span class="p">(</span><span class="n">embedDimension</span><span class="p">)</span>
</span><span id="TransformerBlock.__init__-113"><a href="#TransformerBlock.__init__-113"><span class="linenos">113</span></a>
</span><span id="TransformerBlock.__init__-114"><a href="#TransformerBlock.__init__-114"><span class="linenos">114</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">adam</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">forwardPropagation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">backwardPropagation</span><span class="p">)</span>
</span></pre></div>


    

                            </div>
                            <div id="TransformerBlock.batchSize" class="classattr">
                                <div class="attr variable">
            <span class="name">batchSize</span>

        
    </div>
    <a class="headerlink" href="#TransformerBlock.batchSize"></a>
    
    

                            </div>
                            <div id="TransformerBlock.sequenceLength" class="classattr">
                                <div class="attr variable">
            <span class="name">sequenceLength</span>

        
    </div>
    <a class="headerlink" href="#TransformerBlock.sequenceLength"></a>
    
    

                            </div>
                            <div id="TransformerBlock.vocabSize" class="classattr">
                                <div class="attr variable">
            <span class="name">vocabSize</span>

        
    </div>
    <a class="headerlink" href="#TransformerBlock.vocabSize"></a>
    
    

                            </div>
                            <div id="TransformerBlock.embedDimension" class="classattr">
                                <div class="attr variable">
            <span class="name">embedDimension</span>

        
    </div>
    <a class="headerlink" href="#TransformerBlock.embedDimension"></a>
    
    

                            </div>
                            <div id="TransformerBlock.numberHeads" class="classattr">
                                <div class="attr variable">
            <span class="name">numberHeads</span>

        
    </div>
    <a class="headerlink" href="#TransformerBlock.numberHeads"></a>
    
    

                            </div>
                            <div id="TransformerBlock.hiddenDimensionFFN" class="classattr">
                                <div class="attr variable">
            <span class="name">hiddenDimensionFFN</span>

        
    </div>
    <a class="headerlink" href="#TransformerBlock.hiddenDimensionFFN"></a>
    
    

                            </div>
                            <div id="TransformerBlock.useResidual" class="classattr">
                                <div class="attr variable">
            <span class="name">useResidual</span>

        
    </div>
    <a class="headerlink" href="#TransformerBlock.useResidual"></a>
    
    

                            </div>
                            <div id="TransformerBlock.useNorm" class="classattr">
                                <div class="attr variable">
            <span class="name">useNorm</span>

        
    </div>
    <a class="headerlink" href="#TransformerBlock.useNorm"></a>
    
    

                            </div>
                            <div id="TransformerBlock.positionalEmbddingDimension" class="classattr">
                                <div class="attr variable">
            <span class="name">positionalEmbddingDimension</span>

        
    </div>
    <a class="headerlink" href="#TransformerBlock.positionalEmbddingDimension"></a>
    
    

                            </div>
                            <div id="TransformerBlock.isDecoder" class="classattr">
                                <div class="attr variable">
            <span class="name">isDecoder</span>

        
    </div>
    <a class="headerlink" href="#TransformerBlock.isDecoder"></a>
    
    

                            </div>
                            <div id="TransformerBlock.embedding" class="classattr">
                                <div class="attr variable">
            <span class="name">embedding</span>

        
    </div>
    <a class="headerlink" href="#TransformerBlock.embedding"></a>
    
    

                            </div>
                            <div id="TransformerBlock.positionalEncoding" class="classattr">
                                <div class="attr variable">
            <span class="name">positionalEncoding</span>

        
    </div>
    <a class="headerlink" href="#TransformerBlock.positionalEncoding"></a>
    
    

                            </div>
                            <div id="TransformerBlock.attention" class="classattr">
                                <div class="attr variable">
            <span class="name">attention</span>

        
    </div>
    <a class="headerlink" href="#TransformerBlock.attention"></a>
    
    

                            </div>
                            <div id="TransformerBlock.FFN" class="classattr">
                                <div class="attr variable">
            <span class="name">FFN</span>

        
    </div>
    <a class="headerlink" href="#TransformerBlock.FFN"></a>
    
    

                            </div>
                            <div id="TransformerBlock.adam" class="classattr">
                                <div class="attr variable">
            <span class="name">adam</span>

        
    </div>
    <a class="headerlink" href="#TransformerBlock.adam"></a>
    
    

                            </div>
                            <div id="TransformerBlock.forwardPropagation" class="classattr">
                                        <input id="TransformerBlock.forwardPropagation-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forwardPropagation</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="nb">input</span>, </span><span class="param"><span class="n">firstBlock</span><span class="o">=</span><span class="kc">True</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="TransformerBlock.forwardPropagation-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#TransformerBlock.forwardPropagation"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="TransformerBlock.forwardPropagation-116"><a href="#TransformerBlock.forwardPropagation-116"><span class="linenos">116</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forwardPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">firstBlock</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="TransformerBlock.forwardPropagation-117"><a href="#TransformerBlock.forwardPropagation-117"><span class="linenos">117</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">firstBlock</span> <span class="o">=</span> <span class="n">firstBlock</span>
</span><span id="TransformerBlock.forwardPropagation-118"><a href="#TransformerBlock.forwardPropagation-118"><span class="linenos">118</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">firstBlock</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="TransformerBlock.forwardPropagation-119"><a href="#TransformerBlock.forwardPropagation-119"><span class="linenos">119</span></a>            <span class="nb">input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">forwardPropagation</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="TransformerBlock.forwardPropagation-120"><a href="#TransformerBlock.forwardPropagation-120"><span class="linenos">120</span></a>
</span><span id="TransformerBlock.forwardPropagation-121"><a href="#TransformerBlock.forwardPropagation-121"><span class="linenos">121</span></a>            <span class="nb">input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">positionalEncoding</span><span class="o">.</span><span class="n">forwardPropagation</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="TransformerBlock.forwardPropagation-122"><a href="#TransformerBlock.forwardPropagation-122"><span class="linenos">122</span></a>
</span><span id="TransformerBlock.forwardPropagation-123"><a href="#TransformerBlock.forwardPropagation-123"><span class="linenos">123</span></a>        <span class="n">attentionOutput</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">forwardPropagation</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="TransformerBlock.forwardPropagation-124"><a href="#TransformerBlock.forwardPropagation-124"><span class="linenos">124</span></a>
</span><span id="TransformerBlock.forwardPropagation-125"><a href="#TransformerBlock.forwardPropagation-125"><span class="linenos">125</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">useResidual</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="TransformerBlock.forwardPropagation-126"><a href="#TransformerBlock.forwardPropagation-126"><span class="linenos">126</span></a>            <span class="n">attentionOutput</span> <span class="o">=</span> <span class="n">ResidualConnection</span><span class="o">.</span><span class="n">forwardPropagation</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">attentionOutput</span><span class="p">)</span>
</span><span id="TransformerBlock.forwardPropagation-127"><a href="#TransformerBlock.forwardPropagation-127"><span class="linenos">127</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">useNorm</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="TransformerBlock.forwardPropagation-128"><a href="#TransformerBlock.forwardPropagation-128"><span class="linenos">128</span></a>            <span class="n">attentionOutput</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">forwardPropagation</span><span class="p">(</span><span class="n">attentionOutput</span><span class="p">)</span>
</span><span id="TransformerBlock.forwardPropagation-129"><a href="#TransformerBlock.forwardPropagation-129"><span class="linenos">129</span></a>
</span><span id="TransformerBlock.forwardPropagation-130"><a href="#TransformerBlock.forwardPropagation-130"><span class="linenos">130</span></a>        <span class="n">FNNOutput</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">forwardPropagation</span><span class="p">(</span><span class="n">attentionOutput</span><span class="p">)</span>
</span><span id="TransformerBlock.forwardPropagation-131"><a href="#TransformerBlock.forwardPropagation-131"><span class="linenos">131</span></a>
</span><span id="TransformerBlock.forwardPropagation-132"><a href="#TransformerBlock.forwardPropagation-132"><span class="linenos">132</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">useResidual</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="TransformerBlock.forwardPropagation-133"><a href="#TransformerBlock.forwardPropagation-133"><span class="linenos">133</span></a>            <span class="n">FNNOutput</span> <span class="o">=</span> <span class="n">ResidualConnection</span><span class="o">.</span><span class="n">forwardPropagation</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">FNNOutput</span><span class="p">,</span> <span class="n">attentionOutput</span><span class="p">)</span>
</span><span id="TransformerBlock.forwardPropagation-134"><a href="#TransformerBlock.forwardPropagation-134"><span class="linenos">134</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">useNorm</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="TransformerBlock.forwardPropagation-135"><a href="#TransformerBlock.forwardPropagation-135"><span class="linenos">135</span></a>            <span class="n">FNNOutput</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">forwardPropagation</span><span class="p">(</span><span class="n">FNNOutput</span><span class="p">)</span>
</span><span id="TransformerBlock.forwardPropagation-136"><a href="#TransformerBlock.forwardPropagation-136"><span class="linenos">136</span></a>
</span><span id="TransformerBlock.forwardPropagation-137"><a href="#TransformerBlock.forwardPropagation-137"><span class="linenos">137</span></a>        <span class="k">return</span> <span class="n">FNNOutput</span>
</span></pre></div>


    

                            </div>
                            <div id="TransformerBlock.backwardPropagation" class="classattr">
                                        <input id="TransformerBlock.backwardPropagation-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">backwardPropagation</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">output</span>, </span><span class="param"><span class="n">labels</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="TransformerBlock.backwardPropagation-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#TransformerBlock.backwardPropagation"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="TransformerBlock.backwardPropagation-139"><a href="#TransformerBlock.backwardPropagation-139"><span class="linenos">139</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">backwardPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
</span><span id="TransformerBlock.backwardPropagation-140"><a href="#TransformerBlock.backwardPropagation-140"><span class="linenos">140</span></a>        <span class="k">assert</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;MSE Derivative expected derviative </span><span class="si">{</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">)</span><span class="si">}</span><span class="s2">, got </span><span class="si">{</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="TransformerBlock.backwardPropagation-141"><a href="#TransformerBlock.backwardPropagation-141"><span class="linenos">141</span></a>
</span><span id="TransformerBlock.backwardPropagation-142"><a href="#TransformerBlock.backwardPropagation-142"><span class="linenos">142</span></a>        <span class="n">InputDerivative</span> <span class="o">=</span> <span class="n">MSEDerivative</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span><span id="TransformerBlock.backwardPropagation-143"><a href="#TransformerBlock.backwardPropagation-143"><span class="linenos">143</span></a>        <span class="k">assert</span> <span class="n">InputDerivative</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;MSE Derivative expected derviative </span><span class="si">{</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">)</span><span class="si">}</span><span class="s2">, got </span><span class="si">{</span><span class="n">InputDerivative</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="TransformerBlock.backwardPropagation-144"><a href="#TransformerBlock.backwardPropagation-144"><span class="linenos">144</span></a>        
</span><span id="TransformerBlock.backwardPropagation-145"><a href="#TransformerBlock.backwardPropagation-145"><span class="linenos">145</span></a>        <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span><span class="p">,</span> <span class="n">InputDerivative</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">blockBackPropagation</span><span class="p">(</span><span class="n">InputDerivative</span><span class="p">)</span>
</span><span id="TransformerBlock.backwardPropagation-146"><a href="#TransformerBlock.backwardPropagation-146"><span class="linenos">146</span></a>
</span><span id="TransformerBlock.backwardPropagation-147"><a href="#TransformerBlock.backwardPropagation-147"><span class="linenos">147</span></a>        <span class="k">return</span> <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span>
</span></pre></div>


    

                            </div>
                            <div id="TransformerBlock.blockBackPropagation" class="classattr">
                                        <input id="TransformerBlock.blockBackPropagation-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">blockBackPropagation</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">InputDerivative</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="TransformerBlock.blockBackPropagation-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#TransformerBlock.blockBackPropagation"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="TransformerBlock.blockBackPropagation-149"><a href="#TransformerBlock.blockBackPropagation-149"><span class="linenos">149</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">blockBackPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">InputDerivative</span><span class="p">):</span> <span class="c1">#, output, labels): </span>
</span><span id="TransformerBlock.blockBackPropagation-150"><a href="#TransformerBlock.blockBackPropagation-150"><span class="linenos">150</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="TransformerBlock.blockBackPropagation-151"><a href="#TransformerBlock.blockBackPropagation-151"><span class="linenos">151</span></a><span class="sd">        assert output.shape == (self.batchSize, self.sequenceLength, self.embedDimension), f&quot;MSE Derivative expected derviative {(self.batchSize, self.sequenceLength, self.embedDimension)}, got {output.shape}&quot;</span>
</span><span id="TransformerBlock.blockBackPropagation-152"><a href="#TransformerBlock.blockBackPropagation-152"><span class="linenos">152</span></a>
</span><span id="TransformerBlock.blockBackPropagation-153"><a href="#TransformerBlock.blockBackPropagation-153"><span class="linenos">153</span></a><span class="sd">        InputDerivative = MSEDerivative(output, labels, output.shape[-1])</span>
</span><span id="TransformerBlock.blockBackPropagation-154"><a href="#TransformerBlock.blockBackPropagation-154"><span class="linenos">154</span></a><span class="sd">        assert InputDerivative.shape == (self.batchSize, self.sequenceLength, self.embedDimension), f&quot;MSE Derivative expected derviative {(self.batchSize, self.sequenceLength, self.embedDimension)}, got {InputDerivative.shape}&quot;</span>
</span><span id="TransformerBlock.blockBackPropagation-155"><a href="#TransformerBlock.blockBackPropagation-155"><span class="linenos">155</span></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="TransformerBlock.blockBackPropagation-156"><a href="#TransformerBlock.blockBackPropagation-156"><span class="linenos">156</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">useNorm</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="TransformerBlock.blockBackPropagation-157"><a href="#TransformerBlock.blockBackPropagation-157"><span class="linenos">157</span></a>            <span class="n">InputDerivative</span><span class="p">,</span> <span class="n">norm2GammaDerivative</span><span class="p">,</span> <span class="n">norm2BetaDerivative</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">backwardPropagation</span><span class="p">(</span><span class="n">InputDerivative</span><span class="p">)</span>
</span><span id="TransformerBlock.blockBackPropagation-158"><a href="#TransformerBlock.blockBackPropagation-158"><span class="linenos">158</span></a>            <span class="k">assert</span> <span class="n">InputDerivative</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Norm2 expected derviative </span><span class="si">{</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">)</span><span class="si">}</span><span class="s2">, got </span><span class="si">{</span><span class="n">InputDerivative</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="TransformerBlock.blockBackPropagation-159"><a href="#TransformerBlock.blockBackPropagation-159"><span class="linenos">159</span></a>
</span><span id="TransformerBlock.blockBackPropagation-160"><a href="#TransformerBlock.blockBackPropagation-160"><span class="linenos">160</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">useResidual</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="TransformerBlock.blockBackPropagation-161"><a href="#TransformerBlock.blockBackPropagation-161"><span class="linenos">161</span></a>            <span class="n">residual2</span> <span class="o">=</span> <span class="n">InputDerivative</span>
</span><span id="TransformerBlock.blockBackPropagation-162"><a href="#TransformerBlock.blockBackPropagation-162"><span class="linenos">162</span></a>        
</span><span id="TransformerBlock.blockBackPropagation-163"><a href="#TransformerBlock.blockBackPropagation-163"><span class="linenos">163</span></a>        <span class="n">InputDerivative</span><span class="p">,</span> <span class="n">FFNWeightGradient1</span><span class="p">,</span> <span class="n">FFNBiasGradient1</span><span class="p">,</span> <span class="n">FFNWeightGradient2</span><span class="p">,</span> <span class="n">FFNBiasGradient2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">backwardPropagation</span><span class="p">(</span><span class="n">InputDerivative</span><span class="p">)</span>
</span><span id="TransformerBlock.blockBackPropagation-164"><a href="#TransformerBlock.blockBackPropagation-164"><span class="linenos">164</span></a>        <span class="k">assert</span> <span class="n">InputDerivative</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;FFN expected derviative </span><span class="si">{</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">)</span><span class="si">}</span><span class="s2">, got </span><span class="si">{</span><span class="n">InputDerivative</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="TransformerBlock.blockBackPropagation-165"><a href="#TransformerBlock.blockBackPropagation-165"><span class="linenos">165</span></a>
</span><span id="TransformerBlock.blockBackPropagation-166"><a href="#TransformerBlock.blockBackPropagation-166"><span class="linenos">166</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">useResidual</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="TransformerBlock.blockBackPropagation-167"><a href="#TransformerBlock.blockBackPropagation-167"><span class="linenos">167</span></a>            <span class="n">InputDerivative</span> <span class="o">+=</span> <span class="n">residual2</span>
</span><span id="TransformerBlock.blockBackPropagation-168"><a href="#TransformerBlock.blockBackPropagation-168"><span class="linenos">168</span></a>            <span class="k">assert</span> <span class="n">InputDerivative</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Residual1 expected derviative </span><span class="si">{</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">)</span><span class="si">}</span><span class="s2">, got </span><span class="si">{</span><span class="n">InputDerivative</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="TransformerBlock.blockBackPropagation-169"><a href="#TransformerBlock.blockBackPropagation-169"><span class="linenos">169</span></a>
</span><span id="TransformerBlock.blockBackPropagation-170"><a href="#TransformerBlock.blockBackPropagation-170"><span class="linenos">170</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">useNorm</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="TransformerBlock.blockBackPropagation-171"><a href="#TransformerBlock.blockBackPropagation-171"><span class="linenos">171</span></a>            <span class="n">InputDerivative</span><span class="p">,</span> <span class="n">norm1GammaDerivative</span><span class="p">,</span> <span class="n">norm1BetaDerivative</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">backwardPropagation</span><span class="p">(</span><span class="n">InputDerivative</span><span class="p">)</span>
</span><span id="TransformerBlock.blockBackPropagation-172"><a href="#TransformerBlock.blockBackPropagation-172"><span class="linenos">172</span></a>            <span class="k">assert</span> <span class="n">InputDerivative</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Norm1 expected derviative </span><span class="si">{</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">)</span><span class="si">}</span><span class="s2">, got </span><span class="si">{</span><span class="n">InputDerivative</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="TransformerBlock.blockBackPropagation-173"><a href="#TransformerBlock.blockBackPropagation-173"><span class="linenos">173</span></a>
</span><span id="TransformerBlock.blockBackPropagation-174"><a href="#TransformerBlock.blockBackPropagation-174"><span class="linenos">174</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">useResidual</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="TransformerBlock.blockBackPropagation-175"><a href="#TransformerBlock.blockBackPropagation-175"><span class="linenos">175</span></a>            <span class="n">residual1</span> <span class="o">=</span> <span class="n">InputDerivative</span>
</span><span id="TransformerBlock.blockBackPropagation-176"><a href="#TransformerBlock.blockBackPropagation-176"><span class="linenos">176</span></a>            
</span><span id="TransformerBlock.blockBackPropagation-177"><a href="#TransformerBlock.blockBackPropagation-177"><span class="linenos">177</span></a>        <span class="n">AttentionOutputWeightGradient</span><span class="p">,</span> <span class="n">AttentionOutputBiasGradient</span><span class="p">,</span> <span class="n">InputDerivative</span><span class="p">,</span> <span class="n">AttentionQueryWeightDerivative</span><span class="p">,</span> <span class="n">AttentionKeyWeightDerivative</span><span class="p">,</span> <span class="n">AttentionValueWeightDerivative</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">backwardPropagation</span><span class="p">(</span><span class="n">InputDerivative</span><span class="p">)</span>
</span><span id="TransformerBlock.blockBackPropagation-178"><a href="#TransformerBlock.blockBackPropagation-178"><span class="linenos">178</span></a>        <span class="k">assert</span> <span class="n">InputDerivative</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Attention expected derviative </span><span class="si">{</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">)</span><span class="si">}</span><span class="s2">, got </span><span class="si">{</span><span class="n">InputDerivative</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="TransformerBlock.blockBackPropagation-179"><a href="#TransformerBlock.blockBackPropagation-179"><span class="linenos">179</span></a>
</span><span id="TransformerBlock.blockBackPropagation-180"><a href="#TransformerBlock.blockBackPropagation-180"><span class="linenos">180</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">useResidual</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="TransformerBlock.blockBackPropagation-181"><a href="#TransformerBlock.blockBackPropagation-181"><span class="linenos">181</span></a>            <span class="n">InputDerivative</span> <span class="o">+=</span> <span class="n">residual1</span>
</span><span id="TransformerBlock.blockBackPropagation-182"><a href="#TransformerBlock.blockBackPropagation-182"><span class="linenos">182</span></a>            <span class="k">assert</span> <span class="n">InputDerivative</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Resiudal1 Derivative expected derviative </span><span class="si">{</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">)</span><span class="si">}</span><span class="s2">, got </span><span class="si">{</span><span class="n">InputDerivative</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="TransformerBlock.blockBackPropagation-183"><a href="#TransformerBlock.blockBackPropagation-183"><span class="linenos">183</span></a>
</span><span id="TransformerBlock.blockBackPropagation-184"><a href="#TransformerBlock.blockBackPropagation-184"><span class="linenos">184</span></a>        <span class="n">embeddingGradient</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="TransformerBlock.blockBackPropagation-185"><a href="#TransformerBlock.blockBackPropagation-185"><span class="linenos">185</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">firstBlock</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="TransformerBlock.blockBackPropagation-186"><a href="#TransformerBlock.blockBackPropagation-186"><span class="linenos">186</span></a>            <span class="n">embeddingGradient</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">backwardPropagation</span><span class="p">(</span><span class="n">InputDerivative</span><span class="p">)</span>
</span><span id="TransformerBlock.blockBackPropagation-187"><a href="#TransformerBlock.blockBackPropagation-187"><span class="linenos">187</span></a>            <span class="n">InputDerivative</span> <span class="o">=</span> <span class="n">embeddingGradient</span>
</span><span id="TransformerBlock.blockBackPropagation-188"><a href="#TransformerBlock.blockBackPropagation-188"><span class="linenos">188</span></a>
</span><span id="TransformerBlock.blockBackPropagation-189"><a href="#TransformerBlock.blockBackPropagation-189"><span class="linenos">189</span></a>        <span class="n">Parameters</span> <span class="o">=</span>  <span class="p">{</span>
</span><span id="TransformerBlock.blockBackPropagation-190"><a href="#TransformerBlock.blockBackPropagation-190"><span class="linenos">190</span></a>            <span class="s2">&quot;Norm1_gamma&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">gamma</span><span class="p">,</span> 
</span><span id="TransformerBlock.blockBackPropagation-191"><a href="#TransformerBlock.blockBackPropagation-191"><span class="linenos">191</span></a>            <span class="s2">&quot;Norm1_beta&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">beta</span><span class="p">,</span> 
</span><span id="TransformerBlock.blockBackPropagation-192"><a href="#TransformerBlock.blockBackPropagation-192"><span class="linenos">192</span></a>            <span class="s2">&quot;Norm2_gamma&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">gamma</span><span class="p">,</span> 
</span><span id="TransformerBlock.blockBackPropagation-193"><a href="#TransformerBlock.blockBackPropagation-193"><span class="linenos">193</span></a>            <span class="s2">&quot;Norm2_beta&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">beta</span><span class="p">,</span> 
</span><span id="TransformerBlock.blockBackPropagation-194"><a href="#TransformerBlock.blockBackPropagation-194"><span class="linenos">194</span></a>            <span class="s2">&quot;FFN_W1&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">W1</span><span class="p">,</span> 
</span><span id="TransformerBlock.blockBackPropagation-195"><a href="#TransformerBlock.blockBackPropagation-195"><span class="linenos">195</span></a>            <span class="s2">&quot;FFN_b1&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">b1</span><span class="p">,</span> 
</span><span id="TransformerBlock.blockBackPropagation-196"><a href="#TransformerBlock.blockBackPropagation-196"><span class="linenos">196</span></a>            <span class="s2">&quot;FFN_W2&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">W2</span><span class="p">,</span> 
</span><span id="TransformerBlock.blockBackPropagation-197"><a href="#TransformerBlock.blockBackPropagation-197"><span class="linenos">197</span></a>            <span class="s2">&quot;FFN_b2&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">b2</span><span class="p">,</span> 
</span><span id="TransformerBlock.blockBackPropagation-198"><a href="#TransformerBlock.blockBackPropagation-198"><span class="linenos">198</span></a>            <span class="s2">&quot;ATT_WO&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">outputWeight</span><span class="p">,</span> 
</span><span id="TransformerBlock.blockBackPropagation-199"><a href="#TransformerBlock.blockBackPropagation-199"><span class="linenos">199</span></a>            <span class="s2">&quot;ATT_BO&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">outputBias</span><span class="p">,</span> 
</span><span id="TransformerBlock.blockBackPropagation-200"><a href="#TransformerBlock.blockBackPropagation-200"><span class="linenos">200</span></a>            <span class="s2">&quot;ATT_WQ&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">QueryWeights</span><span class="p">,</span> 
</span><span id="TransformerBlock.blockBackPropagation-201"><a href="#TransformerBlock.blockBackPropagation-201"><span class="linenos">201</span></a>            <span class="s2">&quot;ATT_WK&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">KeyWeights</span><span class="p">,</span> 
</span><span id="TransformerBlock.blockBackPropagation-202"><a href="#TransformerBlock.blockBackPropagation-202"><span class="linenos">202</span></a>            <span class="s2">&quot;ATT_WV&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">ValueWeights</span><span class="p">,</span>
</span><span id="TransformerBlock.blockBackPropagation-203"><a href="#TransformerBlock.blockBackPropagation-203"><span class="linenos">203</span></a>            <span class="s2">&quot;Embed_W&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span>
</span><span id="TransformerBlock.blockBackPropagation-204"><a href="#TransformerBlock.blockBackPropagation-204"><span class="linenos">204</span></a>        <span class="p">}</span>
</span><span id="TransformerBlock.blockBackPropagation-205"><a href="#TransformerBlock.blockBackPropagation-205"><span class="linenos">205</span></a>  
</span><span id="TransformerBlock.blockBackPropagation-206"><a href="#TransformerBlock.blockBackPropagation-206"><span class="linenos">206</span></a>        <span class="n">Gradients</span> <span class="o">=</span>  <span class="p">{</span>
</span><span id="TransformerBlock.blockBackPropagation-207"><a href="#TransformerBlock.blockBackPropagation-207"><span class="linenos">207</span></a>            <span class="s2">&quot;Norm1_gamma&quot;</span><span class="p">:</span> <span class="n">norm1GammaDerivative</span><span class="p">,</span> 
</span><span id="TransformerBlock.blockBackPropagation-208"><a href="#TransformerBlock.blockBackPropagation-208"><span class="linenos">208</span></a>            <span class="s2">&quot;Norm1_beta&quot;</span><span class="p">:</span> <span class="n">norm1BetaDerivative</span><span class="p">,</span> 
</span><span id="TransformerBlock.blockBackPropagation-209"><a href="#TransformerBlock.blockBackPropagation-209"><span class="linenos">209</span></a>            <span class="s2">&quot;Norm2_gamma&quot;</span><span class="p">:</span> <span class="n">norm2GammaDerivative</span><span class="p">,</span> 
</span><span id="TransformerBlock.blockBackPropagation-210"><a href="#TransformerBlock.blockBackPropagation-210"><span class="linenos">210</span></a>            <span class="s2">&quot;Norm2_beta&quot;</span><span class="p">:</span> <span class="n">norm2BetaDerivative</span><span class="p">,</span> 
</span><span id="TransformerBlock.blockBackPropagation-211"><a href="#TransformerBlock.blockBackPropagation-211"><span class="linenos">211</span></a>            <span class="s2">&quot;FFN_W1&quot;</span><span class="p">:</span> <span class="n">FFNWeightGradient1</span><span class="p">,</span> 
</span><span id="TransformerBlock.blockBackPropagation-212"><a href="#TransformerBlock.blockBackPropagation-212"><span class="linenos">212</span></a>            <span class="s2">&quot;FFN_b1&quot;</span><span class="p">:</span> <span class="n">FFNBiasGradient1</span><span class="p">,</span> 
</span><span id="TransformerBlock.blockBackPropagation-213"><a href="#TransformerBlock.blockBackPropagation-213"><span class="linenos">213</span></a>            <span class="s2">&quot;FFN_W2&quot;</span><span class="p">:</span> <span class="n">FFNWeightGradient2</span><span class="p">,</span> 
</span><span id="TransformerBlock.blockBackPropagation-214"><a href="#TransformerBlock.blockBackPropagation-214"><span class="linenos">214</span></a>            <span class="s2">&quot;FFN_b2&quot;</span><span class="p">:</span> <span class="n">FFNBiasGradient2</span><span class="p">,</span> 
</span><span id="TransformerBlock.blockBackPropagation-215"><a href="#TransformerBlock.blockBackPropagation-215"><span class="linenos">215</span></a>            <span class="s2">&quot;ATT_WO&quot;</span><span class="p">:</span> <span class="n">AttentionOutputWeightGradient</span><span class="p">,</span> 
</span><span id="TransformerBlock.blockBackPropagation-216"><a href="#TransformerBlock.blockBackPropagation-216"><span class="linenos">216</span></a>            <span class="s2">&quot;ATT_BO&quot;</span><span class="p">:</span> <span class="n">AttentionOutputBiasGradient</span><span class="p">,</span> 
</span><span id="TransformerBlock.blockBackPropagation-217"><a href="#TransformerBlock.blockBackPropagation-217"><span class="linenos">217</span></a>            <span class="s2">&quot;ATT_WQ&quot;</span><span class="p">:</span> <span class="n">AttentionQueryWeightDerivative</span><span class="p">,</span> 
</span><span id="TransformerBlock.blockBackPropagation-218"><a href="#TransformerBlock.blockBackPropagation-218"><span class="linenos">218</span></a>            <span class="s2">&quot;ATT_WK&quot;</span><span class="p">:</span> <span class="n">AttentionKeyWeightDerivative</span><span class="p">,</span> 
</span><span id="TransformerBlock.blockBackPropagation-219"><a href="#TransformerBlock.blockBackPropagation-219"><span class="linenos">219</span></a>            <span class="s2">&quot;ATT_WV&quot;</span><span class="p">:</span> <span class="n">AttentionValueWeightDerivative</span><span class="p">,</span>
</span><span id="TransformerBlock.blockBackPropagation-220"><a href="#TransformerBlock.blockBackPropagation-220"><span class="linenos">220</span></a>            <span class="s2">&quot;Embed_W&quot;</span><span class="p">:</span> <span class="n">embeddingGradient</span><span class="p">,</span>
</span><span id="TransformerBlock.blockBackPropagation-221"><a href="#TransformerBlock.blockBackPropagation-221"><span class="linenos">221</span></a>        <span class="p">}</span>
</span><span id="TransformerBlock.blockBackPropagation-222"><a href="#TransformerBlock.blockBackPropagation-222"><span class="linenos">222</span></a>        
</span><span id="TransformerBlock.blockBackPropagation-223"><a href="#TransformerBlock.blockBackPropagation-223"><span class="linenos">223</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="s2">&quot;Embed_W&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
</span><span id="TransformerBlock.blockBackPropagation-224"><a href="#TransformerBlock.blockBackPropagation-224"><span class="linenos">224</span></a>            <span class="n">Parameters</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;Embed_W&quot;</span><span class="p">)</span>
</span><span id="TransformerBlock.blockBackPropagation-225"><a href="#TransformerBlock.blockBackPropagation-225"><span class="linenos">225</span></a>            <span class="n">Gradients</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;Embed_W&quot;</span><span class="p">)</span>
</span><span id="TransformerBlock.blockBackPropagation-226"><a href="#TransformerBlock.blockBackPropagation-226"><span class="linenos">226</span></a>
</span><span id="TransformerBlock.blockBackPropagation-227"><a href="#TransformerBlock.blockBackPropagation-227"><span class="linenos">227</span></a>        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">Gradients</span><span class="p">:</span>    
</span><span id="TransformerBlock.blockBackPropagation-228"><a href="#TransformerBlock.blockBackPropagation-228"><span class="linenos">228</span></a>            <span class="k">if</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="n">Parameters</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">ndim</span> <span class="o">+</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="TransformerBlock.blockBackPropagation-229"><a href="#TransformerBlock.blockBackPropagation-229"><span class="linenos">229</span></a>                <span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># without batches some parameters are (1, x, y) which becomes (x, y) so this turns them back into (x, y)</span>
</span><span id="TransformerBlock.blockBackPropagation-230"><a href="#TransformerBlock.blockBackPropagation-230"><span class="linenos">230</span></a>
</span><span id="TransformerBlock.blockBackPropagation-231"><a href="#TransformerBlock.blockBackPropagation-231"><span class="linenos">231</span></a>        <span class="k">return</span> <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span><span class="p">,</span> <span class="n">InputDerivative</span>
</span></pre></div>


            <div class="docstring"><p>assert output.shape == (self.batchSize, self.sequenceLength, self.embedDimension), f"MSE Derivative expected derviative {(self.batchSize, self.sequenceLength, self.embedDimension)}, got {output.shape}"</p>

<p>InputDerivative = MSEDerivative(output, labels, output.shape[-1])
assert InputDerivative.shape == (self.batchSize, self.sequenceLength, self.embedDimension), f"MSE Derivative expected derviative {(self.batchSize, self.sequenceLength, self.embedDimension)}, got {InputDerivative.shape}"</p>
</div>


                            </div>
                            <div id="TransformerBlock.optimiser" class="classattr">
                                        <input id="TransformerBlock.optimiser-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">optimiser</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">inputData</span>,</span><span class="param">	<span class="n">labels</span>,</span><span class="param">	<span class="n">useBatches</span>,</span><span class="param">	<span class="n">batchSize</span>,</span><span class="param">	<span class="n">alpha</span>,</span><span class="param">	<span class="n">beta1</span>,</span><span class="param">	<span class="n">beta2</span>,</span><span class="param">	<span class="n">epsilon</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="TransformerBlock.optimiser-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#TransformerBlock.optimiser"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="TransformerBlock.optimiser-233"><a href="#TransformerBlock.optimiser-233"><span class="linenos">233</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">optimiser</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">useBatches</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">):</span>
</span><span id="TransformerBlock.optimiser-234"><a href="#TransformerBlock.optimiser-234"><span class="linenos">234</span></a>        <span class="n">AllOutputs</span><span class="p">,</span> <span class="n">Parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adam</span><span class="o">.</span><span class="n">optimiser</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">useBatches</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>       
</span><span id="TransformerBlock.optimiser-235"><a href="#TransformerBlock.optimiser-235"><span class="linenos">235</span></a>
</span><span id="TransformerBlock.optimiser-236"><a href="#TransformerBlock.optimiser-236"><span class="linenos">236</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;Norm1_gamma&quot;</span><span class="p">]</span>
</span><span id="TransformerBlock.optimiser-237"><a href="#TransformerBlock.optimiser-237"><span class="linenos">237</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;Norm1_beta&quot;</span><span class="p">]</span> 
</span><span id="TransformerBlock.optimiser-238"><a href="#TransformerBlock.optimiser-238"><span class="linenos">238</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;Norm2_gamma&quot;</span><span class="p">]</span> 
</span><span id="TransformerBlock.optimiser-239"><a href="#TransformerBlock.optimiser-239"><span class="linenos">239</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;Norm2_beta&quot;</span><span class="p">]</span>
</span><span id="TransformerBlock.optimiser-240"><a href="#TransformerBlock.optimiser-240"><span class="linenos">240</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">W1</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;FFN_W1&quot;</span><span class="p">]</span> 
</span><span id="TransformerBlock.optimiser-241"><a href="#TransformerBlock.optimiser-241"><span class="linenos">241</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">b1</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;FFN_b1&quot;</span><span class="p">]</span> 
</span><span id="TransformerBlock.optimiser-242"><a href="#TransformerBlock.optimiser-242"><span class="linenos">242</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;FFN_W2&quot;</span><span class="p">]</span> 
</span><span id="TransformerBlock.optimiser-243"><a href="#TransformerBlock.optimiser-243"><span class="linenos">243</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">b2</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;FFN_b2&quot;</span><span class="p">]</span> 
</span><span id="TransformerBlock.optimiser-244"><a href="#TransformerBlock.optimiser-244"><span class="linenos">244</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">outputWeight</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;ATT_WO&quot;</span><span class="p">]</span> 
</span><span id="TransformerBlock.optimiser-245"><a href="#TransformerBlock.optimiser-245"><span class="linenos">245</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">outputBias</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;ATT_BO&quot;</span><span class="p">]</span> 
</span><span id="TransformerBlock.optimiser-246"><a href="#TransformerBlock.optimiser-246"><span class="linenos">246</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">QueryWeights</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;ATT_WQ&quot;</span><span class="p">]</span> 
</span><span id="TransformerBlock.optimiser-247"><a href="#TransformerBlock.optimiser-247"><span class="linenos">247</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">KeyWeights</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;ATT_WK&quot;</span><span class="p">]</span> 
</span><span id="TransformerBlock.optimiser-248"><a href="#TransformerBlock.optimiser-248"><span class="linenos">248</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">ValueWeights</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;ATT_WV&quot;</span><span class="p">]</span>
</span><span id="TransformerBlock.optimiser-249"><a href="#TransformerBlock.optimiser-249"><span class="linenos">249</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">Parameters</span><span class="p">[</span><span class="s2">&quot;Embed_W&quot;</span><span class="p">]</span>
</span><span id="TransformerBlock.optimiser-250"><a href="#TransformerBlock.optimiser-250"><span class="linenos">250</span></a>
</span><span id="TransformerBlock.optimiser-251"><a href="#TransformerBlock.optimiser-251"><span class="linenos">251</span></a>        <span class="k">return</span> <span class="n">AllOutputs</span><span class="p">,</span> <span class="n">Parameters</span>
</span></pre></div>


    

                            </div>
                            <div id="TransformerBlock.train" class="classattr">
                                        <input id="TransformerBlock.train-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">train</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">inputData</span>,</span><span class="param">	<span class="n">labels</span>,</span><span class="param">	<span class="n">useBatches</span><span class="o">=</span><span class="kc">False</span>,</span><span class="param">	<span class="n">batchSize</span><span class="o">=</span><span class="mi">16</span>,</span><span class="param">	<span class="n">alpha</span><span class="o">=</span><span class="mf">0.001</span>,</span><span class="param">	<span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span>,</span><span class="param">	<span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span>,</span><span class="param">	<span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-08</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="TransformerBlock.train-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#TransformerBlock.train"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="TransformerBlock.train-253"><a href="#TransformerBlock.train-253"><span class="linenos">253</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">useBatches</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">batchSize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span> <span class="n">beta1</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span> <span class="n">beta2</span> <span class="o">=</span> <span class="mf">0.999</span><span class="p">,</span> <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">):</span>
</span><span id="TransformerBlock.train-254"><a href="#TransformerBlock.train-254"><span class="linenos">254</span></a>        <span class="n">AllOutputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimiser</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">useBatches</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
</span><span id="TransformerBlock.train-255"><a href="#TransformerBlock.train-255"><span class="linenos">255</span></a>        <span class="n">loss</span> <span class="o">=</span> <span class="n">MSELossFunction</span><span class="p">(</span><span class="n">AllOutputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</span><span id="TransformerBlock.train-256"><a href="#TransformerBlock.train-256"><span class="linenos">256</span></a>        <span class="k">return</span> <span class="n">loss</span>
</span></pre></div>


    

                            </div>
                            <div id="TransformerBlock.saveWeights" class="classattr">
                                        <input id="TransformerBlock.saveWeights-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">saveWeights</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">fileName</span><span class="o">=</span><span class="s1">&#39;TransformerParameters.npz&#39;</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="TransformerBlock.saveWeights-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#TransformerBlock.saveWeights"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="TransformerBlock.saveWeights-258"><a href="#TransformerBlock.saveWeights-258"><span class="linenos">258</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">saveWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fileName</span> <span class="o">=</span> <span class="s2">&quot;TransformerParameters.npz&quot;</span><span class="p">):</span>
</span><span id="TransformerBlock.saveWeights-259"><a href="#TransformerBlock.saveWeights-259"><span class="linenos">259</span></a>        <span class="n">np</span><span class="o">.</span><span class="n">savez</span><span class="p">(</span><span class="n">fileName</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">Parameters</span><span class="p">)</span>
</span></pre></div>


    

                            </div>
                            <div id="TransformerBlock.loadWeights" class="classattr">
                                        <input id="TransformerBlock.loadWeights-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">loadWeights</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">fileName</span><span class="o">=</span><span class="s1">&#39;TransformerParameters.npz&#39;</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="TransformerBlock.loadWeights-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#TransformerBlock.loadWeights"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="TransformerBlock.loadWeights-261"><a href="#TransformerBlock.loadWeights-261"><span class="linenos">261</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">loadWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fileName</span> <span class="o">=</span> <span class="s2">&quot;TransformerParameters.npz&quot;</span><span class="p">):</span>
</span><span id="TransformerBlock.loadWeights-262"><a href="#TransformerBlock.loadWeights-262"><span class="linenos">262</span></a>        <span class="n">loaded</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">fileName</span><span class="p">)</span>
</span><span id="TransformerBlock.loadWeights-263"><a href="#TransformerBlock.loadWeights-263"><span class="linenos">263</span></a>
</span><span id="TransformerBlock.loadWeights-264"><a href="#TransformerBlock.loadWeights-264"><span class="linenos">264</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">gamma</span>            <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="s2">&quot;Norm1_gamma&quot;</span><span class="p">]</span>
</span><span id="TransformerBlock.loadWeights-265"><a href="#TransformerBlock.loadWeights-265"><span class="linenos">265</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">beta</span>             <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="s2">&quot;Norm1_beta&quot;</span><span class="p">]</span>
</span><span id="TransformerBlock.loadWeights-266"><a href="#TransformerBlock.loadWeights-266"><span class="linenos">266</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">gamma</span>            <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="s2">&quot;Norm2_gamma&quot;</span><span class="p">]</span>
</span><span id="TransformerBlock.loadWeights-267"><a href="#TransformerBlock.loadWeights-267"><span class="linenos">267</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">beta</span>             <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="s2">&quot;Norm2_beta&quot;</span><span class="p">]</span>
</span><span id="TransformerBlock.loadWeights-268"><a href="#TransformerBlock.loadWeights-268"><span class="linenos">268</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">W1</span>                 <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="s2">&quot;FFN_W1&quot;</span><span class="p">]</span>
</span><span id="TransformerBlock.loadWeights-269"><a href="#TransformerBlock.loadWeights-269"><span class="linenos">269</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">b1</span>                 <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="s2">&quot;FFN_b1&quot;</span><span class="p">]</span>
</span><span id="TransformerBlock.loadWeights-270"><a href="#TransformerBlock.loadWeights-270"><span class="linenos">270</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">W2</span>                 <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="s2">&quot;FFN_W2&quot;</span><span class="p">]</span>
</span><span id="TransformerBlock.loadWeights-271"><a href="#TransformerBlock.loadWeights-271"><span class="linenos">271</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">FFN</span><span class="o">.</span><span class="n">b2</span>                 <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="s2">&quot;FFN_b2&quot;</span><span class="p">]</span>
</span><span id="TransformerBlock.loadWeights-272"><a href="#TransformerBlock.loadWeights-272"><span class="linenos">272</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">outputWeight</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="s2">&quot;ATT_WO&quot;</span><span class="p">]</span>
</span><span id="TransformerBlock.loadWeights-273"><a href="#TransformerBlock.loadWeights-273"><span class="linenos">273</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">outputBias</span>   <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="s2">&quot;ATT_BO&quot;</span><span class="p">]</span>
</span><span id="TransformerBlock.loadWeights-274"><a href="#TransformerBlock.loadWeights-274"><span class="linenos">274</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">QueryWeights</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="s2">&quot;ATT_WQ&quot;</span><span class="p">]</span>
</span><span id="TransformerBlock.loadWeights-275"><a href="#TransformerBlock.loadWeights-275"><span class="linenos">275</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">KeyWeights</span>   <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="s2">&quot;ATT_WK&quot;</span><span class="p">]</span>
</span><span id="TransformerBlock.loadWeights-276"><a href="#TransformerBlock.loadWeights-276"><span class="linenos">276</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">ValueWeights</span> <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="s2">&quot;ATT_WV&quot;</span><span class="p">]</span>
</span><span id="TransformerBlock.loadWeights-277"><a href="#TransformerBlock.loadWeights-277"><span class="linenos">277</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">weights</span>      <span class="o">=</span> <span class="n">loaded</span><span class="p">[</span><span class="s2">&quot;Embed_W&quot;</span><span class="p">]</span>
</span></pre></div>


    

                            </div>
                </section>
                <section id="MultiAttentionHeadLayer">
                            <input id="MultiAttentionHeadLayer-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">MultiAttentionHeadLayer</span>:

                <label class="view-source-button" for="MultiAttentionHeadLayer-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MultiAttentionHeadLayer"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MultiAttentionHeadLayer-48"><a href="#MultiAttentionHeadLayer-48"><span class="linenos"> 48</span></a><span class="k">class</span><span class="w"> </span><span class="nc">MultiAttentionHeadLayer</span><span class="p">:</span>
</span><span id="MultiAttentionHeadLayer-49"><a href="#MultiAttentionHeadLayer-49"><span class="linenos"> 49</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">sequenceLength</span><span class="p">,</span> <span class="n">embedDimension</span><span class="p">,</span> <span class="n">numberOfHeads</span><span class="p">,</span> <span class="n">QueryWeights</span><span class="p">,</span> <span class="n">KeyWeights</span><span class="p">,</span> <span class="n">ValueWeights</span><span class="p">,</span> <span class="n">outputWeight</span><span class="p">,</span> <span class="n">outputBias</span><span class="p">,</span> <span class="n">useCasualMasking</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">usePaddingMask</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">paddingMask</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
</span><span id="MultiAttentionHeadLayer-50"><a href="#MultiAttentionHeadLayer-50"><span class="linenos"> 50</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span> <span class="o">=</span> <span class="n">embedDimension</span>
</span><span id="MultiAttentionHeadLayer-51"><a href="#MultiAttentionHeadLayer-51"><span class="linenos"> 51</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">numberOfHeads</span> <span class="o">=</span> <span class="n">numberOfHeads</span>
</span><span id="MultiAttentionHeadLayer-52"><a href="#MultiAttentionHeadLayer-52"><span class="linenos"> 52</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">QueryWeights</span> <span class="o">=</span> <span class="n">QueryWeights</span>
</span><span id="MultiAttentionHeadLayer-53"><a href="#MultiAttentionHeadLayer-53"><span class="linenos"> 53</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">KeyWeights</span> <span class="o">=</span> <span class="n">KeyWeights</span>
</span><span id="MultiAttentionHeadLayer-54"><a href="#MultiAttentionHeadLayer-54"><span class="linenos"> 54</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">ValueWeights</span> <span class="o">=</span> <span class="n">ValueWeights</span>
</span><span id="MultiAttentionHeadLayer-55"><a href="#MultiAttentionHeadLayer-55"><span class="linenos"> 55</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputWeight</span> <span class="o">=</span> <span class="n">outputWeight</span>
</span><span id="MultiAttentionHeadLayer-56"><a href="#MultiAttentionHeadLayer-56"><span class="linenos"> 56</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputBias</span> <span class="o">=</span> <span class="n">outputBias</span>
</span><span id="MultiAttentionHeadLayer-57"><a href="#MultiAttentionHeadLayer-57"><span class="linenos"> 57</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span> <span class="o">=</span> <span class="n">batchSize</span> 
</span><span id="MultiAttentionHeadLayer-58"><a href="#MultiAttentionHeadLayer-58"><span class="linenos"> 58</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span> <span class="o">=</span> <span class="n">sequenceLength</span>
</span><span id="MultiAttentionHeadLayer-59"><a href="#MultiAttentionHeadLayer-59"><span class="linenos"> 59</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">useCasualMasking</span> <span class="o">=</span> <span class="n">useCasualMasking</span>
</span><span id="MultiAttentionHeadLayer-60"><a href="#MultiAttentionHeadLayer-60"><span class="linenos"> 60</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">usePaddingMask</span> <span class="o">=</span> <span class="n">usePaddingMask</span>
</span><span id="MultiAttentionHeadLayer-61"><a href="#MultiAttentionHeadLayer-61"><span class="linenos"> 61</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">useCasualMasking</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="MultiAttentionHeadLayer-62"><a href="#MultiAttentionHeadLayer-62"><span class="linenos"> 62</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">casualMask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">createCausalMask</span><span class="p">(</span><span class="n">batchSize</span><span class="p">,</span> <span class="n">sequenceLength</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer-63"><a href="#MultiAttentionHeadLayer-63"><span class="linenos"> 63</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">paddingMask</span> <span class="o">=</span> <span class="n">paddingMask</span>
</span><span id="MultiAttentionHeadLayer-64"><a href="#MultiAttentionHeadLayer-64"><span class="linenos"> 64</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">batchIndex</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="MultiAttentionHeadLayer-65"><a href="#MultiAttentionHeadLayer-65"><span class="linenos"> 65</span></a>
</span><span id="MultiAttentionHeadLayer-66"><a href="#MultiAttentionHeadLayer-66"><span class="linenos"> 66</span></a>        <span class="k">assert</span> <span class="n">embedDimension</span> <span class="o">%</span> <span class="n">numberOfHeads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Embedding Dimension must be divisible by the number of heads&quot;</span>
</span><span id="MultiAttentionHeadLayer-67"><a href="#MultiAttentionHeadLayer-67"><span class="linenos"> 67</span></a>
</span><span id="MultiAttentionHeadLayer-68"><a href="#MultiAttentionHeadLayer-68"><span class="linenos"> 68</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">createWeights</span><span class="p">()</span>
</span><span id="MultiAttentionHeadLayer-69"><a href="#MultiAttentionHeadLayer-69"><span class="linenos"> 69</span></a>
</span><span id="MultiAttentionHeadLayer-70"><a href="#MultiAttentionHeadLayer-70"><span class="linenos"> 70</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">backProp</span> <span class="o">=</span> <span class="n">MultiHeadAttentionBackpropagation</span><span class="p">(</span>
</span><span id="MultiAttentionHeadLayer-71"><a href="#MultiAttentionHeadLayer-71"><span class="linenos"> 71</span></a>            <span class="n">embededDimension</span><span class="o">=</span><span class="n">embedDimension</span><span class="p">,</span>
</span><span id="MultiAttentionHeadLayer-72"><a href="#MultiAttentionHeadLayer-72"><span class="linenos"> 72</span></a>            <span class="n">outputWeights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">outputWeight</span><span class="p">,</span>
</span><span id="MultiAttentionHeadLayer-73"><a href="#MultiAttentionHeadLayer-73"><span class="linenos"> 73</span></a>            <span class="n">QueryWeights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">QueryWeights</span><span class="p">,</span>
</span><span id="MultiAttentionHeadLayer-74"><a href="#MultiAttentionHeadLayer-74"><span class="linenos"> 74</span></a>            <span class="n">KeyWeights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">KeyWeights</span><span class="p">,</span>
</span><span id="MultiAttentionHeadLayer-75"><a href="#MultiAttentionHeadLayer-75"><span class="linenos"> 75</span></a>            <span class="n">ValueWeights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ValueWeights</span>
</span><span id="MultiAttentionHeadLayer-76"><a href="#MultiAttentionHeadLayer-76"><span class="linenos"> 76</span></a>        <span class="p">)</span>
</span><span id="MultiAttentionHeadLayer-77"><a href="#MultiAttentionHeadLayer-77"><span class="linenos"> 77</span></a>
</span><span id="MultiAttentionHeadLayer-78"><a href="#MultiAttentionHeadLayer-78"><span class="linenos"> 78</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">QKVLinearProjection</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputEmbedding</span><span class="p">):</span>
</span><span id="MultiAttentionHeadLayer-79"><a href="#MultiAttentionHeadLayer-79"><span class="linenos"> 79</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">Query</span> <span class="o">=</span> <span class="n">inputEmbedding</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">QueryWeights</span>
</span><span id="MultiAttentionHeadLayer-80"><a href="#MultiAttentionHeadLayer-80"><span class="linenos"> 80</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">Key</span> <span class="o">=</span> <span class="n">inputEmbedding</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">KeyWeights</span>
</span><span id="MultiAttentionHeadLayer-81"><a href="#MultiAttentionHeadLayer-81"><span class="linenos"> 81</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">Value</span> <span class="o">=</span> <span class="n">inputEmbedding</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">ValueWeights</span>
</span><span id="MultiAttentionHeadLayer-82"><a href="#MultiAttentionHeadLayer-82"><span class="linenos"> 82</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">Query</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Key</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Value</span>
</span><span id="MultiAttentionHeadLayer-83"><a href="#MultiAttentionHeadLayer-83"><span class="linenos"> 83</span></a>    
</span><span id="MultiAttentionHeadLayer-84"><a href="#MultiAttentionHeadLayer-84"><span class="linenos"> 84</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">SplitIntoHeads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Query</span><span class="p">,</span> <span class="n">Key</span><span class="p">,</span> <span class="n">Value</span><span class="p">):</span>
</span><span id="MultiAttentionHeadLayer-85"><a href="#MultiAttentionHeadLayer-85"><span class="linenos"> 85</span></a>        <span class="c1"># QVK has a shape of (batchSize, sequenceLength, embedDimension)</span>
</span><span id="MultiAttentionHeadLayer-86"><a href="#MultiAttentionHeadLayer-86"><span class="linenos"> 86</span></a>        <span class="n">headDimension</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">numberOfHeads</span> <span class="c1"># // returns a whole number (floor division)</span>
</span><span id="MultiAttentionHeadLayer-87"><a href="#MultiAttentionHeadLayer-87"><span class="linenos"> 87</span></a>
</span><span id="MultiAttentionHeadLayer-88"><a href="#MultiAttentionHeadLayer-88"><span class="linenos"> 88</span></a>        <span class="c1"># reshape to split heads</span>
</span><span id="MultiAttentionHeadLayer-89"><a href="#MultiAttentionHeadLayer-89"><span class="linenos"> 89</span></a>        <span class="n">QReshaped</span> <span class="o">=</span> <span class="n">Query</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">numberOfHeads</span><span class="p">,</span> <span class="n">headDimension</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer-90"><a href="#MultiAttentionHeadLayer-90"><span class="linenos"> 90</span></a>        <span class="n">KReshaped</span> <span class="o">=</span> <span class="n">Key</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">numberOfHeads</span><span class="p">,</span> <span class="n">headDimension</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer-91"><a href="#MultiAttentionHeadLayer-91"><span class="linenos"> 91</span></a>        <span class="n">VReshaped</span> <span class="o">=</span> <span class="n">Value</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">numberOfHeads</span><span class="p">,</span> <span class="n">headDimension</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer-92"><a href="#MultiAttentionHeadLayer-92"><span class="linenos"> 92</span></a>        
</span><span id="MultiAttentionHeadLayer-93"><a href="#MultiAttentionHeadLayer-93"><span class="linenos"> 93</span></a>        <span class="c1"># Transpose to (batchSize, numberHeads, sequenceLength, headDimension)</span>
</span><span id="MultiAttentionHeadLayer-94"><a href="#MultiAttentionHeadLayer-94"><span class="linenos"> 94</span></a>        <span class="n">QHead</span> <span class="o">=</span> <span class="n">QReshaped</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer-95"><a href="#MultiAttentionHeadLayer-95"><span class="linenos"> 95</span></a>        <span class="n">KHead</span> <span class="o">=</span> <span class="n">KReshaped</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer-96"><a href="#MultiAttentionHeadLayer-96"><span class="linenos"> 96</span></a>        <span class="n">VHead</span> <span class="o">=</span> <span class="n">VReshaped</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer-97"><a href="#MultiAttentionHeadLayer-97"><span class="linenos"> 97</span></a>
</span><span id="MultiAttentionHeadLayer-98"><a href="#MultiAttentionHeadLayer-98"><span class="linenos"> 98</span></a>        <span class="k">return</span> <span class="n">QHead</span><span class="p">,</span> <span class="n">KHead</span><span class="p">,</span> <span class="n">VHead</span>
</span><span id="MultiAttentionHeadLayer-99"><a href="#MultiAttentionHeadLayer-99"><span class="linenos"> 99</span></a>    
</span><span id="MultiAttentionHeadLayer-100"><a href="#MultiAttentionHeadLayer-100"><span class="linenos">100</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_TransformerSoftMax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">values</span><span class="p">):</span> <span class="c1"># softmax in quacknet.core works for 1D arrays not 3D/4D tensors</span>
</span><span id="MultiAttentionHeadLayer-101"><a href="#MultiAttentionHeadLayer-101"><span class="linenos">101</span></a>        <span class="n">values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer-102"><a href="#MultiAttentionHeadLayer-102"><span class="linenos">102</span></a>        <span class="n">maxVal</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer-103"><a href="#MultiAttentionHeadLayer-103"><span class="linenos">103</span></a>        <span class="n">values</span> <span class="o">=</span> <span class="n">values</span> <span class="o">-</span> <span class="n">maxVal</span>
</span><span id="MultiAttentionHeadLayer-104"><a href="#MultiAttentionHeadLayer-104"><span class="linenos">104</span></a>        <span class="n">summ</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">values</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer-105"><a href="#MultiAttentionHeadLayer-105"><span class="linenos">105</span></a>        <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">values</span><span class="p">)</span> <span class="o">/</span> <span class="n">summ</span>
</span><span id="MultiAttentionHeadLayer-106"><a href="#MultiAttentionHeadLayer-106"><span class="linenos">106</span></a>        <span class="k">return</span> <span class="n">out</span>
</span><span id="MultiAttentionHeadLayer-107"><a href="#MultiAttentionHeadLayer-107"><span class="linenos">107</span></a>
</span><span id="MultiAttentionHeadLayer-108"><a href="#MultiAttentionHeadLayer-108"><span class="linenos">108</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_calculateAttentionForOneHead</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">QueryHead</span><span class="p">,</span> <span class="n">KeyHead</span><span class="p">,</span> <span class="n">ValueHead</span><span class="p">):</span>
</span><span id="MultiAttentionHeadLayer-109"><a href="#MultiAttentionHeadLayer-109"><span class="linenos">109</span></a>        <span class="c1"># a(Q, K, V) = softmax( (Q @ K.T) / sqrt(d) ) @ V </span>
</span><span id="MultiAttentionHeadLayer-110"><a href="#MultiAttentionHeadLayer-110"><span class="linenos">110</span></a>        <span class="n">attentionScore</span> <span class="o">=</span> <span class="p">(</span><span class="n">QueryHead</span> <span class="o">@</span> <span class="n">KeyHead</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">ValueHead</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span><span id="MultiAttentionHeadLayer-111"><a href="#MultiAttentionHeadLayer-111"><span class="linenos">111</span></a>        
</span><span id="MultiAttentionHeadLayer-112"><a href="#MultiAttentionHeadLayer-112"><span class="linenos">112</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">useCasualMasking</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="MultiAttentionHeadLayer-113"><a href="#MultiAttentionHeadLayer-113"><span class="linenos">113</span></a>            <span class="n">attentionScore</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">casualMask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">,</span> <span class="n">attentionScore</span><span class="p">)</span> <span class="c1"># masks attention with very large negative number </span>
</span><span id="MultiAttentionHeadLayer-114"><a href="#MultiAttentionHeadLayer-114"><span class="linenos">114</span></a>
</span><span id="MultiAttentionHeadLayer-115"><a href="#MultiAttentionHeadLayer-115"><span class="linenos">115</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">usePaddingMask</span> <span class="o">==</span> <span class="kc">True</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">paddingMask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
</span><span id="MultiAttentionHeadLayer-116"><a href="#MultiAttentionHeadLayer-116"><span class="linenos">116</span></a>            <span class="n">attentionScore</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">paddingMask</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batchIndex</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">:</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchIndex</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">,</span> <span class="n">attentionScore</span><span class="p">)</span> <span class="c1"># masks attention with very large negative number  </span>
</span><span id="MultiAttentionHeadLayer-117"><a href="#MultiAttentionHeadLayer-117"><span class="linenos">117</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">batchIndex</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span id="MultiAttentionHeadLayer-118"><a href="#MultiAttentionHeadLayer-118"><span class="linenos">118</span></a>
</span><span id="MultiAttentionHeadLayer-119"><a href="#MultiAttentionHeadLayer-119"><span class="linenos">119</span></a>        <span class="n">attentionWeights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_TransformerSoftMax</span><span class="p">(</span><span class="n">attentionScore</span><span class="p">)</span>  <span class="c1"># this is used in backprop</span>
</span><span id="MultiAttentionHeadLayer-120"><a href="#MultiAttentionHeadLayer-120"><span class="linenos">120</span></a>        <span class="n">attentionOutput</span> <span class="o">=</span> <span class="n">attentionWeights</span> <span class="o">@</span> <span class="n">ValueHead</span>
</span><span id="MultiAttentionHeadLayer-121"><a href="#MultiAttentionHeadLayer-121"><span class="linenos">121</span></a>        <span class="k">return</span> <span class="n">attentionOutput</span><span class="p">,</span> <span class="n">attentionWeights</span>
</span><span id="MultiAttentionHeadLayer-122"><a href="#MultiAttentionHeadLayer-122"><span class="linenos">122</span></a>
</span><span id="MultiAttentionHeadLayer-123"><a href="#MultiAttentionHeadLayer-123"><span class="linenos">123</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">calculateAttention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">QueryHead</span><span class="p">,</span> <span class="n">KeyHead</span><span class="p">,</span> <span class="n">ValueHead</span><span class="p">):</span>
</span><span id="MultiAttentionHeadLayer-124"><a href="#MultiAttentionHeadLayer-124"><span class="linenos">124</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attentionHeads</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="MultiAttentionHeadLayer-125"><a href="#MultiAttentionHeadLayer-125"><span class="linenos">125</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attentionWeights</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="MultiAttentionHeadLayer-126"><a href="#MultiAttentionHeadLayer-126"><span class="linenos">126</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">numberOfHeads</span><span class="p">):</span>
</span><span id="MultiAttentionHeadLayer-127"><a href="#MultiAttentionHeadLayer-127"><span class="linenos">127</span></a>            <span class="n">att</span><span class="p">,</span> <span class="n">att_W</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calculateAttentionForOneHead</span><span class="p">(</span>
</span><span id="MultiAttentionHeadLayer-128"><a href="#MultiAttentionHeadLayer-128"><span class="linenos">128</span></a>                <span class="n">QueryHead</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:],</span>
</span><span id="MultiAttentionHeadLayer-129"><a href="#MultiAttentionHeadLayer-129"><span class="linenos">129</span></a>                <span class="n">KeyHead</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:],</span> 
</span><span id="MultiAttentionHeadLayer-130"><a href="#MultiAttentionHeadLayer-130"><span class="linenos">130</span></a>                <span class="n">ValueHead</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:],</span>
</span><span id="MultiAttentionHeadLayer-131"><a href="#MultiAttentionHeadLayer-131"><span class="linenos">131</span></a>            <span class="p">)</span>
</span><span id="MultiAttentionHeadLayer-132"><a href="#MultiAttentionHeadLayer-132"><span class="linenos">132</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">attentionHeads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">att</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer-133"><a href="#MultiAttentionHeadLayer-133"><span class="linenos">133</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">attentionWeights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">att_W</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer-134"><a href="#MultiAttentionHeadLayer-134"><span class="linenos">134</span></a>            
</span><span id="MultiAttentionHeadLayer-135"><a href="#MultiAttentionHeadLayer-135"><span class="linenos">135</span></a>        <span class="n">stackedHeads</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attentionHeads</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer-136"><a href="#MultiAttentionHeadLayer-136"><span class="linenos">136</span></a>        <span class="n">stackedHeads</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">stackedHeads</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</span><span id="MultiAttentionHeadLayer-137"><a href="#MultiAttentionHeadLayer-137"><span class="linenos">137</span></a>        <span class="n">batchSize</span><span class="p">,</span> <span class="n">sequenceLength</span><span class="p">,</span> <span class="n">numberHeads</span><span class="p">,</span> <span class="n">headDimension</span> <span class="o">=</span> <span class="n">stackedHeads</span><span class="o">.</span><span class="n">shape</span>
</span><span id="MultiAttentionHeadLayer-138"><a href="#MultiAttentionHeadLayer-138"><span class="linenos">138</span></a>        <span class="n">combinedAttention</span> <span class="o">=</span> <span class="n">stackedHeads</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batchSize</span><span class="p">,</span> <span class="n">sequenceLength</span><span class="p">,</span> <span class="n">numberHeads</span> <span class="o">*</span> <span class="n">headDimension</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer-139"><a href="#MultiAttentionHeadLayer-139"><span class="linenos">139</span></a>        <span class="k">return</span> <span class="n">combinedAttention</span>
</span><span id="MultiAttentionHeadLayer-140"><a href="#MultiAttentionHeadLayer-140"><span class="linenos">140</span></a>        
</span><span id="MultiAttentionHeadLayer-141"><a href="#MultiAttentionHeadLayer-141"><span class="linenos">141</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">outputProjectionLayer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">combinedAttention</span><span class="p">):</span>
</span><span id="MultiAttentionHeadLayer-142"><a href="#MultiAttentionHeadLayer-142"><span class="linenos">142</span></a>        <span class="n">output</span> <span class="o">=</span> <span class="n">combinedAttention</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputWeight</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputBias</span>
</span><span id="MultiAttentionHeadLayer-143"><a href="#MultiAttentionHeadLayer-143"><span class="linenos">143</span></a>        <span class="k">return</span> <span class="n">output</span>
</span><span id="MultiAttentionHeadLayer-144"><a href="#MultiAttentionHeadLayer-144"><span class="linenos">144</span></a>    
</span><span id="MultiAttentionHeadLayer-145"><a href="#MultiAttentionHeadLayer-145"><span class="linenos">145</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forwardPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputEmbedding</span><span class="p">):</span>
</span><span id="MultiAttentionHeadLayer-146"><a href="#MultiAttentionHeadLayer-146"><span class="linenos">146</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">inputEmbedding</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">):</span>
</span><span id="MultiAttentionHeadLayer-147"><a href="#MultiAttentionHeadLayer-147"><span class="linenos">147</span></a>            <span class="n">inputEmbedding</span> <span class="o">=</span> <span class="n">inputEmbedding</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
</span><span id="MultiAttentionHeadLayer-148"><a href="#MultiAttentionHeadLayer-148"><span class="linenos">148</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">originalInput</span> <span class="o">=</span> <span class="n">inputEmbedding</span> <span class="c1"># for backprop</span>
</span><span id="MultiAttentionHeadLayer-149"><a href="#MultiAttentionHeadLayer-149"><span class="linenos">149</span></a>        <span class="n">Query</span><span class="p">,</span> <span class="n">Key</span><span class="p">,</span> <span class="n">Value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">QKVLinearProjection</span><span class="p">(</span><span class="n">inputEmbedding</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer-150"><a href="#MultiAttentionHeadLayer-150"><span class="linenos">150</span></a>        <span class="n">QHead</span><span class="p">,</span> <span class="n">KHead</span><span class="p">,</span> <span class="n">VHead</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">SplitIntoHeads</span><span class="p">(</span><span class="n">Query</span><span class="p">,</span> <span class="n">Key</span><span class="p">,</span> <span class="n">Value</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer-151"><a href="#MultiAttentionHeadLayer-151"><span class="linenos">151</span></a>        <span class="n">combinedAttention</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">calculateAttention</span><span class="p">(</span><span class="n">QHead</span><span class="p">,</span> <span class="n">KHead</span><span class="p">,</span> <span class="n">VHead</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer-152"><a href="#MultiAttentionHeadLayer-152"><span class="linenos">152</span></a>        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputProjectionLayer</span><span class="p">(</span><span class="n">combinedAttention</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer-153"><a href="#MultiAttentionHeadLayer-153"><span class="linenos">153</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">batchIndex</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="MultiAttentionHeadLayer-154"><a href="#MultiAttentionHeadLayer-154"><span class="linenos">154</span></a>        <span class="k">return</span> <span class="n">output</span>
</span><span id="MultiAttentionHeadLayer-155"><a href="#MultiAttentionHeadLayer-155"><span class="linenos">155</span></a>    
</span><span id="MultiAttentionHeadLayer-156"><a href="#MultiAttentionHeadLayer-156"><span class="linenos">156</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">createWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="MultiAttentionHeadLayer-157"><a href="#MultiAttentionHeadLayer-157"><span class="linenos">157</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">QueryWeights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initiaseWeight</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer-158"><a href="#MultiAttentionHeadLayer-158"><span class="linenos">158</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">KeyWeights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initiaseWeight</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer-159"><a href="#MultiAttentionHeadLayer-159"><span class="linenos">159</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">ValueWeights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initiaseWeight</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer-160"><a href="#MultiAttentionHeadLayer-160"><span class="linenos">160</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputWeight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initiaseWeight</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer-161"><a href="#MultiAttentionHeadLayer-161"><span class="linenos">161</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputBias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">))</span>
</span><span id="MultiAttentionHeadLayer-162"><a href="#MultiAttentionHeadLayer-162"><span class="linenos">162</span></a>
</span><span id="MultiAttentionHeadLayer-163"><a href="#MultiAttentionHeadLayer-163"><span class="linenos">163</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_initiaseWeight</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputDimension</span><span class="p">,</span> <span class="n">outputDimension</span><span class="p">):</span>
</span><span id="MultiAttentionHeadLayer-164"><a href="#MultiAttentionHeadLayer-164"><span class="linenos">164</span></a>        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">inputDimension</span><span class="p">,</span> <span class="n">outputDimension</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">inputDimension</span><span class="p">))</span>
</span><span id="MultiAttentionHeadLayer-165"><a href="#MultiAttentionHeadLayer-165"><span class="linenos">165</span></a>    
</span><span id="MultiAttentionHeadLayer-166"><a href="#MultiAttentionHeadLayer-166"><span class="linenos">166</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">backwardPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputGradient</span><span class="p">):</span>
</span><span id="MultiAttentionHeadLayer-167"><a href="#MultiAttentionHeadLayer-167"><span class="linenos">167</span></a>        <span class="n">outputWeightGradient</span><span class="p">,</span> <span class="n">outputBiasGradient</span><span class="p">,</span> <span class="n">inputDerivative</span><span class="p">,</span> <span class="n">QueryWeightDerivative</span><span class="p">,</span> <span class="n">KeyWeightDerivative</span><span class="p">,</span> <span class="n">ValueWeightDerivative</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backProp</span><span class="o">.</span><span class="n">backPropagation</span><span class="p">(</span>
</span><span id="MultiAttentionHeadLayer-168"><a href="#MultiAttentionHeadLayer-168"><span class="linenos">168</span></a>            <span class="n">originalInput</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">originalInput</span><span class="p">,</span>
</span><span id="MultiAttentionHeadLayer-169"><a href="#MultiAttentionHeadLayer-169"><span class="linenos">169</span></a>            <span class="n">batchSize</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span>
</span><span id="MultiAttentionHeadLayer-170"><a href="#MultiAttentionHeadLayer-170"><span class="linenos">170</span></a>            <span class="n">sequenceLength</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span>
</span><span id="MultiAttentionHeadLayer-171"><a href="#MultiAttentionHeadLayer-171"><span class="linenos">171</span></a>            <span class="n">outputGradient</span><span class="o">=</span><span class="n">outputGradient</span><span class="p">,</span>
</span><span id="MultiAttentionHeadLayer-172"><a href="#MultiAttentionHeadLayer-172"><span class="linenos">172</span></a>            <span class="n">attentionHeads</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">attentionHeads</span><span class="p">,</span>
</span><span id="MultiAttentionHeadLayer-173"><a href="#MultiAttentionHeadLayer-173"><span class="linenos">173</span></a>            <span class="n">attentionWeights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">attentionWeights</span><span class="p">,</span>
</span><span id="MultiAttentionHeadLayer-174"><a href="#MultiAttentionHeadLayer-174"><span class="linenos">174</span></a>            <span class="n">numberOfHeads</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">numberOfHeads</span><span class="p">,</span>
</span><span id="MultiAttentionHeadLayer-175"><a href="#MultiAttentionHeadLayer-175"><span class="linenos">175</span></a>            <span class="n">Query</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">Query</span><span class="p">,</span>
</span><span id="MultiAttentionHeadLayer-176"><a href="#MultiAttentionHeadLayer-176"><span class="linenos">176</span></a>            <span class="n">Key</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">Key</span><span class="p">,</span>
</span><span id="MultiAttentionHeadLayer-177"><a href="#MultiAttentionHeadLayer-177"><span class="linenos">177</span></a>            <span class="n">Value</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">Value</span>
</span><span id="MultiAttentionHeadLayer-178"><a href="#MultiAttentionHeadLayer-178"><span class="linenos">178</span></a>        <span class="p">)</span>
</span><span id="MultiAttentionHeadLayer-179"><a href="#MultiAttentionHeadLayer-179"><span class="linenos">179</span></a>        <span class="k">return</span> <span class="n">outputWeightGradient</span><span class="p">,</span> <span class="n">outputBiasGradient</span><span class="p">,</span> <span class="n">inputDerivative</span><span class="p">,</span> <span class="n">QueryWeightDerivative</span><span class="p">,</span> <span class="n">KeyWeightDerivative</span><span class="p">,</span> <span class="n">ValueWeightDerivative</span>
</span><span id="MultiAttentionHeadLayer-180"><a href="#MultiAttentionHeadLayer-180"><span class="linenos">180</span></a>    
</span><span id="MultiAttentionHeadLayer-181"><a href="#MultiAttentionHeadLayer-181"><span class="linenos">181</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">createCausalMask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">sequenceLength</span><span class="p">):</span>
</span><span id="MultiAttentionHeadLayer-182"><a href="#MultiAttentionHeadLayer-182"><span class="linenos">182</span></a>        <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">sequenceLength</span><span class="p">,</span> <span class="n">sequenceLength</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">bool_</span><span class="p">))</span>
</span><span id="MultiAttentionHeadLayer-183"><a href="#MultiAttentionHeadLayer-183"><span class="linenos">183</span></a>        <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
</span><span id="MultiAttentionHeadLayer-184"><a href="#MultiAttentionHeadLayer-184"><span class="linenos">184</span></a>        <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer-185"><a href="#MultiAttentionHeadLayer-185"><span class="linenos">185</span></a>        <span class="k">return</span> <span class="n">mask</span>
</span></pre></div>


    

                            <div id="MultiAttentionHeadLayer.__init__" class="classattr">
                                        <input id="MultiAttentionHeadLayer.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">MultiAttentionHeadLayer</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">batchSize</span>,</span><span class="param">	<span class="n">sequenceLength</span>,</span><span class="param">	<span class="n">embedDimension</span>,</span><span class="param">	<span class="n">numberOfHeads</span>,</span><span class="param">	<span class="n">QueryWeights</span>,</span><span class="param">	<span class="n">KeyWeights</span>,</span><span class="param">	<span class="n">ValueWeights</span>,</span><span class="param">	<span class="n">outputWeight</span>,</span><span class="param">	<span class="n">outputBias</span>,</span><span class="param">	<span class="n">useCasualMasking</span><span class="o">=</span><span class="kc">False</span>,</span><span class="param">	<span class="n">usePaddingMask</span><span class="o">=</span><span class="kc">False</span>,</span><span class="param">	<span class="n">paddingMask</span><span class="o">=</span><span class="kc">None</span></span>)</span>

                <label class="view-source-button" for="MultiAttentionHeadLayer.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MultiAttentionHeadLayer.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MultiAttentionHeadLayer.__init__-49"><a href="#MultiAttentionHeadLayer.__init__-49"><span class="linenos">49</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">sequenceLength</span><span class="p">,</span> <span class="n">embedDimension</span><span class="p">,</span> <span class="n">numberOfHeads</span><span class="p">,</span> <span class="n">QueryWeights</span><span class="p">,</span> <span class="n">KeyWeights</span><span class="p">,</span> <span class="n">ValueWeights</span><span class="p">,</span> <span class="n">outputWeight</span><span class="p">,</span> <span class="n">outputBias</span><span class="p">,</span> <span class="n">useCasualMasking</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">usePaddingMask</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">paddingMask</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
</span><span id="MultiAttentionHeadLayer.__init__-50"><a href="#MultiAttentionHeadLayer.__init__-50"><span class="linenos">50</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span> <span class="o">=</span> <span class="n">embedDimension</span>
</span><span id="MultiAttentionHeadLayer.__init__-51"><a href="#MultiAttentionHeadLayer.__init__-51"><span class="linenos">51</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">numberOfHeads</span> <span class="o">=</span> <span class="n">numberOfHeads</span>
</span><span id="MultiAttentionHeadLayer.__init__-52"><a href="#MultiAttentionHeadLayer.__init__-52"><span class="linenos">52</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">QueryWeights</span> <span class="o">=</span> <span class="n">QueryWeights</span>
</span><span id="MultiAttentionHeadLayer.__init__-53"><a href="#MultiAttentionHeadLayer.__init__-53"><span class="linenos">53</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">KeyWeights</span> <span class="o">=</span> <span class="n">KeyWeights</span>
</span><span id="MultiAttentionHeadLayer.__init__-54"><a href="#MultiAttentionHeadLayer.__init__-54"><span class="linenos">54</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">ValueWeights</span> <span class="o">=</span> <span class="n">ValueWeights</span>
</span><span id="MultiAttentionHeadLayer.__init__-55"><a href="#MultiAttentionHeadLayer.__init__-55"><span class="linenos">55</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputWeight</span> <span class="o">=</span> <span class="n">outputWeight</span>
</span><span id="MultiAttentionHeadLayer.__init__-56"><a href="#MultiAttentionHeadLayer.__init__-56"><span class="linenos">56</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputBias</span> <span class="o">=</span> <span class="n">outputBias</span>
</span><span id="MultiAttentionHeadLayer.__init__-57"><a href="#MultiAttentionHeadLayer.__init__-57"><span class="linenos">57</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span> <span class="o">=</span> <span class="n">batchSize</span> 
</span><span id="MultiAttentionHeadLayer.__init__-58"><a href="#MultiAttentionHeadLayer.__init__-58"><span class="linenos">58</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span> <span class="o">=</span> <span class="n">sequenceLength</span>
</span><span id="MultiAttentionHeadLayer.__init__-59"><a href="#MultiAttentionHeadLayer.__init__-59"><span class="linenos">59</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">useCasualMasking</span> <span class="o">=</span> <span class="n">useCasualMasking</span>
</span><span id="MultiAttentionHeadLayer.__init__-60"><a href="#MultiAttentionHeadLayer.__init__-60"><span class="linenos">60</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">usePaddingMask</span> <span class="o">=</span> <span class="n">usePaddingMask</span>
</span><span id="MultiAttentionHeadLayer.__init__-61"><a href="#MultiAttentionHeadLayer.__init__-61"><span class="linenos">61</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">useCasualMasking</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="MultiAttentionHeadLayer.__init__-62"><a href="#MultiAttentionHeadLayer.__init__-62"><span class="linenos">62</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">casualMask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">createCausalMask</span><span class="p">(</span><span class="n">batchSize</span><span class="p">,</span> <span class="n">sequenceLength</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer.__init__-63"><a href="#MultiAttentionHeadLayer.__init__-63"><span class="linenos">63</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">paddingMask</span> <span class="o">=</span> <span class="n">paddingMask</span>
</span><span id="MultiAttentionHeadLayer.__init__-64"><a href="#MultiAttentionHeadLayer.__init__-64"><span class="linenos">64</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">batchIndex</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="MultiAttentionHeadLayer.__init__-65"><a href="#MultiAttentionHeadLayer.__init__-65"><span class="linenos">65</span></a>
</span><span id="MultiAttentionHeadLayer.__init__-66"><a href="#MultiAttentionHeadLayer.__init__-66"><span class="linenos">66</span></a>        <span class="k">assert</span> <span class="n">embedDimension</span> <span class="o">%</span> <span class="n">numberOfHeads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Embedding Dimension must be divisible by the number of heads&quot;</span>
</span><span id="MultiAttentionHeadLayer.__init__-67"><a href="#MultiAttentionHeadLayer.__init__-67"><span class="linenos">67</span></a>
</span><span id="MultiAttentionHeadLayer.__init__-68"><a href="#MultiAttentionHeadLayer.__init__-68"><span class="linenos">68</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">createWeights</span><span class="p">()</span>
</span><span id="MultiAttentionHeadLayer.__init__-69"><a href="#MultiAttentionHeadLayer.__init__-69"><span class="linenos">69</span></a>
</span><span id="MultiAttentionHeadLayer.__init__-70"><a href="#MultiAttentionHeadLayer.__init__-70"><span class="linenos">70</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">backProp</span> <span class="o">=</span> <span class="n">MultiHeadAttentionBackpropagation</span><span class="p">(</span>
</span><span id="MultiAttentionHeadLayer.__init__-71"><a href="#MultiAttentionHeadLayer.__init__-71"><span class="linenos">71</span></a>            <span class="n">embededDimension</span><span class="o">=</span><span class="n">embedDimension</span><span class="p">,</span>
</span><span id="MultiAttentionHeadLayer.__init__-72"><a href="#MultiAttentionHeadLayer.__init__-72"><span class="linenos">72</span></a>            <span class="n">outputWeights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">outputWeight</span><span class="p">,</span>
</span><span id="MultiAttentionHeadLayer.__init__-73"><a href="#MultiAttentionHeadLayer.__init__-73"><span class="linenos">73</span></a>            <span class="n">QueryWeights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">QueryWeights</span><span class="p">,</span>
</span><span id="MultiAttentionHeadLayer.__init__-74"><a href="#MultiAttentionHeadLayer.__init__-74"><span class="linenos">74</span></a>            <span class="n">KeyWeights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">KeyWeights</span><span class="p">,</span>
</span><span id="MultiAttentionHeadLayer.__init__-75"><a href="#MultiAttentionHeadLayer.__init__-75"><span class="linenos">75</span></a>            <span class="n">ValueWeights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ValueWeights</span>
</span><span id="MultiAttentionHeadLayer.__init__-76"><a href="#MultiAttentionHeadLayer.__init__-76"><span class="linenos">76</span></a>        <span class="p">)</span>
</span></pre></div>


    

                            </div>
                            <div id="MultiAttentionHeadLayer.embedDimension" class="classattr">
                                <div class="attr variable">
            <span class="name">embedDimension</span>

        
    </div>
    <a class="headerlink" href="#MultiAttentionHeadLayer.embedDimension"></a>
    
    

                            </div>
                            <div id="MultiAttentionHeadLayer.numberOfHeads" class="classattr">
                                <div class="attr variable">
            <span class="name">numberOfHeads</span>

        
    </div>
    <a class="headerlink" href="#MultiAttentionHeadLayer.numberOfHeads"></a>
    
    

                            </div>
                            <div id="MultiAttentionHeadLayer.QueryWeights" class="classattr">
                                <div class="attr variable">
            <span class="name">QueryWeights</span>

        
    </div>
    <a class="headerlink" href="#MultiAttentionHeadLayer.QueryWeights"></a>
    
    

                            </div>
                            <div id="MultiAttentionHeadLayer.KeyWeights" class="classattr">
                                <div class="attr variable">
            <span class="name">KeyWeights</span>

        
    </div>
    <a class="headerlink" href="#MultiAttentionHeadLayer.KeyWeights"></a>
    
    

                            </div>
                            <div id="MultiAttentionHeadLayer.ValueWeights" class="classattr">
                                <div class="attr variable">
            <span class="name">ValueWeights</span>

        
    </div>
    <a class="headerlink" href="#MultiAttentionHeadLayer.ValueWeights"></a>
    
    

                            </div>
                            <div id="MultiAttentionHeadLayer.outputWeight" class="classattr">
                                <div class="attr variable">
            <span class="name">outputWeight</span>

        
    </div>
    <a class="headerlink" href="#MultiAttentionHeadLayer.outputWeight"></a>
    
    

                            </div>
                            <div id="MultiAttentionHeadLayer.outputBias" class="classattr">
                                <div class="attr variable">
            <span class="name">outputBias</span>

        
    </div>
    <a class="headerlink" href="#MultiAttentionHeadLayer.outputBias"></a>
    
    

                            </div>
                            <div id="MultiAttentionHeadLayer.batchSize" class="classattr">
                                <div class="attr variable">
            <span class="name">batchSize</span>

        
    </div>
    <a class="headerlink" href="#MultiAttentionHeadLayer.batchSize"></a>
    
    

                            </div>
                            <div id="MultiAttentionHeadLayer.sequenceLength" class="classattr">
                                <div class="attr variable">
            <span class="name">sequenceLength</span>

        
    </div>
    <a class="headerlink" href="#MultiAttentionHeadLayer.sequenceLength"></a>
    
    

                            </div>
                            <div id="MultiAttentionHeadLayer.useCasualMasking" class="classattr">
                                <div class="attr variable">
            <span class="name">useCasualMasking</span>

        
    </div>
    <a class="headerlink" href="#MultiAttentionHeadLayer.useCasualMasking"></a>
    
    

                            </div>
                            <div id="MultiAttentionHeadLayer.usePaddingMask" class="classattr">
                                <div class="attr variable">
            <span class="name">usePaddingMask</span>

        
    </div>
    <a class="headerlink" href="#MultiAttentionHeadLayer.usePaddingMask"></a>
    
    

                            </div>
                            <div id="MultiAttentionHeadLayer.paddingMask" class="classattr">
                                <div class="attr variable">
            <span class="name">paddingMask</span>

        
    </div>
    <a class="headerlink" href="#MultiAttentionHeadLayer.paddingMask"></a>
    
    

                            </div>
                            <div id="MultiAttentionHeadLayer.batchIndex" class="classattr">
                                <div class="attr variable">
            <span class="name">batchIndex</span>

        
    </div>
    <a class="headerlink" href="#MultiAttentionHeadLayer.batchIndex"></a>
    
    

                            </div>
                            <div id="MultiAttentionHeadLayer.backProp" class="classattr">
                                <div class="attr variable">
            <span class="name">backProp</span>

        
    </div>
    <a class="headerlink" href="#MultiAttentionHeadLayer.backProp"></a>
    
    

                            </div>
                            <div id="MultiAttentionHeadLayer.QKVLinearProjection" class="classattr">
                                        <input id="MultiAttentionHeadLayer.QKVLinearProjection-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">QKVLinearProjection</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">inputEmbedding</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="MultiAttentionHeadLayer.QKVLinearProjection-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MultiAttentionHeadLayer.QKVLinearProjection"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MultiAttentionHeadLayer.QKVLinearProjection-78"><a href="#MultiAttentionHeadLayer.QKVLinearProjection-78"><span class="linenos">78</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">QKVLinearProjection</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputEmbedding</span><span class="p">):</span>
</span><span id="MultiAttentionHeadLayer.QKVLinearProjection-79"><a href="#MultiAttentionHeadLayer.QKVLinearProjection-79"><span class="linenos">79</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">Query</span> <span class="o">=</span> <span class="n">inputEmbedding</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">QueryWeights</span>
</span><span id="MultiAttentionHeadLayer.QKVLinearProjection-80"><a href="#MultiAttentionHeadLayer.QKVLinearProjection-80"><span class="linenos">80</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">Key</span> <span class="o">=</span> <span class="n">inputEmbedding</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">KeyWeights</span>
</span><span id="MultiAttentionHeadLayer.QKVLinearProjection-81"><a href="#MultiAttentionHeadLayer.QKVLinearProjection-81"><span class="linenos">81</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">Value</span> <span class="o">=</span> <span class="n">inputEmbedding</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">ValueWeights</span>
</span><span id="MultiAttentionHeadLayer.QKVLinearProjection-82"><a href="#MultiAttentionHeadLayer.QKVLinearProjection-82"><span class="linenos">82</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">Query</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Key</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Value</span>
</span></pre></div>


    

                            </div>
                            <div id="MultiAttentionHeadLayer.SplitIntoHeads" class="classattr">
                                        <input id="MultiAttentionHeadLayer.SplitIntoHeads-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">SplitIntoHeads</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">Query</span>, </span><span class="param"><span class="n">Key</span>, </span><span class="param"><span class="n">Value</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="MultiAttentionHeadLayer.SplitIntoHeads-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MultiAttentionHeadLayer.SplitIntoHeads"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MultiAttentionHeadLayer.SplitIntoHeads-84"><a href="#MultiAttentionHeadLayer.SplitIntoHeads-84"><span class="linenos">84</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">SplitIntoHeads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Query</span><span class="p">,</span> <span class="n">Key</span><span class="p">,</span> <span class="n">Value</span><span class="p">):</span>
</span><span id="MultiAttentionHeadLayer.SplitIntoHeads-85"><a href="#MultiAttentionHeadLayer.SplitIntoHeads-85"><span class="linenos">85</span></a>        <span class="c1"># QVK has a shape of (batchSize, sequenceLength, embedDimension)</span>
</span><span id="MultiAttentionHeadLayer.SplitIntoHeads-86"><a href="#MultiAttentionHeadLayer.SplitIntoHeads-86"><span class="linenos">86</span></a>        <span class="n">headDimension</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">numberOfHeads</span> <span class="c1"># // returns a whole number (floor division)</span>
</span><span id="MultiAttentionHeadLayer.SplitIntoHeads-87"><a href="#MultiAttentionHeadLayer.SplitIntoHeads-87"><span class="linenos">87</span></a>
</span><span id="MultiAttentionHeadLayer.SplitIntoHeads-88"><a href="#MultiAttentionHeadLayer.SplitIntoHeads-88"><span class="linenos">88</span></a>        <span class="c1"># reshape to split heads</span>
</span><span id="MultiAttentionHeadLayer.SplitIntoHeads-89"><a href="#MultiAttentionHeadLayer.SplitIntoHeads-89"><span class="linenos">89</span></a>        <span class="n">QReshaped</span> <span class="o">=</span> <span class="n">Query</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">numberOfHeads</span><span class="p">,</span> <span class="n">headDimension</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer.SplitIntoHeads-90"><a href="#MultiAttentionHeadLayer.SplitIntoHeads-90"><span class="linenos">90</span></a>        <span class="n">KReshaped</span> <span class="o">=</span> <span class="n">Key</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">numberOfHeads</span><span class="p">,</span> <span class="n">headDimension</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer.SplitIntoHeads-91"><a href="#MultiAttentionHeadLayer.SplitIntoHeads-91"><span class="linenos">91</span></a>        <span class="n">VReshaped</span> <span class="o">=</span> <span class="n">Value</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">numberOfHeads</span><span class="p">,</span> <span class="n">headDimension</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer.SplitIntoHeads-92"><a href="#MultiAttentionHeadLayer.SplitIntoHeads-92"><span class="linenos">92</span></a>        
</span><span id="MultiAttentionHeadLayer.SplitIntoHeads-93"><a href="#MultiAttentionHeadLayer.SplitIntoHeads-93"><span class="linenos">93</span></a>        <span class="c1"># Transpose to (batchSize, numberHeads, sequenceLength, headDimension)</span>
</span><span id="MultiAttentionHeadLayer.SplitIntoHeads-94"><a href="#MultiAttentionHeadLayer.SplitIntoHeads-94"><span class="linenos">94</span></a>        <span class="n">QHead</span> <span class="o">=</span> <span class="n">QReshaped</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer.SplitIntoHeads-95"><a href="#MultiAttentionHeadLayer.SplitIntoHeads-95"><span class="linenos">95</span></a>        <span class="n">KHead</span> <span class="o">=</span> <span class="n">KReshaped</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer.SplitIntoHeads-96"><a href="#MultiAttentionHeadLayer.SplitIntoHeads-96"><span class="linenos">96</span></a>        <span class="n">VHead</span> <span class="o">=</span> <span class="n">VReshaped</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer.SplitIntoHeads-97"><a href="#MultiAttentionHeadLayer.SplitIntoHeads-97"><span class="linenos">97</span></a>
</span><span id="MultiAttentionHeadLayer.SplitIntoHeads-98"><a href="#MultiAttentionHeadLayer.SplitIntoHeads-98"><span class="linenos">98</span></a>        <span class="k">return</span> <span class="n">QHead</span><span class="p">,</span> <span class="n">KHead</span><span class="p">,</span> <span class="n">VHead</span>
</span></pre></div>


    

                            </div>
                            <div id="MultiAttentionHeadLayer.calculateAttention" class="classattr">
                                        <input id="MultiAttentionHeadLayer.calculateAttention-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">calculateAttention</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">QueryHead</span>, </span><span class="param"><span class="n">KeyHead</span>, </span><span class="param"><span class="n">ValueHead</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="MultiAttentionHeadLayer.calculateAttention-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MultiAttentionHeadLayer.calculateAttention"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MultiAttentionHeadLayer.calculateAttention-123"><a href="#MultiAttentionHeadLayer.calculateAttention-123"><span class="linenos">123</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">calculateAttention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">QueryHead</span><span class="p">,</span> <span class="n">KeyHead</span><span class="p">,</span> <span class="n">ValueHead</span><span class="p">):</span>
</span><span id="MultiAttentionHeadLayer.calculateAttention-124"><a href="#MultiAttentionHeadLayer.calculateAttention-124"><span class="linenos">124</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attentionHeads</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="MultiAttentionHeadLayer.calculateAttention-125"><a href="#MultiAttentionHeadLayer.calculateAttention-125"><span class="linenos">125</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attentionWeights</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="MultiAttentionHeadLayer.calculateAttention-126"><a href="#MultiAttentionHeadLayer.calculateAttention-126"><span class="linenos">126</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">numberOfHeads</span><span class="p">):</span>
</span><span id="MultiAttentionHeadLayer.calculateAttention-127"><a href="#MultiAttentionHeadLayer.calculateAttention-127"><span class="linenos">127</span></a>            <span class="n">att</span><span class="p">,</span> <span class="n">att_W</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calculateAttentionForOneHead</span><span class="p">(</span>
</span><span id="MultiAttentionHeadLayer.calculateAttention-128"><a href="#MultiAttentionHeadLayer.calculateAttention-128"><span class="linenos">128</span></a>                <span class="n">QueryHead</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:],</span>
</span><span id="MultiAttentionHeadLayer.calculateAttention-129"><a href="#MultiAttentionHeadLayer.calculateAttention-129"><span class="linenos">129</span></a>                <span class="n">KeyHead</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:],</span> 
</span><span id="MultiAttentionHeadLayer.calculateAttention-130"><a href="#MultiAttentionHeadLayer.calculateAttention-130"><span class="linenos">130</span></a>                <span class="n">ValueHead</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:],</span>
</span><span id="MultiAttentionHeadLayer.calculateAttention-131"><a href="#MultiAttentionHeadLayer.calculateAttention-131"><span class="linenos">131</span></a>            <span class="p">)</span>
</span><span id="MultiAttentionHeadLayer.calculateAttention-132"><a href="#MultiAttentionHeadLayer.calculateAttention-132"><span class="linenos">132</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">attentionHeads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">att</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer.calculateAttention-133"><a href="#MultiAttentionHeadLayer.calculateAttention-133"><span class="linenos">133</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">attentionWeights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">att_W</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer.calculateAttention-134"><a href="#MultiAttentionHeadLayer.calculateAttention-134"><span class="linenos">134</span></a>            
</span><span id="MultiAttentionHeadLayer.calculateAttention-135"><a href="#MultiAttentionHeadLayer.calculateAttention-135"><span class="linenos">135</span></a>        <span class="n">stackedHeads</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attentionHeads</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer.calculateAttention-136"><a href="#MultiAttentionHeadLayer.calculateAttention-136"><span class="linenos">136</span></a>        <span class="n">stackedHeads</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">stackedHeads</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</span><span id="MultiAttentionHeadLayer.calculateAttention-137"><a href="#MultiAttentionHeadLayer.calculateAttention-137"><span class="linenos">137</span></a>        <span class="n">batchSize</span><span class="p">,</span> <span class="n">sequenceLength</span><span class="p">,</span> <span class="n">numberHeads</span><span class="p">,</span> <span class="n">headDimension</span> <span class="o">=</span> <span class="n">stackedHeads</span><span class="o">.</span><span class="n">shape</span>
</span><span id="MultiAttentionHeadLayer.calculateAttention-138"><a href="#MultiAttentionHeadLayer.calculateAttention-138"><span class="linenos">138</span></a>        <span class="n">combinedAttention</span> <span class="o">=</span> <span class="n">stackedHeads</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batchSize</span><span class="p">,</span> <span class="n">sequenceLength</span><span class="p">,</span> <span class="n">numberHeads</span> <span class="o">*</span> <span class="n">headDimension</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer.calculateAttention-139"><a href="#MultiAttentionHeadLayer.calculateAttention-139"><span class="linenos">139</span></a>        <span class="k">return</span> <span class="n">combinedAttention</span>
</span></pre></div>


    

                            </div>
                            <div id="MultiAttentionHeadLayer.outputProjectionLayer" class="classattr">
                                        <input id="MultiAttentionHeadLayer.outputProjectionLayer-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">outputProjectionLayer</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">combinedAttention</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="MultiAttentionHeadLayer.outputProjectionLayer-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MultiAttentionHeadLayer.outputProjectionLayer"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MultiAttentionHeadLayer.outputProjectionLayer-141"><a href="#MultiAttentionHeadLayer.outputProjectionLayer-141"><span class="linenos">141</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">outputProjectionLayer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">combinedAttention</span><span class="p">):</span>
</span><span id="MultiAttentionHeadLayer.outputProjectionLayer-142"><a href="#MultiAttentionHeadLayer.outputProjectionLayer-142"><span class="linenos">142</span></a>        <span class="n">output</span> <span class="o">=</span> <span class="n">combinedAttention</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputWeight</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputBias</span>
</span><span id="MultiAttentionHeadLayer.outputProjectionLayer-143"><a href="#MultiAttentionHeadLayer.outputProjectionLayer-143"><span class="linenos">143</span></a>        <span class="k">return</span> <span class="n">output</span>
</span></pre></div>


    

                            </div>
                            <div id="MultiAttentionHeadLayer.forwardPropagation" class="classattr">
                                        <input id="MultiAttentionHeadLayer.forwardPropagation-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forwardPropagation</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">inputEmbedding</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="MultiAttentionHeadLayer.forwardPropagation-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MultiAttentionHeadLayer.forwardPropagation"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MultiAttentionHeadLayer.forwardPropagation-145"><a href="#MultiAttentionHeadLayer.forwardPropagation-145"><span class="linenos">145</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forwardPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputEmbedding</span><span class="p">):</span>
</span><span id="MultiAttentionHeadLayer.forwardPropagation-146"><a href="#MultiAttentionHeadLayer.forwardPropagation-146"><span class="linenos">146</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">inputEmbedding</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">):</span>
</span><span id="MultiAttentionHeadLayer.forwardPropagation-147"><a href="#MultiAttentionHeadLayer.forwardPropagation-147"><span class="linenos">147</span></a>            <span class="n">inputEmbedding</span> <span class="o">=</span> <span class="n">inputEmbedding</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
</span><span id="MultiAttentionHeadLayer.forwardPropagation-148"><a href="#MultiAttentionHeadLayer.forwardPropagation-148"><span class="linenos">148</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">originalInput</span> <span class="o">=</span> <span class="n">inputEmbedding</span> <span class="c1"># for backprop</span>
</span><span id="MultiAttentionHeadLayer.forwardPropagation-149"><a href="#MultiAttentionHeadLayer.forwardPropagation-149"><span class="linenos">149</span></a>        <span class="n">Query</span><span class="p">,</span> <span class="n">Key</span><span class="p">,</span> <span class="n">Value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">QKVLinearProjection</span><span class="p">(</span><span class="n">inputEmbedding</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer.forwardPropagation-150"><a href="#MultiAttentionHeadLayer.forwardPropagation-150"><span class="linenos">150</span></a>        <span class="n">QHead</span><span class="p">,</span> <span class="n">KHead</span><span class="p">,</span> <span class="n">VHead</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">SplitIntoHeads</span><span class="p">(</span><span class="n">Query</span><span class="p">,</span> <span class="n">Key</span><span class="p">,</span> <span class="n">Value</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer.forwardPropagation-151"><a href="#MultiAttentionHeadLayer.forwardPropagation-151"><span class="linenos">151</span></a>        <span class="n">combinedAttention</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">calculateAttention</span><span class="p">(</span><span class="n">QHead</span><span class="p">,</span> <span class="n">KHead</span><span class="p">,</span> <span class="n">VHead</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer.forwardPropagation-152"><a href="#MultiAttentionHeadLayer.forwardPropagation-152"><span class="linenos">152</span></a>        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputProjectionLayer</span><span class="p">(</span><span class="n">combinedAttention</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer.forwardPropagation-153"><a href="#MultiAttentionHeadLayer.forwardPropagation-153"><span class="linenos">153</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">batchIndex</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="MultiAttentionHeadLayer.forwardPropagation-154"><a href="#MultiAttentionHeadLayer.forwardPropagation-154"><span class="linenos">154</span></a>        <span class="k">return</span> <span class="n">output</span>
</span></pre></div>


    

                            </div>
                            <div id="MultiAttentionHeadLayer.createWeights" class="classattr">
                                        <input id="MultiAttentionHeadLayer.createWeights-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">createWeights</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="MultiAttentionHeadLayer.createWeights-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MultiAttentionHeadLayer.createWeights"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MultiAttentionHeadLayer.createWeights-156"><a href="#MultiAttentionHeadLayer.createWeights-156"><span class="linenos">156</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">createWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="MultiAttentionHeadLayer.createWeights-157"><a href="#MultiAttentionHeadLayer.createWeights-157"><span class="linenos">157</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">QueryWeights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initiaseWeight</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer.createWeights-158"><a href="#MultiAttentionHeadLayer.createWeights-158"><span class="linenos">158</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">KeyWeights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initiaseWeight</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer.createWeights-159"><a href="#MultiAttentionHeadLayer.createWeights-159"><span class="linenos">159</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">ValueWeights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initiaseWeight</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer.createWeights-160"><a href="#MultiAttentionHeadLayer.createWeights-160"><span class="linenos">160</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputWeight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initiaseWeight</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer.createWeights-161"><a href="#MultiAttentionHeadLayer.createWeights-161"><span class="linenos">161</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">outputBias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span><span class="p">))</span>
</span></pre></div>


    

                            </div>
                            <div id="MultiAttentionHeadLayer.backwardPropagation" class="classattr">
                                        <input id="MultiAttentionHeadLayer.backwardPropagation-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">backwardPropagation</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">outputGradient</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="MultiAttentionHeadLayer.backwardPropagation-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MultiAttentionHeadLayer.backwardPropagation"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MultiAttentionHeadLayer.backwardPropagation-166"><a href="#MultiAttentionHeadLayer.backwardPropagation-166"><span class="linenos">166</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">backwardPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputGradient</span><span class="p">):</span>
</span><span id="MultiAttentionHeadLayer.backwardPropagation-167"><a href="#MultiAttentionHeadLayer.backwardPropagation-167"><span class="linenos">167</span></a>        <span class="n">outputWeightGradient</span><span class="p">,</span> <span class="n">outputBiasGradient</span><span class="p">,</span> <span class="n">inputDerivative</span><span class="p">,</span> <span class="n">QueryWeightDerivative</span><span class="p">,</span> <span class="n">KeyWeightDerivative</span><span class="p">,</span> <span class="n">ValueWeightDerivative</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backProp</span><span class="o">.</span><span class="n">backPropagation</span><span class="p">(</span>
</span><span id="MultiAttentionHeadLayer.backwardPropagation-168"><a href="#MultiAttentionHeadLayer.backwardPropagation-168"><span class="linenos">168</span></a>            <span class="n">originalInput</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">originalInput</span><span class="p">,</span>
</span><span id="MultiAttentionHeadLayer.backwardPropagation-169"><a href="#MultiAttentionHeadLayer.backwardPropagation-169"><span class="linenos">169</span></a>            <span class="n">batchSize</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batchSize</span><span class="p">,</span>
</span><span id="MultiAttentionHeadLayer.backwardPropagation-170"><a href="#MultiAttentionHeadLayer.backwardPropagation-170"><span class="linenos">170</span></a>            <span class="n">sequenceLength</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sequenceLength</span><span class="p">,</span>
</span><span id="MultiAttentionHeadLayer.backwardPropagation-171"><a href="#MultiAttentionHeadLayer.backwardPropagation-171"><span class="linenos">171</span></a>            <span class="n">outputGradient</span><span class="o">=</span><span class="n">outputGradient</span><span class="p">,</span>
</span><span id="MultiAttentionHeadLayer.backwardPropagation-172"><a href="#MultiAttentionHeadLayer.backwardPropagation-172"><span class="linenos">172</span></a>            <span class="n">attentionHeads</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">attentionHeads</span><span class="p">,</span>
</span><span id="MultiAttentionHeadLayer.backwardPropagation-173"><a href="#MultiAttentionHeadLayer.backwardPropagation-173"><span class="linenos">173</span></a>            <span class="n">attentionWeights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">attentionWeights</span><span class="p">,</span>
</span><span id="MultiAttentionHeadLayer.backwardPropagation-174"><a href="#MultiAttentionHeadLayer.backwardPropagation-174"><span class="linenos">174</span></a>            <span class="n">numberOfHeads</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">numberOfHeads</span><span class="p">,</span>
</span><span id="MultiAttentionHeadLayer.backwardPropagation-175"><a href="#MultiAttentionHeadLayer.backwardPropagation-175"><span class="linenos">175</span></a>            <span class="n">Query</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">Query</span><span class="p">,</span>
</span><span id="MultiAttentionHeadLayer.backwardPropagation-176"><a href="#MultiAttentionHeadLayer.backwardPropagation-176"><span class="linenos">176</span></a>            <span class="n">Key</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">Key</span><span class="p">,</span>
</span><span id="MultiAttentionHeadLayer.backwardPropagation-177"><a href="#MultiAttentionHeadLayer.backwardPropagation-177"><span class="linenos">177</span></a>            <span class="n">Value</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">Value</span>
</span><span id="MultiAttentionHeadLayer.backwardPropagation-178"><a href="#MultiAttentionHeadLayer.backwardPropagation-178"><span class="linenos">178</span></a>        <span class="p">)</span>
</span><span id="MultiAttentionHeadLayer.backwardPropagation-179"><a href="#MultiAttentionHeadLayer.backwardPropagation-179"><span class="linenos">179</span></a>        <span class="k">return</span> <span class="n">outputWeightGradient</span><span class="p">,</span> <span class="n">outputBiasGradient</span><span class="p">,</span> <span class="n">inputDerivative</span><span class="p">,</span> <span class="n">QueryWeightDerivative</span><span class="p">,</span> <span class="n">KeyWeightDerivative</span><span class="p">,</span> <span class="n">ValueWeightDerivative</span>
</span></pre></div>


    

                            </div>
                            <div id="MultiAttentionHeadLayer.createCausalMask" class="classattr">
                                        <input id="MultiAttentionHeadLayer.createCausalMask-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">createCausalMask</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">batchSize</span>, </span><span class="param"><span class="n">sequenceLength</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="MultiAttentionHeadLayer.createCausalMask-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MultiAttentionHeadLayer.createCausalMask"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MultiAttentionHeadLayer.createCausalMask-181"><a href="#MultiAttentionHeadLayer.createCausalMask-181"><span class="linenos">181</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">createCausalMask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">sequenceLength</span><span class="p">):</span>
</span><span id="MultiAttentionHeadLayer.createCausalMask-182"><a href="#MultiAttentionHeadLayer.createCausalMask-182"><span class="linenos">182</span></a>        <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">sequenceLength</span><span class="p">,</span> <span class="n">sequenceLength</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">bool_</span><span class="p">))</span>
</span><span id="MultiAttentionHeadLayer.createCausalMask-183"><a href="#MultiAttentionHeadLayer.createCausalMask-183"><span class="linenos">183</span></a>        <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
</span><span id="MultiAttentionHeadLayer.createCausalMask-184"><a href="#MultiAttentionHeadLayer.createCausalMask-184"><span class="linenos">184</span></a>        <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="MultiAttentionHeadLayer.createCausalMask-185"><a href="#MultiAttentionHeadLayer.createCausalMask-185"><span class="linenos">185</span></a>        <span class="k">return</span> <span class="n">mask</span>
</span></pre></div>


    

                            </div>
                </section>
                <section id="EmbeddingLayer">
                            <input id="EmbeddingLayer-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">EmbeddingLayer</span>:

                <label class="view-source-button" for="EmbeddingLayer-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#EmbeddingLayer"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="EmbeddingLayer-4"><a href="#EmbeddingLayer-4"><span class="linenos"> 4</span></a><span class="k">class</span><span class="w"> </span><span class="nc">EmbeddingLayer</span><span class="p">:</span>
</span><span id="EmbeddingLayer-5"><a href="#EmbeddingLayer-5"><span class="linenos"> 5</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocabSize</span><span class="p">,</span> <span class="n">embedDimension</span><span class="p">):</span>
</span><span id="EmbeddingLayer-6"><a href="#EmbeddingLayer-6"><span class="linenos"> 6</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">vocabSize</span> <span class="o">=</span> <span class="n">vocabSize</span> <span class="c1">#number of unique tokens </span>
</span><span id="EmbeddingLayer-7"><a href="#EmbeddingLayer-7"><span class="linenos"> 7</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span> <span class="o">=</span> <span class="n">embedDimension</span>
</span><span id="EmbeddingLayer-8"><a href="#EmbeddingLayer-8"><span class="linenos"> 8</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">vocabSize</span><span class="p">,</span> <span class="n">embedDimension</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
</span><span id="EmbeddingLayer-9"><a href="#EmbeddingLayer-9"><span class="linenos"> 9</span></a>
</span><span id="EmbeddingLayer-10"><a href="#EmbeddingLayer-10"><span class="linenos">10</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forwardPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
</span><span id="EmbeddingLayer-11"><a href="#EmbeddingLayer-11"><span class="linenos">11</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">input</span> <span class="o">=</span> <span class="nb">input</span>
</span><span id="EmbeddingLayer-12"><a href="#EmbeddingLayer-12"><span class="linenos">12</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="nb">input</span><span class="p">]</span>
</span><span id="EmbeddingLayer-13"><a href="#EmbeddingLayer-13"><span class="linenos">13</span></a>    
</span><span id="EmbeddingLayer-14"><a href="#EmbeddingLayer-14"><span class="linenos">14</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">backwardPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputGradient</span><span class="p">):</span>
</span><span id="EmbeddingLayer-15"><a href="#EmbeddingLayer-15"><span class="linenos">15</span></a>        <span class="n">gradients</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span>
</span><span id="EmbeddingLayer-16"><a href="#EmbeddingLayer-16"><span class="linenos">16</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="EmbeddingLayer-17"><a href="#EmbeddingLayer-17"><span class="linenos">17</span></a>            <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
</span><span id="EmbeddingLayer-18"><a href="#EmbeddingLayer-18"><span class="linenos">18</span></a>                <span class="n">idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="p">[</span><span class="n">seq</span><span class="p">]</span>
</span><span id="EmbeddingLayer-19"><a href="#EmbeddingLayer-19"><span class="linenos">19</span></a>                <span class="n">gradients</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">+=</span> <span class="n">outputGradient</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">seq</span><span class="p">]</span>
</span><span id="EmbeddingLayer-20"><a href="#EmbeddingLayer-20"><span class="linenos">20</span></a>        <span class="k">elif</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">):</span>
</span><span id="EmbeddingLayer-21"><a href="#EmbeddingLayer-21"><span class="linenos">21</span></a>            <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
</span><span id="EmbeddingLayer-22"><a href="#EmbeddingLayer-22"><span class="linenos">22</span></a>                <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
</span><span id="EmbeddingLayer-23"><a href="#EmbeddingLayer-23"><span class="linenos">23</span></a>                    <span class="n">idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">seq</span><span class="p">]</span>
</span><span id="EmbeddingLayer-24"><a href="#EmbeddingLayer-24"><span class="linenos">24</span></a>                    <span class="n">gradients</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">+=</span> <span class="n">outputGradient</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">seq</span><span class="p">]</span>
</span><span id="EmbeddingLayer-25"><a href="#EmbeddingLayer-25"><span class="linenos">25</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="EmbeddingLayer-26"><a href="#EmbeddingLayer-26"><span class="linenos">26</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unexpected input dimension </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">D&quot;</span><span class="p">)</span>
</span><span id="EmbeddingLayer-27"><a href="#EmbeddingLayer-27"><span class="linenos">27</span></a>        <span class="k">return</span> <span class="n">gradients</span>
</span></pre></div>


    

                            <div id="EmbeddingLayer.__init__" class="classattr">
                                        <input id="EmbeddingLayer.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">EmbeddingLayer</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">vocabSize</span>, </span><span class="param"><span class="n">embedDimension</span></span>)</span>

                <label class="view-source-button" for="EmbeddingLayer.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#EmbeddingLayer.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="EmbeddingLayer.__init__-5"><a href="#EmbeddingLayer.__init__-5"><span class="linenos">5</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocabSize</span><span class="p">,</span> <span class="n">embedDimension</span><span class="p">):</span>
</span><span id="EmbeddingLayer.__init__-6"><a href="#EmbeddingLayer.__init__-6"><span class="linenos">6</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">vocabSize</span> <span class="o">=</span> <span class="n">vocabSize</span> <span class="c1">#number of unique tokens </span>
</span><span id="EmbeddingLayer.__init__-7"><a href="#EmbeddingLayer.__init__-7"><span class="linenos">7</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">embedDimension</span> <span class="o">=</span> <span class="n">embedDimension</span>
</span><span id="EmbeddingLayer.__init__-8"><a href="#EmbeddingLayer.__init__-8"><span class="linenos">8</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">vocabSize</span><span class="p">,</span> <span class="n">embedDimension</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
</span></pre></div>


    

                            </div>
                            <div id="EmbeddingLayer.vocabSize" class="classattr">
                                <div class="attr variable">
            <span class="name">vocabSize</span>

        
    </div>
    <a class="headerlink" href="#EmbeddingLayer.vocabSize"></a>
    
    

                            </div>
                            <div id="EmbeddingLayer.embedDimension" class="classattr">
                                <div class="attr variable">
            <span class="name">embedDimension</span>

        
    </div>
    <a class="headerlink" href="#EmbeddingLayer.embedDimension"></a>
    
    

                            </div>
                            <div id="EmbeddingLayer.weights" class="classattr">
                                <div class="attr variable">
            <span class="name">weights</span>

        
    </div>
    <a class="headerlink" href="#EmbeddingLayer.weights"></a>
    
    

                            </div>
                            <div id="EmbeddingLayer.forwardPropagation" class="classattr">
                                        <input id="EmbeddingLayer.forwardPropagation-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forwardPropagation</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="nb">input</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="EmbeddingLayer.forwardPropagation-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#EmbeddingLayer.forwardPropagation"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="EmbeddingLayer.forwardPropagation-10"><a href="#EmbeddingLayer.forwardPropagation-10"><span class="linenos">10</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forwardPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
</span><span id="EmbeddingLayer.forwardPropagation-11"><a href="#EmbeddingLayer.forwardPropagation-11"><span class="linenos">11</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">input</span> <span class="o">=</span> <span class="nb">input</span>
</span><span id="EmbeddingLayer.forwardPropagation-12"><a href="#EmbeddingLayer.forwardPropagation-12"><span class="linenos">12</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="nb">input</span><span class="p">]</span>
</span></pre></div>


    

                            </div>
                            <div id="EmbeddingLayer.backwardPropagation" class="classattr">
                                        <input id="EmbeddingLayer.backwardPropagation-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">backwardPropagation</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">outputGradient</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="EmbeddingLayer.backwardPropagation-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#EmbeddingLayer.backwardPropagation"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="EmbeddingLayer.backwardPropagation-14"><a href="#EmbeddingLayer.backwardPropagation-14"><span class="linenos">14</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">backwardPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputGradient</span><span class="p">):</span>
</span><span id="EmbeddingLayer.backwardPropagation-15"><a href="#EmbeddingLayer.backwardPropagation-15"><span class="linenos">15</span></a>        <span class="n">gradients</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span>
</span><span id="EmbeddingLayer.backwardPropagation-16"><a href="#EmbeddingLayer.backwardPropagation-16"><span class="linenos">16</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="EmbeddingLayer.backwardPropagation-17"><a href="#EmbeddingLayer.backwardPropagation-17"><span class="linenos">17</span></a>            <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
</span><span id="EmbeddingLayer.backwardPropagation-18"><a href="#EmbeddingLayer.backwardPropagation-18"><span class="linenos">18</span></a>                <span class="n">idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="p">[</span><span class="n">seq</span><span class="p">]</span>
</span><span id="EmbeddingLayer.backwardPropagation-19"><a href="#EmbeddingLayer.backwardPropagation-19"><span class="linenos">19</span></a>                <span class="n">gradients</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">+=</span> <span class="n">outputGradient</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">seq</span><span class="p">]</span>
</span><span id="EmbeddingLayer.backwardPropagation-20"><a href="#EmbeddingLayer.backwardPropagation-20"><span class="linenos">20</span></a>        <span class="k">elif</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">):</span>
</span><span id="EmbeddingLayer.backwardPropagation-21"><a href="#EmbeddingLayer.backwardPropagation-21"><span class="linenos">21</span></a>            <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
</span><span id="EmbeddingLayer.backwardPropagation-22"><a href="#EmbeddingLayer.backwardPropagation-22"><span class="linenos">22</span></a>                <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
</span><span id="EmbeddingLayer.backwardPropagation-23"><a href="#EmbeddingLayer.backwardPropagation-23"><span class="linenos">23</span></a>                    <span class="n">idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">seq</span><span class="p">]</span>
</span><span id="EmbeddingLayer.backwardPropagation-24"><a href="#EmbeddingLayer.backwardPropagation-24"><span class="linenos">24</span></a>                    <span class="n">gradients</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">+=</span> <span class="n">outputGradient</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">seq</span><span class="p">]</span>
</span><span id="EmbeddingLayer.backwardPropagation-25"><a href="#EmbeddingLayer.backwardPropagation-25"><span class="linenos">25</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="EmbeddingLayer.backwardPropagation-26"><a href="#EmbeddingLayer.backwardPropagation-26"><span class="linenos">26</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unexpected input dimension </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">D&quot;</span><span class="p">)</span>
</span><span id="EmbeddingLayer.backwardPropagation-27"><a href="#EmbeddingLayer.backwardPropagation-27"><span class="linenos">27</span></a>        <span class="k">return</span> <span class="n">gradients</span>
</span></pre></div>


    

                            </div>
                </section>
                <section id="FeedForwardNetwork">
                            <input id="FeedForwardNetwork-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">FeedForwardNetwork</span>:

                <label class="view-source-button" for="FeedForwardNetwork-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FeedForwardNetwork"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FeedForwardNetwork-12"><a href="#FeedForwardNetwork-12"><span class="linenos">12</span></a><span class="k">class</span><span class="w"> </span><span class="nc">FeedForwardNetwork</span><span class="p">:</span>
</span><span id="FeedForwardNetwork-13"><a href="#FeedForwardNetwork-13"><span class="linenos">13</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputDimension</span><span class="p">,</span> <span class="n">hiddenDimension</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">):</span>
</span><span id="FeedForwardNetwork-14"><a href="#FeedForwardNetwork-14"><span class="linenos">14</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">inputDimension</span> <span class="o">=</span> <span class="n">inputDimension</span>
</span><span id="FeedForwardNetwork-15"><a href="#FeedForwardNetwork-15"><span class="linenos">15</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hiddenDimension</span> <span class="o">=</span> <span class="n">hiddenDimension</span>
</span><span id="FeedForwardNetwork-16"><a href="#FeedForwardNetwork-16"><span class="linenos">16</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">W1</span> <span class="o">=</span> <span class="n">W1</span>
</span><span id="FeedForwardNetwork-17"><a href="#FeedForwardNetwork-17"><span class="linenos">17</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">b1</span> <span class="o">=</span> <span class="n">b1</span>
</span><span id="FeedForwardNetwork-18"><a href="#FeedForwardNetwork-18"><span class="linenos">18</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">W2</span>
</span><span id="FeedForwardNetwork-19"><a href="#FeedForwardNetwork-19"><span class="linenos">19</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">b2</span> <span class="o">=</span> <span class="n">b2</span>
</span><span id="FeedForwardNetwork-20"><a href="#FeedForwardNetwork-20"><span class="linenos">20</span></a>
</span><span id="FeedForwardNetwork-21"><a href="#FeedForwardNetwork-21"><span class="linenos">21</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">createWeights</span><span class="p">()</span>
</span><span id="FeedForwardNetwork-22"><a href="#FeedForwardNetwork-22"><span class="linenos">22</span></a>
</span><span id="FeedForwardNetwork-23"><a href="#FeedForwardNetwork-23"><span class="linenos">23</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">createWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="FeedForwardNetwork-24"><a href="#FeedForwardNetwork-24"><span class="linenos">24</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">W1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initiaseWeight</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inputDimension</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hiddenDimension</span><span class="p">)</span>
</span><span id="FeedForwardNetwork-25"><a href="#FeedForwardNetwork-25"><span class="linenos">25</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">W2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initiaseWeight</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hiddenDimension</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputDimension</span><span class="p">)</span>
</span><span id="FeedForwardNetwork-26"><a href="#FeedForwardNetwork-26"><span class="linenos">26</span></a>        
</span><span id="FeedForwardNetwork-27"><a href="#FeedForwardNetwork-27"><span class="linenos">27</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">b1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hiddenDimension</span><span class="p">))</span>
</span><span id="FeedForwardNetwork-28"><a href="#FeedForwardNetwork-28"><span class="linenos">28</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">b2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputDimension</span><span class="p">))</span>
</span><span id="FeedForwardNetwork-29"><a href="#FeedForwardNetwork-29"><span class="linenos">29</span></a>
</span><span id="FeedForwardNetwork-30"><a href="#FeedForwardNetwork-30"><span class="linenos">30</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_initiaseWeight</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputDimension</span><span class="p">,</span> <span class="n">outputDimension</span><span class="p">):</span>
</span><span id="FeedForwardNetwork-31"><a href="#FeedForwardNetwork-31"><span class="linenos">31</span></a>        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">inputDimension</span><span class="p">,</span> <span class="n">outputDimension</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">inputDimension</span><span class="p">))</span>
</span><span id="FeedForwardNetwork-32"><a href="#FeedForwardNetwork-32"><span class="linenos">32</span></a>    
</span><span id="FeedForwardNetwork-33"><a href="#FeedForwardNetwork-33"><span class="linenos">33</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forwardPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputTokens</span><span class="p">):</span> 
</span><span id="FeedForwardNetwork-34"><a href="#FeedForwardNetwork-34"><span class="linenos">34</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">input</span> <span class="o">=</span> <span class="n">inputTokens</span>
</span><span id="FeedForwardNetwork-35"><a href="#FeedForwardNetwork-35"><span class="linenos">35</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">firstLayer</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputTokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b1</span>
</span><span id="FeedForwardNetwork-36"><a href="#FeedForwardNetwork-36"><span class="linenos">36</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activated</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">firstLayer</span><span class="p">)</span> <span class="c1"># ReLU</span>
</span><span id="FeedForwardNetwork-37"><a href="#FeedForwardNetwork-37"><span class="linenos">37</span></a>        <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">activated</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b2</span>
</span><span id="FeedForwardNetwork-38"><a href="#FeedForwardNetwork-38"><span class="linenos">38</span></a>        <span class="k">return</span> <span class="n">output</span>
</span><span id="FeedForwardNetwork-39"><a href="#FeedForwardNetwork-39"><span class="linenos">39</span></a>    
</span><span id="FeedForwardNetwork-40"><a href="#FeedForwardNetwork-40"><span class="linenos">40</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">backwardPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputGradient</span><span class="p">):</span>
</span><span id="FeedForwardNetwork-41"><a href="#FeedForwardNetwork-41"><span class="linenos">41</span></a>        <span class="n">weightGradient2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">activated</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">outputGradient</span><span class="p">)</span>
</span><span id="FeedForwardNetwork-42"><a href="#FeedForwardNetwork-42"><span class="linenos">42</span></a>        <span class="n">biasGradient2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">outputGradient</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="FeedForwardNetwork-43"><a href="#FeedForwardNetwork-43"><span class="linenos">43</span></a>
</span><span id="FeedForwardNetwork-44"><a href="#FeedForwardNetwork-44"><span class="linenos">44</span></a>        <span class="n">activatedGradient</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">outputGradient</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</span><span id="FeedForwardNetwork-45"><a href="#FeedForwardNetwork-45"><span class="linenos">45</span></a>        <span class="n">firstLayerDerivative</span> <span class="o">=</span> <span class="n">activatedGradient</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">firstLayer</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
</span><span id="FeedForwardNetwork-46"><a href="#FeedForwardNetwork-46"><span class="linenos">46</span></a>
</span><span id="FeedForwardNetwork-47"><a href="#FeedForwardNetwork-47"><span class="linenos">47</span></a>        <span class="n">weightGradient1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">firstLayerDerivative</span><span class="p">)</span>
</span><span id="FeedForwardNetwork-48"><a href="#FeedForwardNetwork-48"><span class="linenos">48</span></a>        <span class="n">biasGradient1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">firstLayerDerivative</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="FeedForwardNetwork-49"><a href="#FeedForwardNetwork-49"><span class="linenos">49</span></a>
</span><span id="FeedForwardNetwork-50"><a href="#FeedForwardNetwork-50"><span class="linenos">50</span></a>        <span class="n">inputDerivative</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">firstLayerDerivative</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W1</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</span><span id="FeedForwardNetwork-51"><a href="#FeedForwardNetwork-51"><span class="linenos">51</span></a>
</span><span id="FeedForwardNetwork-52"><a href="#FeedForwardNetwork-52"><span class="linenos">52</span></a>        <span class="k">return</span> <span class="n">inputDerivative</span><span class="p">,</span> <span class="n">weightGradient1</span><span class="p">,</span> <span class="n">biasGradient1</span><span class="p">,</span> <span class="n">weightGradient2</span><span class="p">,</span> <span class="n">biasGradient2</span>
</span></pre></div>


    

                            <div id="FeedForwardNetwork.__init__" class="classattr">
                                        <input id="FeedForwardNetwork.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">FeedForwardNetwork</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">inputDimension</span>, </span><span class="param"><span class="n">hiddenDimension</span>, </span><span class="param"><span class="n">W1</span>, </span><span class="param"><span class="n">b1</span>, </span><span class="param"><span class="n">W2</span>, </span><span class="param"><span class="n">b2</span></span>)</span>

                <label class="view-source-button" for="FeedForwardNetwork.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FeedForwardNetwork.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FeedForwardNetwork.__init__-13"><a href="#FeedForwardNetwork.__init__-13"><span class="linenos">13</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputDimension</span><span class="p">,</span> <span class="n">hiddenDimension</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">):</span>
</span><span id="FeedForwardNetwork.__init__-14"><a href="#FeedForwardNetwork.__init__-14"><span class="linenos">14</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">inputDimension</span> <span class="o">=</span> <span class="n">inputDimension</span>
</span><span id="FeedForwardNetwork.__init__-15"><a href="#FeedForwardNetwork.__init__-15"><span class="linenos">15</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hiddenDimension</span> <span class="o">=</span> <span class="n">hiddenDimension</span>
</span><span id="FeedForwardNetwork.__init__-16"><a href="#FeedForwardNetwork.__init__-16"><span class="linenos">16</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">W1</span> <span class="o">=</span> <span class="n">W1</span>
</span><span id="FeedForwardNetwork.__init__-17"><a href="#FeedForwardNetwork.__init__-17"><span class="linenos">17</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">b1</span> <span class="o">=</span> <span class="n">b1</span>
</span><span id="FeedForwardNetwork.__init__-18"><a href="#FeedForwardNetwork.__init__-18"><span class="linenos">18</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">W2</span>
</span><span id="FeedForwardNetwork.__init__-19"><a href="#FeedForwardNetwork.__init__-19"><span class="linenos">19</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">b2</span> <span class="o">=</span> <span class="n">b2</span>
</span><span id="FeedForwardNetwork.__init__-20"><a href="#FeedForwardNetwork.__init__-20"><span class="linenos">20</span></a>
</span><span id="FeedForwardNetwork.__init__-21"><a href="#FeedForwardNetwork.__init__-21"><span class="linenos">21</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">createWeights</span><span class="p">()</span>
</span></pre></div>


    

                            </div>
                            <div id="FeedForwardNetwork.inputDimension" class="classattr">
                                <div class="attr variable">
            <span class="name">inputDimension</span>

        
    </div>
    <a class="headerlink" href="#FeedForwardNetwork.inputDimension"></a>
    
    

                            </div>
                            <div id="FeedForwardNetwork.hiddenDimension" class="classattr">
                                <div class="attr variable">
            <span class="name">hiddenDimension</span>

        
    </div>
    <a class="headerlink" href="#FeedForwardNetwork.hiddenDimension"></a>
    
    

                            </div>
                            <div id="FeedForwardNetwork.W1" class="classattr">
                                <div class="attr variable">
            <span class="name">W1</span>

        
    </div>
    <a class="headerlink" href="#FeedForwardNetwork.W1"></a>
    
    

                            </div>
                            <div id="FeedForwardNetwork.b1" class="classattr">
                                <div class="attr variable">
            <span class="name">b1</span>

        
    </div>
    <a class="headerlink" href="#FeedForwardNetwork.b1"></a>
    
    

                            </div>
                            <div id="FeedForwardNetwork.W2" class="classattr">
                                <div class="attr variable">
            <span class="name">W2</span>

        
    </div>
    <a class="headerlink" href="#FeedForwardNetwork.W2"></a>
    
    

                            </div>
                            <div id="FeedForwardNetwork.b2" class="classattr">
                                <div class="attr variable">
            <span class="name">b2</span>

        
    </div>
    <a class="headerlink" href="#FeedForwardNetwork.b2"></a>
    
    

                            </div>
                            <div id="FeedForwardNetwork.createWeights" class="classattr">
                                        <input id="FeedForwardNetwork.createWeights-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">createWeights</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="FeedForwardNetwork.createWeights-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FeedForwardNetwork.createWeights"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FeedForwardNetwork.createWeights-23"><a href="#FeedForwardNetwork.createWeights-23"><span class="linenos">23</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">createWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="FeedForwardNetwork.createWeights-24"><a href="#FeedForwardNetwork.createWeights-24"><span class="linenos">24</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">W1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initiaseWeight</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inputDimension</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hiddenDimension</span><span class="p">)</span>
</span><span id="FeedForwardNetwork.createWeights-25"><a href="#FeedForwardNetwork.createWeights-25"><span class="linenos">25</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">W2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initiaseWeight</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hiddenDimension</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputDimension</span><span class="p">)</span>
</span><span id="FeedForwardNetwork.createWeights-26"><a href="#FeedForwardNetwork.createWeights-26"><span class="linenos">26</span></a>        
</span><span id="FeedForwardNetwork.createWeights-27"><a href="#FeedForwardNetwork.createWeights-27"><span class="linenos">27</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">b1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hiddenDimension</span><span class="p">))</span>
</span><span id="FeedForwardNetwork.createWeights-28"><a href="#FeedForwardNetwork.createWeights-28"><span class="linenos">28</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">b2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputDimension</span><span class="p">))</span>
</span></pre></div>


    

                            </div>
                            <div id="FeedForwardNetwork.forwardPropagation" class="classattr">
                                        <input id="FeedForwardNetwork.forwardPropagation-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forwardPropagation</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">inputTokens</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="FeedForwardNetwork.forwardPropagation-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FeedForwardNetwork.forwardPropagation"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FeedForwardNetwork.forwardPropagation-33"><a href="#FeedForwardNetwork.forwardPropagation-33"><span class="linenos">33</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forwardPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputTokens</span><span class="p">):</span> 
</span><span id="FeedForwardNetwork.forwardPropagation-34"><a href="#FeedForwardNetwork.forwardPropagation-34"><span class="linenos">34</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">input</span> <span class="o">=</span> <span class="n">inputTokens</span>
</span><span id="FeedForwardNetwork.forwardPropagation-35"><a href="#FeedForwardNetwork.forwardPropagation-35"><span class="linenos">35</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">firstLayer</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputTokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b1</span>
</span><span id="FeedForwardNetwork.forwardPropagation-36"><a href="#FeedForwardNetwork.forwardPropagation-36"><span class="linenos">36</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activated</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">firstLayer</span><span class="p">)</span> <span class="c1"># ReLU</span>
</span><span id="FeedForwardNetwork.forwardPropagation-37"><a href="#FeedForwardNetwork.forwardPropagation-37"><span class="linenos">37</span></a>        <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">activated</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b2</span>
</span><span id="FeedForwardNetwork.forwardPropagation-38"><a href="#FeedForwardNetwork.forwardPropagation-38"><span class="linenos">38</span></a>        <span class="k">return</span> <span class="n">output</span>
</span></pre></div>


    

                            </div>
                            <div id="FeedForwardNetwork.backwardPropagation" class="classattr">
                                        <input id="FeedForwardNetwork.backwardPropagation-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">backwardPropagation</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">outputGradient</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="FeedForwardNetwork.backwardPropagation-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#FeedForwardNetwork.backwardPropagation"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="FeedForwardNetwork.backwardPropagation-40"><a href="#FeedForwardNetwork.backwardPropagation-40"><span class="linenos">40</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">backwardPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputGradient</span><span class="p">):</span>
</span><span id="FeedForwardNetwork.backwardPropagation-41"><a href="#FeedForwardNetwork.backwardPropagation-41"><span class="linenos">41</span></a>        <span class="n">weightGradient2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">activated</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">outputGradient</span><span class="p">)</span>
</span><span id="FeedForwardNetwork.backwardPropagation-42"><a href="#FeedForwardNetwork.backwardPropagation-42"><span class="linenos">42</span></a>        <span class="n">biasGradient2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">outputGradient</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="FeedForwardNetwork.backwardPropagation-43"><a href="#FeedForwardNetwork.backwardPropagation-43"><span class="linenos">43</span></a>
</span><span id="FeedForwardNetwork.backwardPropagation-44"><a href="#FeedForwardNetwork.backwardPropagation-44"><span class="linenos">44</span></a>        <span class="n">activatedGradient</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">outputGradient</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</span><span id="FeedForwardNetwork.backwardPropagation-45"><a href="#FeedForwardNetwork.backwardPropagation-45"><span class="linenos">45</span></a>        <span class="n">firstLayerDerivative</span> <span class="o">=</span> <span class="n">activatedGradient</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">firstLayer</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
</span><span id="FeedForwardNetwork.backwardPropagation-46"><a href="#FeedForwardNetwork.backwardPropagation-46"><span class="linenos">46</span></a>
</span><span id="FeedForwardNetwork.backwardPropagation-47"><a href="#FeedForwardNetwork.backwardPropagation-47"><span class="linenos">47</span></a>        <span class="n">weightGradient1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">firstLayerDerivative</span><span class="p">)</span>
</span><span id="FeedForwardNetwork.backwardPropagation-48"><a href="#FeedForwardNetwork.backwardPropagation-48"><span class="linenos">48</span></a>        <span class="n">biasGradient1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">firstLayerDerivative</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="FeedForwardNetwork.backwardPropagation-49"><a href="#FeedForwardNetwork.backwardPropagation-49"><span class="linenos">49</span></a>
</span><span id="FeedForwardNetwork.backwardPropagation-50"><a href="#FeedForwardNetwork.backwardPropagation-50"><span class="linenos">50</span></a>        <span class="n">inputDerivative</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">firstLayerDerivative</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W1</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</span><span id="FeedForwardNetwork.backwardPropagation-51"><a href="#FeedForwardNetwork.backwardPropagation-51"><span class="linenos">51</span></a>
</span><span id="FeedForwardNetwork.backwardPropagation-52"><a href="#FeedForwardNetwork.backwardPropagation-52"><span class="linenos">52</span></a>        <span class="k">return</span> <span class="n">inputDerivative</span><span class="p">,</span> <span class="n">weightGradient1</span><span class="p">,</span> <span class="n">biasGradient1</span><span class="p">,</span> <span class="n">weightGradient2</span><span class="p">,</span> <span class="n">biasGradient2</span>
</span></pre></div>


    

                            </div>
                </section>
                <section id="NormLayer">
                            <input id="NormLayer-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">NormLayer</span>:

                <label class="view-source-button" for="NormLayer-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#NormLayer"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="NormLayer-4"><a href="#NormLayer-4"><span class="linenos"> 4</span></a><span class="k">class</span><span class="w"> </span><span class="nc">NormLayer</span><span class="p">:</span>
</span><span id="NormLayer-5"><a href="#NormLayer-5"><span class="linenos"> 5</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">):</span>
</span><span id="NormLayer-6"><a href="#NormLayer-6"><span class="linenos"> 6</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">features</span><span class="p">))</span> <span class="c1"># scale</span>
</span><span id="NormLayer-7"><a href="#NormLayer-7"><span class="linenos"> 7</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">features</span><span class="p">))</span> <span class="c1"># shift</span>
</span><span id="NormLayer-8"><a href="#NormLayer-8"><span class="linenos"> 8</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span> <span class="c1"># adds a small constant to avoid division by 0</span>
</span><span id="NormLayer-9"><a href="#NormLayer-9"><span class="linenos"> 9</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">input</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="NormLayer-10"><a href="#NormLayer-10"><span class="linenos">10</span></a>
</span><span id="NormLayer-11"><a href="#NormLayer-11"><span class="linenos">11</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forwardPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="NormLayer-12"><a href="#NormLayer-12"><span class="linenos">12</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">input</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="NormLayer-13"><a href="#NormLayer-13"><span class="linenos">13</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
</span><span id="NormLayer-14"><a href="#NormLayer-14"><span class="linenos">14</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="NormLayer-15"><a href="#NormLayer-15"><span class="linenos">15</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">variance</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span>
</span><span id="NormLayer-16"><a href="#NormLayer-16"><span class="linenos">16</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">normalised</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">std</span>
</span><span id="NormLayer-17"><a href="#NormLayer-17"><span class="linenos">17</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalised</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span>
</span><span id="NormLayer-18"><a href="#NormLayer-18"><span class="linenos">18</span></a>    
</span><span id="NormLayer-19"><a href="#NormLayer-19"><span class="linenos">19</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">backwardPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputDerivative</span><span class="p">):</span>
</span><span id="NormLayer-20"><a href="#NormLayer-20"><span class="linenos">20</span></a>        <span class="n">m</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span><span id="NormLayer-21"><a href="#NormLayer-21"><span class="linenos">21</span></a>
</span><span id="NormLayer-22"><a href="#NormLayer-22"><span class="linenos">22</span></a>        <span class="n">gammaDerivative</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">outputDerivative</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalised</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="NormLayer-23"><a href="#NormLayer-23"><span class="linenos">23</span></a>        <span class="n">betaDerivative</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">outputDerivative</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="NormLayer-24"><a href="#NormLayer-24"><span class="linenos">24</span></a>
</span><span id="NormLayer-25"><a href="#NormLayer-25"><span class="linenos">25</span></a>        <span class="n">dx</span> <span class="o">=</span> <span class="n">outputDerivative</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span>
</span><span id="NormLayer-26"><a href="#NormLayer-26"><span class="linenos">26</span></a>
</span><span id="NormLayer-27"><a href="#NormLayer-27"><span class="linenos">27</span></a>        <span class="n">inputDerivative</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">std</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span>
</span><span id="NormLayer-28"><a href="#NormLayer-28"><span class="linenos">28</span></a>            <span class="n">m</span> <span class="o">*</span> <span class="n">dx</span> 
</span><span id="NormLayer-29"><a href="#NormLayer-29"><span class="linenos">29</span></a>            <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dx</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
</span><span id="NormLayer-30"><a href="#NormLayer-30"><span class="linenos">30</span></a>            <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalised</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dx</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalised</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="NormLayer-31"><a href="#NormLayer-31"><span class="linenos">31</span></a>        <span class="p">)</span>
</span><span id="NormLayer-32"><a href="#NormLayer-32"><span class="linenos">32</span></a>
</span><span id="NormLayer-33"><a href="#NormLayer-33"><span class="linenos">33</span></a>        <span class="k">return</span> <span class="n">inputDerivative</span><span class="p">,</span> <span class="n">gammaDerivative</span><span class="p">,</span> <span class="n">betaDerivative</span>
</span></pre></div>


    

                            <div id="NormLayer.__init__" class="classattr">
                                        <input id="NormLayer.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">NormLayer</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">features</span>, </span><span class="param"><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-06</span></span>)</span>

                <label class="view-source-button" for="NormLayer.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#NormLayer.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="NormLayer.__init__-5"><a href="#NormLayer.__init__-5"><span class="linenos">5</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">):</span>
</span><span id="NormLayer.__init__-6"><a href="#NormLayer.__init__-6"><span class="linenos">6</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">features</span><span class="p">))</span> <span class="c1"># scale</span>
</span><span id="NormLayer.__init__-7"><a href="#NormLayer.__init__-7"><span class="linenos">7</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">features</span><span class="p">))</span> <span class="c1"># shift</span>
</span><span id="NormLayer.__init__-8"><a href="#NormLayer.__init__-8"><span class="linenos">8</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span> <span class="c1"># adds a small constant to avoid division by 0</span>
</span><span id="NormLayer.__init__-9"><a href="#NormLayer.__init__-9"><span class="linenos">9</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">input</span> <span class="o">=</span> <span class="kc">None</span>
</span></pre></div>


    

                            </div>
                            <div id="NormLayer.gamma" class="classattr">
                                <div class="attr variable">
            <span class="name">gamma</span>

        
    </div>
    <a class="headerlink" href="#NormLayer.gamma"></a>
    
    

                            </div>
                            <div id="NormLayer.beta" class="classattr">
                                <div class="attr variable">
            <span class="name">beta</span>

        
    </div>
    <a class="headerlink" href="#NormLayer.beta"></a>
    
    

                            </div>
                            <div id="NormLayer.epsilon" class="classattr">
                                <div class="attr variable">
            <span class="name">epsilon</span>

        
    </div>
    <a class="headerlink" href="#NormLayer.epsilon"></a>
    
    

                            </div>
                            <div id="NormLayer.input" class="classattr">
                                <div class="attr variable">
            <span class="name">input</span>

        
    </div>
    <a class="headerlink" href="#NormLayer.input"></a>
    
    

                            </div>
                            <div id="NormLayer.forwardPropagation" class="classattr">
                                        <input id="NormLayer.forwardPropagation-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forwardPropagation</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">x</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="NormLayer.forwardPropagation-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#NormLayer.forwardPropagation"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="NormLayer.forwardPropagation-11"><a href="#NormLayer.forwardPropagation-11"><span class="linenos">11</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forwardPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="NormLayer.forwardPropagation-12"><a href="#NormLayer.forwardPropagation-12"><span class="linenos">12</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">input</span> <span class="o">=</span> <span class="n">x</span>
</span><span id="NormLayer.forwardPropagation-13"><a href="#NormLayer.forwardPropagation-13"><span class="linenos">13</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
</span><span id="NormLayer.forwardPropagation-14"><a href="#NormLayer.forwardPropagation-14"><span class="linenos">14</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="NormLayer.forwardPropagation-15"><a href="#NormLayer.forwardPropagation-15"><span class="linenos">15</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">variance</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span>
</span><span id="NormLayer.forwardPropagation-16"><a href="#NormLayer.forwardPropagation-16"><span class="linenos">16</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">normalised</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">std</span>
</span><span id="NormLayer.forwardPropagation-17"><a href="#NormLayer.forwardPropagation-17"><span class="linenos">17</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalised</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span>
</span></pre></div>


    

                            </div>
                            <div id="NormLayer.backwardPropagation" class="classattr">
                                        <input id="NormLayer.backwardPropagation-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">backwardPropagation</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">outputDerivative</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="NormLayer.backwardPropagation-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#NormLayer.backwardPropagation"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="NormLayer.backwardPropagation-19"><a href="#NormLayer.backwardPropagation-19"><span class="linenos">19</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">backwardPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputDerivative</span><span class="p">):</span>
</span><span id="NormLayer.backwardPropagation-20"><a href="#NormLayer.backwardPropagation-20"><span class="linenos">20</span></a>        <span class="n">m</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span><span id="NormLayer.backwardPropagation-21"><a href="#NormLayer.backwardPropagation-21"><span class="linenos">21</span></a>
</span><span id="NormLayer.backwardPropagation-22"><a href="#NormLayer.backwardPropagation-22"><span class="linenos">22</span></a>        <span class="n">gammaDerivative</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">outputDerivative</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalised</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="NormLayer.backwardPropagation-23"><a href="#NormLayer.backwardPropagation-23"><span class="linenos">23</span></a>        <span class="n">betaDerivative</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">outputDerivative</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="NormLayer.backwardPropagation-24"><a href="#NormLayer.backwardPropagation-24"><span class="linenos">24</span></a>
</span><span id="NormLayer.backwardPropagation-25"><a href="#NormLayer.backwardPropagation-25"><span class="linenos">25</span></a>        <span class="n">dx</span> <span class="o">=</span> <span class="n">outputDerivative</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span>
</span><span id="NormLayer.backwardPropagation-26"><a href="#NormLayer.backwardPropagation-26"><span class="linenos">26</span></a>
</span><span id="NormLayer.backwardPropagation-27"><a href="#NormLayer.backwardPropagation-27"><span class="linenos">27</span></a>        <span class="n">inputDerivative</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">std</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span>
</span><span id="NormLayer.backwardPropagation-28"><a href="#NormLayer.backwardPropagation-28"><span class="linenos">28</span></a>            <span class="n">m</span> <span class="o">*</span> <span class="n">dx</span> 
</span><span id="NormLayer.backwardPropagation-29"><a href="#NormLayer.backwardPropagation-29"><span class="linenos">29</span></a>            <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dx</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
</span><span id="NormLayer.backwardPropagation-30"><a href="#NormLayer.backwardPropagation-30"><span class="linenos">30</span></a>            <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalised</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dx</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalised</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="NormLayer.backwardPropagation-31"><a href="#NormLayer.backwardPropagation-31"><span class="linenos">31</span></a>        <span class="p">)</span>
</span><span id="NormLayer.backwardPropagation-32"><a href="#NormLayer.backwardPropagation-32"><span class="linenos">32</span></a>
</span><span id="NormLayer.backwardPropagation-33"><a href="#NormLayer.backwardPropagation-33"><span class="linenos">33</span></a>        <span class="k">return</span> <span class="n">inputDerivative</span><span class="p">,</span> <span class="n">gammaDerivative</span><span class="p">,</span> <span class="n">betaDerivative</span>
</span></pre></div>


    

                            </div>
                </section>
                <section id="PositionalEncoding">
                            <input id="PositionalEncoding-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">PositionalEncoding</span>:

                <label class="view-source-button" for="PositionalEncoding-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#PositionalEncoding"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="PositionalEncoding-9"><a href="#PositionalEncoding-9"><span class="linenos"> 9</span></a><span class="k">class</span><span class="w"> </span><span class="nc">PositionalEncoding</span><span class="p">:</span>
</span><span id="PositionalEncoding-10"><a href="#PositionalEncoding-10"><span class="linenos">10</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">maxDimension</span><span class="p">,</span> <span class="n">embeddingSize</span><span class="p">):</span>
</span><span id="PositionalEncoding-11"><a href="#PositionalEncoding-11"><span class="linenos">11</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">maxDimension</span> <span class="o">=</span> <span class="n">maxDimension</span>
</span><span id="PositionalEncoding-12"><a href="#PositionalEncoding-12"><span class="linenos">12</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">embeddingSize</span> <span class="o">=</span> <span class="n">embeddingSize</span>
</span><span id="PositionalEncoding-13"><a href="#PositionalEncoding-13"><span class="linenos">13</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">encoding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">createEmbed</span><span class="p">()</span>
</span><span id="PositionalEncoding-14"><a href="#PositionalEncoding-14"><span class="linenos">14</span></a>
</span><span id="PositionalEncoding-15"><a href="#PositionalEncoding-15"><span class="linenos">15</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">createEmbed</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="PositionalEncoding-16"><a href="#PositionalEncoding-16"><span class="linenos">16</span></a>        <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">maxDimension</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
</span><span id="PositionalEncoding-17"><a href="#PositionalEncoding-17"><span class="linenos">17</span></a>        <span class="n">divTerm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddingSize</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">100000</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddingSize</span><span class="p">))</span>
</span><span id="PositionalEncoding-18"><a href="#PositionalEncoding-18"><span class="linenos">18</span></a>
</span><span id="PositionalEncoding-19"><a href="#PositionalEncoding-19"><span class="linenos">19</span></a>        <span class="n">positionalEmbedding</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">maxDimension</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddingSize</span><span class="p">))</span>
</span><span id="PositionalEncoding-20"><a href="#PositionalEncoding-20"><span class="linenos">20</span></a>        <span class="n">positionalEmbedding</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">divTerm</span><span class="p">)</span>
</span><span id="PositionalEncoding-21"><a href="#PositionalEncoding-21"><span class="linenos">21</span></a>        <span class="n">positionalEmbedding</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">divTerm</span><span class="p">)</span>
</span><span id="PositionalEncoding-22"><a href="#PositionalEncoding-22"><span class="linenos">22</span></a>        <span class="k">return</span> <span class="n">positionalEmbedding</span>
</span><span id="PositionalEncoding-23"><a href="#PositionalEncoding-23"><span class="linenos">23</span></a>    
</span><span id="PositionalEncoding-24"><a href="#PositionalEncoding-24"><span class="linenos">24</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forwardPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">):</span>
</span><span id="PositionalEncoding-25"><a href="#PositionalEncoding-25"><span class="linenos">25</span></a>        <span class="k">return</span> <span class="n">inputData</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoding</span><span class="p">[:</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputData</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
</span></pre></div>


    

                            <div id="PositionalEncoding.__init__" class="classattr">
                                        <input id="PositionalEncoding.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">PositionalEncoding</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">maxDimension</span>, </span><span class="param"><span class="n">embeddingSize</span></span>)</span>

                <label class="view-source-button" for="PositionalEncoding.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#PositionalEncoding.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="PositionalEncoding.__init__-10"><a href="#PositionalEncoding.__init__-10"><span class="linenos">10</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">maxDimension</span><span class="p">,</span> <span class="n">embeddingSize</span><span class="p">):</span>
</span><span id="PositionalEncoding.__init__-11"><a href="#PositionalEncoding.__init__-11"><span class="linenos">11</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">maxDimension</span> <span class="o">=</span> <span class="n">maxDimension</span>
</span><span id="PositionalEncoding.__init__-12"><a href="#PositionalEncoding.__init__-12"><span class="linenos">12</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">embeddingSize</span> <span class="o">=</span> <span class="n">embeddingSize</span>
</span><span id="PositionalEncoding.__init__-13"><a href="#PositionalEncoding.__init__-13"><span class="linenos">13</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">encoding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">createEmbed</span><span class="p">()</span>
</span></pre></div>


    

                            </div>
                            <div id="PositionalEncoding.maxDimension" class="classattr">
                                <div class="attr variable">
            <span class="name">maxDimension</span>

        
    </div>
    <a class="headerlink" href="#PositionalEncoding.maxDimension"></a>
    
    

                            </div>
                            <div id="PositionalEncoding.embeddingSize" class="classattr">
                                <div class="attr variable">
            <span class="name">embeddingSize</span>

        
    </div>
    <a class="headerlink" href="#PositionalEncoding.embeddingSize"></a>
    
    

                            </div>
                            <div id="PositionalEncoding.encoding" class="classattr">
                                <div class="attr variable">
            <span class="name">encoding</span>

        
    </div>
    <a class="headerlink" href="#PositionalEncoding.encoding"></a>
    
    

                            </div>
                            <div id="PositionalEncoding.createEmbed" class="classattr">
                                        <input id="PositionalEncoding.createEmbed-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">createEmbed</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="PositionalEncoding.createEmbed-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#PositionalEncoding.createEmbed"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="PositionalEncoding.createEmbed-15"><a href="#PositionalEncoding.createEmbed-15"><span class="linenos">15</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">createEmbed</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="PositionalEncoding.createEmbed-16"><a href="#PositionalEncoding.createEmbed-16"><span class="linenos">16</span></a>        <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">maxDimension</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
</span><span id="PositionalEncoding.createEmbed-17"><a href="#PositionalEncoding.createEmbed-17"><span class="linenos">17</span></a>        <span class="n">divTerm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddingSize</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">100000</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddingSize</span><span class="p">))</span>
</span><span id="PositionalEncoding.createEmbed-18"><a href="#PositionalEncoding.createEmbed-18"><span class="linenos">18</span></a>
</span><span id="PositionalEncoding.createEmbed-19"><a href="#PositionalEncoding.createEmbed-19"><span class="linenos">19</span></a>        <span class="n">positionalEmbedding</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">maxDimension</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddingSize</span><span class="p">))</span>
</span><span id="PositionalEncoding.createEmbed-20"><a href="#PositionalEncoding.createEmbed-20"><span class="linenos">20</span></a>        <span class="n">positionalEmbedding</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">divTerm</span><span class="p">)</span>
</span><span id="PositionalEncoding.createEmbed-21"><a href="#PositionalEncoding.createEmbed-21"><span class="linenos">21</span></a>        <span class="n">positionalEmbedding</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">divTerm</span><span class="p">)</span>
</span><span id="PositionalEncoding.createEmbed-22"><a href="#PositionalEncoding.createEmbed-22"><span class="linenos">22</span></a>        <span class="k">return</span> <span class="n">positionalEmbedding</span>
</span></pre></div>


    

                            </div>
                            <div id="PositionalEncoding.forwardPropagation" class="classattr">
                                        <input id="PositionalEncoding.forwardPropagation-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forwardPropagation</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">inputData</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="PositionalEncoding.forwardPropagation-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#PositionalEncoding.forwardPropagation"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="PositionalEncoding.forwardPropagation-24"><a href="#PositionalEncoding.forwardPropagation-24"><span class="linenos">24</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forwardPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">):</span>
</span><span id="PositionalEncoding.forwardPropagation-25"><a href="#PositionalEncoding.forwardPropagation-25"><span class="linenos">25</span></a>        <span class="k">return</span> <span class="n">inputData</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoding</span><span class="p">[:</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputData</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
</span></pre></div>


    

                            </div>
                </section>
                <section id="ResidualConnection">
                            <input id="ResidualConnection-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">ResidualConnection</span>:

                <label class="view-source-button" for="ResidualConnection-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ResidualConnection"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ResidualConnection-3"><a href="#ResidualConnection-3"><span class="linenos">3</span></a><span class="k">class</span><span class="w"> </span><span class="nc">ResidualConnection</span><span class="p">:</span>
</span><span id="ResidualConnection-4"><a href="#ResidualConnection-4"><span class="linenos">4</span></a>    <span class="c1">#output being the output of the layer before</span>
</span><span id="ResidualConnection-5"><a href="#ResidualConnection-5"><span class="linenos">5</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forwardPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span> <span class="c1"># input being the input of the layer before</span>
</span><span id="ResidualConnection-6"><a href="#ResidualConnection-6"><span class="linenos">6</span></a>        <span class="k">return</span> <span class="nb">input</span> <span class="o">+</span> <span class="n">output</span> <span class="c1"># y = x + f(x)</span>
</span></pre></div>


    

                            <div id="ResidualConnection.forwardPropagation" class="classattr">
                                        <input id="ResidualConnection.forwardPropagation-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forwardPropagation</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="nb">input</span>, </span><span class="param"><span class="n">output</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="ResidualConnection.forwardPropagation-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ResidualConnection.forwardPropagation"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ResidualConnection.forwardPropagation-5"><a href="#ResidualConnection.forwardPropagation-5"><span class="linenos">5</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forwardPropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span> <span class="c1"># input being the input of the layer before</span>
</span><span id="ResidualConnection.forwardPropagation-6"><a href="#ResidualConnection.forwardPropagation-6"><span class="linenos">6</span></a>        <span class="k">return</span> <span class="nb">input</span> <span class="o">+</span> <span class="n">output</span> <span class="c1"># y = x + f(x)</span>
</span></pre></div>


    

                            </div>
                </section>
                <section id="Dropout">
                            <input id="Dropout-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">Dropout</span>:

                <label class="view-source-button" for="Dropout-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Dropout"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Dropout-4"><a href="#Dropout-4"><span class="linenos"> 4</span></a><span class="k">class</span><span class="w"> </span><span class="nc">Dropout</span><span class="p">:</span>
</span><span id="Dropout-5"><a href="#Dropout-5"><span class="linenos"> 5</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dropProbability</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
</span><span id="Dropout-6"><a href="#Dropout-6"><span class="linenos"> 6</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dropProbability</span> <span class="o">=</span> <span class="n">dropProbability</span>
</span><span id="Dropout-7"><a href="#Dropout-7"><span class="linenos"> 7</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mask</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="Dropout-8"><a href="#Dropout-8"><span class="linenos"> 8</span></a>
</span><span id="Dropout-9"><a href="#Dropout-9"><span class="linenos"> 9</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputTensor</span><span class="p">,</span> <span class="n">training</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="Dropout-10"><a href="#Dropout-10"><span class="linenos">10</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">training</span> <span class="o">==</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="Dropout-11"><a href="#Dropout-11"><span class="linenos">11</span></a>            <span class="k">return</span> <span class="n">inputTensor</span>
</span><span id="Dropout-12"><a href="#Dropout-12"><span class="linenos">12</span></a>        
</span><span id="Dropout-13"><a href="#Dropout-13"><span class="linenos">13</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropProbability</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
</span><span id="Dropout-14"><a href="#Dropout-14"><span class="linenos">14</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">inputTensor</span><span class="p">)</span>
</span><span id="Dropout-15"><a href="#Dropout-15"><span class="linenos">15</span></a>            <span class="k">return</span> <span class="n">inputTensor</span>
</span><span id="Dropout-16"><a href="#Dropout-16"><span class="linenos">16</span></a>        <span class="k">elif</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropProbability</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="Dropout-17"><a href="#Dropout-17"><span class="linenos">17</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">inputTensor</span><span class="p">)</span>
</span><span id="Dropout-18"><a href="#Dropout-18"><span class="linenos">18</span></a>            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">inputTensor</span><span class="p">)</span>
</span><span id="Dropout-19"><a href="#Dropout-19"><span class="linenos">19</span></a>
</span><span id="Dropout-20"><a href="#Dropout-20"><span class="linenos">20</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="o">*</span><span class="n">inputTensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropProbability</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">inputTensor</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span><span id="Dropout-21"><a href="#Dropout-21"><span class="linenos">21</span></a>        <span class="k">return</span> <span class="n">inputTensor</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropProbability</span><span class="p">)</span>
</span><span id="Dropout-22"><a href="#Dropout-22"><span class="linenos">22</span></a>
</span><span id="Dropout-23"><a href="#Dropout-23"><span class="linenos">23</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_backpropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gradient</span><span class="p">):</span>
</span><span id="Dropout-24"><a href="#Dropout-24"><span class="linenos">24</span></a>        <span class="k">return</span> <span class="n">gradient</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropProbability</span><span class="p">)</span>
</span></pre></div>


    

                            <div id="Dropout.__init__" class="classattr">
                                        <input id="Dropout.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">Dropout</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">dropProbability</span><span class="o">=</span><span class="mf">0.5</span></span>)</span>

                <label class="view-source-button" for="Dropout.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Dropout.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Dropout.__init__-5"><a href="#Dropout.__init__-5"><span class="linenos">5</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dropProbability</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
</span><span id="Dropout.__init__-6"><a href="#Dropout.__init__-6"><span class="linenos">6</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dropProbability</span> <span class="o">=</span> <span class="n">dropProbability</span>
</span><span id="Dropout.__init__-7"><a href="#Dropout.__init__-7"><span class="linenos">7</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mask</span> <span class="o">=</span> <span class="kc">None</span>
</span></pre></div>


    

                            </div>
                            <div id="Dropout.dropProbability" class="classattr">
                                <div class="attr variable">
            <span class="name">dropProbability</span>

        
    </div>
    <a class="headerlink" href="#Dropout.dropProbability"></a>
    
    

                            </div>
                            <div id="Dropout.mask" class="classattr">
                                <div class="attr variable">
            <span class="name">mask</span>

        
    </div>
    <a class="headerlink" href="#Dropout.mask"></a>
    
    

                            </div>
                            <div id="Dropout.forward" class="classattr">
                                        <input id="Dropout.forward-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">forward</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">inputTensor</span>, </span><span class="param"><span class="n">training</span><span class="o">=</span><span class="kc">True</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="Dropout.forward-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Dropout.forward"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Dropout.forward-9"><a href="#Dropout.forward-9"><span class="linenos"> 9</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputTensor</span><span class="p">,</span> <span class="n">training</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="Dropout.forward-10"><a href="#Dropout.forward-10"><span class="linenos">10</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">training</span> <span class="o">==</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="Dropout.forward-11"><a href="#Dropout.forward-11"><span class="linenos">11</span></a>            <span class="k">return</span> <span class="n">inputTensor</span>
</span><span id="Dropout.forward-12"><a href="#Dropout.forward-12"><span class="linenos">12</span></a>        
</span><span id="Dropout.forward-13"><a href="#Dropout.forward-13"><span class="linenos">13</span></a>        <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropProbability</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
</span><span id="Dropout.forward-14"><a href="#Dropout.forward-14"><span class="linenos">14</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">inputTensor</span><span class="p">)</span>
</span><span id="Dropout.forward-15"><a href="#Dropout.forward-15"><span class="linenos">15</span></a>            <span class="k">return</span> <span class="n">inputTensor</span>
</span><span id="Dropout.forward-16"><a href="#Dropout.forward-16"><span class="linenos">16</span></a>        <span class="k">elif</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropProbability</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>
</span><span id="Dropout.forward-17"><a href="#Dropout.forward-17"><span class="linenos">17</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">inputTensor</span><span class="p">)</span>
</span><span id="Dropout.forward-18"><a href="#Dropout.forward-18"><span class="linenos">18</span></a>            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">inputTensor</span><span class="p">)</span>
</span><span id="Dropout.forward-19"><a href="#Dropout.forward-19"><span class="linenos">19</span></a>
</span><span id="Dropout.forward-20"><a href="#Dropout.forward-20"><span class="linenos">20</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="o">*</span><span class="n">inputTensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropProbability</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">inputTensor</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span><span id="Dropout.forward-21"><a href="#Dropout.forward-21"><span class="linenos">21</span></a>        <span class="k">return</span> <span class="n">inputTensor</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropProbability</span><span class="p">)</span>
</span></pre></div>


    

                            </div>
                </section>
                <section id="Adam">
                            <input id="Adam-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">Adam</span>:

                <label class="view-source-button" for="Adam-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Adam"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Adam-4"><a href="#Adam-4"><span class="linenos"> 4</span></a><span class="k">class</span><span class="w"> </span><span class="nc">Adam</span><span class="p">:</span>
</span><span id="Adam-5"><a href="#Adam-5"><span class="linenos"> 5</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">forwardPropagationFunction</span><span class="p">,</span> <span class="n">backwardPropagationFunction</span><span class="p">,</span> <span class="n">giveInputsToBackprop</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="Adam-6"><a href="#Adam-6"><span class="linenos"> 6</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">firstMoment</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="Adam-7"><a href="#Adam-7"><span class="linenos"> 7</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">secondMoment</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="Adam-8"><a href="#Adam-8"><span class="linenos"> 8</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">t</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="Adam-9"><a href="#Adam-9"><span class="linenos"> 9</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">forwardPropagationFunction</span> <span class="o">=</span> <span class="n">forwardPropagationFunction</span>
</span><span id="Adam-10"><a href="#Adam-10"><span class="linenos">10</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">backwardPropagationFunction</span> <span class="o">=</span> <span class="n">backwardPropagationFunction</span>
</span><span id="Adam-11"><a href="#Adam-11"><span class="linenos">11</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">giveInputsToBackprop</span> <span class="o">=</span> <span class="n">giveInputsToBackprop</span> 
</span><span id="Adam-12"><a href="#Adam-12"><span class="linenos">12</span></a>
</span><span id="Adam-13"><a href="#Adam-13"><span class="linenos">13</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">optimiser</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">useBatches</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
</span><span id="Adam-14"><a href="#Adam-14"><span class="linenos">14</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">useBatches</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="Adam-15"><a href="#Adam-15"><span class="linenos">15</span></a>            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AdamsOptimiserWithBatches</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
</span><span id="Adam-16"><a href="#Adam-16"><span class="linenos">16</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="Adam-17"><a href="#Adam-17"><span class="linenos">17</span></a>            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AdamsOptimiserWithoutBatches</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
</span><span id="Adam-18"><a href="#Adam-18"><span class="linenos">18</span></a>
</span><span id="Adam-19"><a href="#Adam-19"><span class="linenos">19</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_AdamsOptimiserWithBatches</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">):</span>     
</span><span id="Adam-20"><a href="#Adam-20"><span class="linenos">20</span></a>        <span class="n">AllOutputs</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="Adam-21"><a href="#Adam-21"><span class="linenos">21</span></a>        <span class="n">numBatches</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputData</span><span class="p">)</span> <span class="o">//</span> <span class="n">batchSize</span>
</span><span id="Adam-22"><a href="#Adam-22"><span class="linenos">22</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">numBatches</span><span class="p">):</span>
</span><span id="Adam-23"><a href="#Adam-23"><span class="linenos">23</span></a>            <span class="n">batchData</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputData</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">batchSize</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">batchSize</span><span class="p">])</span>
</span><span id="Adam-24"><a href="#Adam-24"><span class="linenos">24</span></a>            <span class="n">batchLabels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">batchSize</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">batchSize</span><span class="p">])</span>
</span><span id="Adam-25"><a href="#Adam-25"><span class="linenos">25</span></a>            <span class="n">Parameters</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="Adam-26"><a href="#Adam-26"><span class="linenos">26</span></a>
</span><span id="Adam-27"><a href="#Adam-27"><span class="linenos">27</span></a>            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forwardPropagationFunction</span><span class="p">(</span><span class="n">batchData</span><span class="p">)</span>
</span><span id="Adam-28"><a href="#Adam-28"><span class="linenos">28</span></a>            <span class="n">AllOutputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</span><span id="Adam-29"><a href="#Adam-29"><span class="linenos">29</span></a>
</span><span id="Adam-30"><a href="#Adam-30"><span class="linenos">30</span></a>            <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">giveInputsToBackprop</span> <span class="o">==</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="Adam-31"><a href="#Adam-31"><span class="linenos">31</span></a>                <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backwardPropagationFunction</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">batchLabels</span><span class="p">)</span>
</span><span id="Adam-32"><a href="#Adam-32"><span class="linenos">32</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="Adam-33"><a href="#Adam-33"><span class="linenos">33</span></a>                <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backwardPropagationFunction</span><span class="p">(</span><span class="n">batchData</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">batchLabels</span><span class="p">)</span>
</span><span id="Adam-34"><a href="#Adam-34"><span class="linenos">34</span></a>
</span><span id="Adam-35"><a href="#Adam-35"><span class="linenos">35</span></a>            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">Gradients</span><span class="p">:</span>
</span><span id="Adam-36"><a href="#Adam-36"><span class="linenos">36</span></a>                <span class="k">if</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="nb">list</span><span class="p">)):</span> <span class="c1"># inhomengous array</span>
</span><span id="Adam-37"><a href="#Adam-37"><span class="linenos">37</span></a>                    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">])):</span>
</span><span id="Adam-38"><a href="#Adam-38"><span class="linenos">38</span></a>                        <span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">j</span><span class="p">])</span> <span class="o">/</span> <span class="n">batchSize</span>
</span><span id="Adam-39"><a href="#Adam-39"><span class="linenos">39</span></a>                <span class="k">else</span><span class="p">:</span>
</span><span id="Adam-40"><a href="#Adam-40"><span class="linenos">40</span></a>                    <span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">/</span> <span class="n">batchSize</span>
</span><span id="Adam-41"><a href="#Adam-41"><span class="linenos">41</span></a>
</span><span id="Adam-42"><a href="#Adam-42"><span class="linenos">42</span></a>            <span class="n">Parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Adams</span><span class="p">(</span><span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
</span><span id="Adam-43"><a href="#Adam-43"><span class="linenos">43</span></a>        <span class="k">return</span> <span class="n">AllOutputs</span><span class="p">,</span> <span class="n">Parameters</span>
</span><span id="Adam-44"><a href="#Adam-44"><span class="linenos">44</span></a>
</span><span id="Adam-45"><a href="#Adam-45"><span class="linenos">45</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_AdamsOptimiserWithoutBatches</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">):</span>   
</span><span id="Adam-46"><a href="#Adam-46"><span class="linenos">46</span></a>        <span class="n">AllOutputs</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="Adam-47"><a href="#Adam-47"><span class="linenos">47</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputData</span><span class="p">)):</span>
</span><span id="Adam-48"><a href="#Adam-48"><span class="linenos">48</span></a>            <span class="nb">input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">inputData</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span>
</span><span id="Adam-49"><a href="#Adam-49"><span class="linenos">49</span></a>            <span class="n">lab</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span>
</span><span id="Adam-50"><a href="#Adam-50"><span class="linenos">50</span></a>            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forwardPropagationFunction</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="Adam-51"><a href="#Adam-51"><span class="linenos">51</span></a>            <span class="n">AllOutputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</span><span id="Adam-52"><a href="#Adam-52"><span class="linenos">52</span></a>
</span><span id="Adam-53"><a href="#Adam-53"><span class="linenos">53</span></a>            <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">giveInputsToBackprop</span> <span class="o">==</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="Adam-54"><a href="#Adam-54"><span class="linenos">54</span></a>                <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backwardPropagationFunction</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">lab</span><span class="p">)</span>
</span><span id="Adam-55"><a href="#Adam-55"><span class="linenos">55</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="Adam-56"><a href="#Adam-56"><span class="linenos">56</span></a>                <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backwardPropagationFunction</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">lab</span><span class="p">)</span>
</span><span id="Adam-57"><a href="#Adam-57"><span class="linenos">57</span></a>
</span><span id="Adam-58"><a href="#Adam-58"><span class="linenos">58</span></a>            <span class="n">Parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_Adams</span><span class="p">(</span><span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
</span><span id="Adam-59"><a href="#Adam-59"><span class="linenos">59</span></a>        <span class="k">return</span> <span class="n">AllOutputs</span><span class="p">,</span> <span class="n">Parameters</span>
</span><span id="Adam-60"><a href="#Adam-60"><span class="linenos">60</span></a>
</span><span id="Adam-61"><a href="#Adam-61"><span class="linenos">61</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_Adams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">):</span>
</span><span id="Adam-62"><a href="#Adam-62"><span class="linenos">62</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">t</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span id="Adam-63"><a href="#Adam-63"><span class="linenos">63</span></a>        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">Gradients</span><span class="p">:</span>
</span><span id="Adam-64"><a href="#Adam-64"><span class="linenos">64</span></a>            <span class="k">if</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="nb">list</span><span class="p">)):</span> <span class="c1"># if the Gradient is a inhomengous list (jagged array, which numpy doesnt like)</span>
</span><span id="Adam-65"><a href="#Adam-65"><span class="linenos">65</span></a>                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">grad</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]):</span>
</span><span id="Adam-66"><a href="#Adam-66"><span class="linenos">66</span></a>                    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
</span><span id="Adam-67"><a href="#Adam-67"><span class="linenos">67</span></a>
</span><span id="Adam-68"><a href="#Adam-68"><span class="linenos">68</span></a>                    <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">firstMoment</span><span class="p">:</span>
</span><span id="Adam-69"><a href="#Adam-69"><span class="linenos">69</span></a>                        <span class="bp">self</span><span class="o">.</span><span class="n">firstMoment</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">g</span><span class="p">)</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]]</span>
</span><span id="Adam-70"><a href="#Adam-70"><span class="linenos">70</span></a>                        <span class="bp">self</span><span class="o">.</span><span class="n">secondMoment</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">g</span><span class="p">)</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]]</span>
</span><span id="Adam-71"><a href="#Adam-71"><span class="linenos">71</span></a>                        
</span><span id="Adam-72"><a href="#Adam-72"><span class="linenos">72</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">firstMoment</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">beta1</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">firstMoment</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span> <span class="o">*</span> <span class="n">grad</span>
</span><span id="Adam-73"><a href="#Adam-73"><span class="linenos">73</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">secondMoment</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">beta2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">secondMoment</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">grad</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="Adam-74"><a href="#Adam-74"><span class="linenos">74</span></a>
</span><span id="Adam-75"><a href="#Adam-75"><span class="linenos">75</span></a>                    <span class="n">firstMomentWeightHat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">firstMoment</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="p">)</span>
</span><span id="Adam-76"><a href="#Adam-76"><span class="linenos">76</span></a>                    <span class="n">secondMomentWeightHat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">secondMoment</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="p">)</span>
</span><span id="Adam-77"><a href="#Adam-77"><span class="linenos">77</span></a>
</span><span id="Adam-78"><a href="#Adam-78"><span class="linenos">78</span></a>                    <span class="n">Parameters</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">firstMomentWeightHat</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">secondMomentWeightHat</span><span class="p">)</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>
</span><span id="Adam-79"><a href="#Adam-79"><span class="linenos">79</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="Adam-80"><a href="#Adam-80"><span class="linenos">80</span></a>                <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">firstMoment</span><span class="p">:</span>
</span><span id="Adam-81"><a href="#Adam-81"><span class="linenos">81</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">firstMoment</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
</span><span id="Adam-82"><a href="#Adam-82"><span class="linenos">82</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">secondMoment</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
</span><span id="Adam-83"><a href="#Adam-83"><span class="linenos">83</span></a>                        
</span><span id="Adam-84"><a href="#Adam-84"><span class="linenos">84</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">firstMoment</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">beta1</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">firstMoment</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span> <span class="o">*</span> <span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
</span><span id="Adam-85"><a href="#Adam-85"><span class="linenos">85</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">secondMoment</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">beta2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">secondMoment</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="Adam-86"><a href="#Adam-86"><span class="linenos">86</span></a>
</span><span id="Adam-87"><a href="#Adam-87"><span class="linenos">87</span></a>                <span class="n">firstMomentWeightHat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">firstMoment</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="p">)</span>
</span><span id="Adam-88"><a href="#Adam-88"><span class="linenos">88</span></a>                <span class="n">secondMomentWeightHat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">secondMoment</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="p">)</span>
</span><span id="Adam-89"><a href="#Adam-89"><span class="linenos">89</span></a>
</span><span id="Adam-90"><a href="#Adam-90"><span class="linenos">90</span></a>                <span class="n">Parameters</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">firstMomentWeightHat</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">secondMomentWeightHat</span><span class="p">)</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>
</span><span id="Adam-91"><a href="#Adam-91"><span class="linenos">91</span></a>        <span class="k">return</span> <span class="n">Parameters</span>
</span></pre></div>


    

                            <div id="Adam.__init__" class="classattr">
                                        <input id="Adam.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">Adam</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">forwardPropagationFunction</span>,</span><span class="param">	<span class="n">backwardPropagationFunction</span>,</span><span class="param">	<span class="n">giveInputsToBackprop</span><span class="o">=</span><span class="kc">False</span></span>)</span>

                <label class="view-source-button" for="Adam.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Adam.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Adam.__init__-5"><a href="#Adam.__init__-5"><span class="linenos"> 5</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">forwardPropagationFunction</span><span class="p">,</span> <span class="n">backwardPropagationFunction</span><span class="p">,</span> <span class="n">giveInputsToBackprop</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="Adam.__init__-6"><a href="#Adam.__init__-6"><span class="linenos"> 6</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">firstMoment</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="Adam.__init__-7"><a href="#Adam.__init__-7"><span class="linenos"> 7</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">secondMoment</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="Adam.__init__-8"><a href="#Adam.__init__-8"><span class="linenos"> 8</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">t</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="Adam.__init__-9"><a href="#Adam.__init__-9"><span class="linenos"> 9</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">forwardPropagationFunction</span> <span class="o">=</span> <span class="n">forwardPropagationFunction</span>
</span><span id="Adam.__init__-10"><a href="#Adam.__init__-10"><span class="linenos">10</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">backwardPropagationFunction</span> <span class="o">=</span> <span class="n">backwardPropagationFunction</span>
</span><span id="Adam.__init__-11"><a href="#Adam.__init__-11"><span class="linenos">11</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">giveInputsToBackprop</span> <span class="o">=</span> <span class="n">giveInputsToBackprop</span> 
</span></pre></div>


    

                            </div>
                            <div id="Adam.firstMoment" class="classattr">
                                <div class="attr variable">
            <span class="name">firstMoment</span>

        
    </div>
    <a class="headerlink" href="#Adam.firstMoment"></a>
    
    

                            </div>
                            <div id="Adam.secondMoment" class="classattr">
                                <div class="attr variable">
            <span class="name">secondMoment</span>

        
    </div>
    <a class="headerlink" href="#Adam.secondMoment"></a>
    
    

                            </div>
                            <div id="Adam.t" class="classattr">
                                <div class="attr variable">
            <span class="name">t</span>

        
    </div>
    <a class="headerlink" href="#Adam.t"></a>
    
    

                            </div>
                            <div id="Adam.forwardPropagationFunction" class="classattr">
                                <div class="attr variable">
            <span class="name">forwardPropagationFunction</span>

        
    </div>
    <a class="headerlink" href="#Adam.forwardPropagationFunction"></a>
    
    

                            </div>
                            <div id="Adam.backwardPropagationFunction" class="classattr">
                                <div class="attr variable">
            <span class="name">backwardPropagationFunction</span>

        
    </div>
    <a class="headerlink" href="#Adam.backwardPropagationFunction"></a>
    
    

                            </div>
                            <div id="Adam.giveInputsToBackprop" class="classattr">
                                <div class="attr variable">
            <span class="name">giveInputsToBackprop</span>

        
    </div>
    <a class="headerlink" href="#Adam.giveInputsToBackprop"></a>
    
    

                            </div>
                            <div id="Adam.optimiser" class="classattr">
                                        <input id="Adam.optimiser-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">optimiser</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">inputData</span>,</span><span class="param">	<span class="n">labels</span>,</span><span class="param">	<span class="n">useBatches</span>,</span><span class="param">	<span class="n">batchSize</span>,</span><span class="param">	<span class="n">alpha</span>,</span><span class="param">	<span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span>,</span><span class="param">	<span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span>,</span><span class="param">	<span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-08</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="Adam.optimiser-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Adam.optimiser"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Adam.optimiser-13"><a href="#Adam.optimiser-13"><span class="linenos">13</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">optimiser</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">useBatches</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
</span><span id="Adam.optimiser-14"><a href="#Adam.optimiser-14"><span class="linenos">14</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">useBatches</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="Adam.optimiser-15"><a href="#Adam.optimiser-15"><span class="linenos">15</span></a>            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AdamsOptimiserWithBatches</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
</span><span id="Adam.optimiser-16"><a href="#Adam.optimiser-16"><span class="linenos">16</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="Adam.optimiser-17"><a href="#Adam.optimiser-17"><span class="linenos">17</span></a>            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AdamsOptimiserWithoutBatches</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
</span></pre></div>


    

                            </div>
                </section>
                <section id="AdamW">
                            <input id="AdamW-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">AdamW</span>:

                <label class="view-source-button" for="AdamW-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#AdamW"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="AdamW-4"><a href="#AdamW-4"><span class="linenos"> 4</span></a><span class="k">class</span><span class="w"> </span><span class="nc">AdamW</span><span class="p">:</span>
</span><span id="AdamW-5"><a href="#AdamW-5"><span class="linenos"> 5</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">forwardPropagationFunction</span><span class="p">,</span> <span class="n">backwardPropagationFunction</span><span class="p">,</span> <span class="n">giveInputsToBackprop</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">weightDecay</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">):</span>
</span><span id="AdamW-6"><a href="#AdamW-6"><span class="linenos"> 6</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">firstMoment</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="AdamW-7"><a href="#AdamW-7"><span class="linenos"> 7</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">secondMoment</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="AdamW-8"><a href="#AdamW-8"><span class="linenos"> 8</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">t</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="AdamW-9"><a href="#AdamW-9"><span class="linenos"> 9</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">forwardPropagationFunction</span> <span class="o">=</span> <span class="n">forwardPropagationFunction</span>
</span><span id="AdamW-10"><a href="#AdamW-10"><span class="linenos">10</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">backwardPropagationFunction</span> <span class="o">=</span> <span class="n">backwardPropagationFunction</span>
</span><span id="AdamW-11"><a href="#AdamW-11"><span class="linenos">11</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">giveInputsToBackprop</span> <span class="o">=</span> <span class="n">giveInputsToBackprop</span> 
</span><span id="AdamW-12"><a href="#AdamW-12"><span class="linenos">12</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weightDecay</span> <span class="o">=</span> <span class="n">weightDecay</span>
</span><span id="AdamW-13"><a href="#AdamW-13"><span class="linenos">13</span></a>
</span><span id="AdamW-14"><a href="#AdamW-14"><span class="linenos">14</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">optimiser</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">useBatches</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
</span><span id="AdamW-15"><a href="#AdamW-15"><span class="linenos">15</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">useBatches</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="AdamW-16"><a href="#AdamW-16"><span class="linenos">16</span></a>            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AdamWOptimiserWithBatches</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
</span><span id="AdamW-17"><a href="#AdamW-17"><span class="linenos">17</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="AdamW-18"><a href="#AdamW-18"><span class="linenos">18</span></a>            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AdamWOptimiserWithoutBatches</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
</span><span id="AdamW-19"><a href="#AdamW-19"><span class="linenos">19</span></a>
</span><span id="AdamW-20"><a href="#AdamW-20"><span class="linenos">20</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_AdamWOptimiserWithBatches</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">):</span>     
</span><span id="AdamW-21"><a href="#AdamW-21"><span class="linenos">21</span></a>        <span class="n">AllOutputs</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="AdamW-22"><a href="#AdamW-22"><span class="linenos">22</span></a>        <span class="n">numBatches</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputData</span><span class="p">)</span> <span class="o">//</span> <span class="n">batchSize</span>
</span><span id="AdamW-23"><a href="#AdamW-23"><span class="linenos">23</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">numBatches</span><span class="p">):</span>
</span><span id="AdamW-24"><a href="#AdamW-24"><span class="linenos">24</span></a>            <span class="n">batchData</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputData</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">batchSize</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">batchSize</span><span class="p">])</span>
</span><span id="AdamW-25"><a href="#AdamW-25"><span class="linenos">25</span></a>            <span class="n">batchLabels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">batchSize</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">batchSize</span><span class="p">])</span>
</span><span id="AdamW-26"><a href="#AdamW-26"><span class="linenos">26</span></a>            <span class="n">Parameters</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="AdamW-27"><a href="#AdamW-27"><span class="linenos">27</span></a>
</span><span id="AdamW-28"><a href="#AdamW-28"><span class="linenos">28</span></a>            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forwardPropagationFunction</span><span class="p">(</span><span class="n">batchData</span><span class="p">)</span>
</span><span id="AdamW-29"><a href="#AdamW-29"><span class="linenos">29</span></a>            <span class="n">AllOutputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</span><span id="AdamW-30"><a href="#AdamW-30"><span class="linenos">30</span></a>
</span><span id="AdamW-31"><a href="#AdamW-31"><span class="linenos">31</span></a>            <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">giveInputsToBackprop</span> <span class="o">==</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="AdamW-32"><a href="#AdamW-32"><span class="linenos">32</span></a>                <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backwardPropagationFunction</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">batchLabels</span><span class="p">)</span>
</span><span id="AdamW-33"><a href="#AdamW-33"><span class="linenos">33</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="AdamW-34"><a href="#AdamW-34"><span class="linenos">34</span></a>                <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backwardPropagationFunction</span><span class="p">(</span><span class="n">batchData</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">batchLabels</span><span class="p">)</span>
</span><span id="AdamW-35"><a href="#AdamW-35"><span class="linenos">35</span></a>
</span><span id="AdamW-36"><a href="#AdamW-36"><span class="linenos">36</span></a>            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">Gradients</span><span class="p">:</span>
</span><span id="AdamW-37"><a href="#AdamW-37"><span class="linenos">37</span></a>                <span class="k">if</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="nb">list</span><span class="p">)):</span> <span class="c1"># inhomengous array</span>
</span><span id="AdamW-38"><a href="#AdamW-38"><span class="linenos">38</span></a>                    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">])):</span>
</span><span id="AdamW-39"><a href="#AdamW-39"><span class="linenos">39</span></a>                        <span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">j</span><span class="p">])</span> <span class="o">/</span> <span class="n">batchSize</span>
</span><span id="AdamW-40"><a href="#AdamW-40"><span class="linenos">40</span></a>                <span class="k">else</span><span class="p">:</span>
</span><span id="AdamW-41"><a href="#AdamW-41"><span class="linenos">41</span></a>                    <span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">/</span> <span class="n">batchSize</span>
</span><span id="AdamW-42"><a href="#AdamW-42"><span class="linenos">42</span></a>
</span><span id="AdamW-43"><a href="#AdamW-43"><span class="linenos">43</span></a>            <span class="n">Parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AdamW</span><span class="p">(</span><span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
</span><span id="AdamW-44"><a href="#AdamW-44"><span class="linenos">44</span></a>        <span class="k">return</span> <span class="n">AllOutputs</span><span class="p">,</span> <span class="n">Parameters</span>
</span><span id="AdamW-45"><a href="#AdamW-45"><span class="linenos">45</span></a>
</span><span id="AdamW-46"><a href="#AdamW-46"><span class="linenos">46</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_AdamWOptimiserWithoutBatches</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">):</span>   
</span><span id="AdamW-47"><a href="#AdamW-47"><span class="linenos">47</span></a>        <span class="n">AllOutputs</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="AdamW-48"><a href="#AdamW-48"><span class="linenos">48</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputData</span><span class="p">)):</span>
</span><span id="AdamW-49"><a href="#AdamW-49"><span class="linenos">49</span></a>            <span class="nb">input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">inputData</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span>
</span><span id="AdamW-50"><a href="#AdamW-50"><span class="linenos">50</span></a>            <span class="n">lab</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span>
</span><span id="AdamW-51"><a href="#AdamW-51"><span class="linenos">51</span></a>            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forwardPropagationFunction</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="AdamW-52"><a href="#AdamW-52"><span class="linenos">52</span></a>            <span class="n">AllOutputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</span><span id="AdamW-53"><a href="#AdamW-53"><span class="linenos">53</span></a>
</span><span id="AdamW-54"><a href="#AdamW-54"><span class="linenos">54</span></a>            <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">giveInputsToBackprop</span> <span class="o">==</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="AdamW-55"><a href="#AdamW-55"><span class="linenos">55</span></a>                <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backwardPropagationFunction</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">lab</span><span class="p">)</span>
</span><span id="AdamW-56"><a href="#AdamW-56"><span class="linenos">56</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="AdamW-57"><a href="#AdamW-57"><span class="linenos">57</span></a>                <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backwardPropagationFunction</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">lab</span><span class="p">)</span>
</span><span id="AdamW-58"><a href="#AdamW-58"><span class="linenos">58</span></a>
</span><span id="AdamW-59"><a href="#AdamW-59"><span class="linenos">59</span></a>            <span class="n">Parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AdamW</span><span class="p">(</span><span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
</span><span id="AdamW-60"><a href="#AdamW-60"><span class="linenos">60</span></a>        <span class="k">return</span> <span class="n">AllOutputs</span><span class="p">,</span> <span class="n">Parameters</span>
</span><span id="AdamW-61"><a href="#AdamW-61"><span class="linenos">61</span></a>
</span><span id="AdamW-62"><a href="#AdamW-62"><span class="linenos">62</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_AdamW</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">):</span>
</span><span id="AdamW-63"><a href="#AdamW-63"><span class="linenos">63</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">t</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span id="AdamW-64"><a href="#AdamW-64"><span class="linenos">64</span></a>        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">Gradients</span><span class="p">:</span>
</span><span id="AdamW-65"><a href="#AdamW-65"><span class="linenos">65</span></a>            <span class="k">if</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="nb">list</span><span class="p">)):</span> <span class="c1"># if the Gradient is a inhomengous list (jagged array, which numpy doesnt like)</span>
</span><span id="AdamW-66"><a href="#AdamW-66"><span class="linenos">66</span></a>                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">grad</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]):</span>
</span><span id="AdamW-67"><a href="#AdamW-67"><span class="linenos">67</span></a>                    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
</span><span id="AdamW-68"><a href="#AdamW-68"><span class="linenos">68</span></a>
</span><span id="AdamW-69"><a href="#AdamW-69"><span class="linenos">69</span></a>                    <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">firstMoment</span><span class="p">:</span>
</span><span id="AdamW-70"><a href="#AdamW-70"><span class="linenos">70</span></a>                        <span class="bp">self</span><span class="o">.</span><span class="n">firstMoment</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">g</span><span class="p">)</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]]</span>
</span><span id="AdamW-71"><a href="#AdamW-71"><span class="linenos">71</span></a>                        <span class="bp">self</span><span class="o">.</span><span class="n">secondMoment</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">g</span><span class="p">)</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]]</span>
</span><span id="AdamW-72"><a href="#AdamW-72"><span class="linenos">72</span></a>                        
</span><span id="AdamW-73"><a href="#AdamW-73"><span class="linenos">73</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">firstMoment</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">beta1</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">firstMoment</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span> <span class="o">*</span> <span class="n">grad</span>
</span><span id="AdamW-74"><a href="#AdamW-74"><span class="linenos">74</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">secondMoment</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">beta2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">secondMoment</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">grad</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="AdamW-75"><a href="#AdamW-75"><span class="linenos">75</span></a>
</span><span id="AdamW-76"><a href="#AdamW-76"><span class="linenos">76</span></a>                    <span class="n">firstMomentWeightHat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">firstMoment</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="p">)</span>
</span><span id="AdamW-77"><a href="#AdamW-77"><span class="linenos">77</span></a>                    <span class="n">secondMomentWeightHat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">secondMoment</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="p">)</span>
</span><span id="AdamW-78"><a href="#AdamW-78"><span class="linenos">78</span></a>
</span><span id="AdamW-79"><a href="#AdamW-79"><span class="linenos">79</span></a>                    <span class="n">Parameters</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">firstMomentWeightHat</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">secondMomentWeightHat</span><span class="p">)</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>
</span><span id="AdamW-80"><a href="#AdamW-80"><span class="linenos">80</span></a>
</span><span id="AdamW-81"><a href="#AdamW-81"><span class="linenos">81</span></a>                    <span class="n">Parameters</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">weightDecay</span> <span class="o">*</span> <span class="n">Parameters</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
</span><span id="AdamW-82"><a href="#AdamW-82"><span class="linenos">82</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="AdamW-83"><a href="#AdamW-83"><span class="linenos">83</span></a>                <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">firstMoment</span><span class="p">:</span>
</span><span id="AdamW-84"><a href="#AdamW-84"><span class="linenos">84</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">firstMoment</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
</span><span id="AdamW-85"><a href="#AdamW-85"><span class="linenos">85</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">secondMoment</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
</span><span id="AdamW-86"><a href="#AdamW-86"><span class="linenos">86</span></a>                        
</span><span id="AdamW-87"><a href="#AdamW-87"><span class="linenos">87</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">firstMoment</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">beta1</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">firstMoment</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span> <span class="o">*</span> <span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
</span><span id="AdamW-88"><a href="#AdamW-88"><span class="linenos">88</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">secondMoment</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">beta2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">secondMoment</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="AdamW-89"><a href="#AdamW-89"><span class="linenos">89</span></a>
</span><span id="AdamW-90"><a href="#AdamW-90"><span class="linenos">90</span></a>                <span class="n">firstMomentWeightHat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">firstMoment</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="p">)</span>
</span><span id="AdamW-91"><a href="#AdamW-91"><span class="linenos">91</span></a>                <span class="n">secondMomentWeightHat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">secondMoment</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="p">)</span>
</span><span id="AdamW-92"><a href="#AdamW-92"><span class="linenos">92</span></a>
</span><span id="AdamW-93"><a href="#AdamW-93"><span class="linenos">93</span></a>                <span class="n">Parameters</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">firstMomentWeightHat</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">secondMomentWeightHat</span><span class="p">)</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>
</span><span id="AdamW-94"><a href="#AdamW-94"><span class="linenos">94</span></a>
</span><span id="AdamW-95"><a href="#AdamW-95"><span class="linenos">95</span></a>                <span class="n">Parameters</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">weightDecay</span> <span class="o">*</span> <span class="n">Parameters</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
</span><span id="AdamW-96"><a href="#AdamW-96"><span class="linenos">96</span></a>        <span class="k">return</span> <span class="n">Parameters</span>
</span></pre></div>


    

                            <div id="AdamW.__init__" class="classattr">
                                        <input id="AdamW.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">AdamW</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">forwardPropagationFunction</span>,</span><span class="param">	<span class="n">backwardPropagationFunction</span>,</span><span class="param">	<span class="n">giveInputsToBackprop</span><span class="o">=</span><span class="kc">False</span>,</span><span class="param">	<span class="n">weightDecay</span><span class="o">=</span><span class="mf">0.01</span></span>)</span>

                <label class="view-source-button" for="AdamW.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#AdamW.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="AdamW.__init__-5"><a href="#AdamW.__init__-5"><span class="linenos"> 5</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">forwardPropagationFunction</span><span class="p">,</span> <span class="n">backwardPropagationFunction</span><span class="p">,</span> <span class="n">giveInputsToBackprop</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">weightDecay</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">):</span>
</span><span id="AdamW.__init__-6"><a href="#AdamW.__init__-6"><span class="linenos"> 6</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">firstMoment</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="AdamW.__init__-7"><a href="#AdamW.__init__-7"><span class="linenos"> 7</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">secondMoment</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="AdamW.__init__-8"><a href="#AdamW.__init__-8"><span class="linenos"> 8</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">t</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="AdamW.__init__-9"><a href="#AdamW.__init__-9"><span class="linenos"> 9</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">forwardPropagationFunction</span> <span class="o">=</span> <span class="n">forwardPropagationFunction</span>
</span><span id="AdamW.__init__-10"><a href="#AdamW.__init__-10"><span class="linenos">10</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">backwardPropagationFunction</span> <span class="o">=</span> <span class="n">backwardPropagationFunction</span>
</span><span id="AdamW.__init__-11"><a href="#AdamW.__init__-11"><span class="linenos">11</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">giveInputsToBackprop</span> <span class="o">=</span> <span class="n">giveInputsToBackprop</span> 
</span><span id="AdamW.__init__-12"><a href="#AdamW.__init__-12"><span class="linenos">12</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weightDecay</span> <span class="o">=</span> <span class="n">weightDecay</span>
</span></pre></div>


    

                            </div>
                            <div id="AdamW.firstMoment" class="classattr">
                                <div class="attr variable">
            <span class="name">firstMoment</span>

        
    </div>
    <a class="headerlink" href="#AdamW.firstMoment"></a>
    
    

                            </div>
                            <div id="AdamW.secondMoment" class="classattr">
                                <div class="attr variable">
            <span class="name">secondMoment</span>

        
    </div>
    <a class="headerlink" href="#AdamW.secondMoment"></a>
    
    

                            </div>
                            <div id="AdamW.t" class="classattr">
                                <div class="attr variable">
            <span class="name">t</span>

        
    </div>
    <a class="headerlink" href="#AdamW.t"></a>
    
    

                            </div>
                            <div id="AdamW.forwardPropagationFunction" class="classattr">
                                <div class="attr variable">
            <span class="name">forwardPropagationFunction</span>

        
    </div>
    <a class="headerlink" href="#AdamW.forwardPropagationFunction"></a>
    
    

                            </div>
                            <div id="AdamW.backwardPropagationFunction" class="classattr">
                                <div class="attr variable">
            <span class="name">backwardPropagationFunction</span>

        
    </div>
    <a class="headerlink" href="#AdamW.backwardPropagationFunction"></a>
    
    

                            </div>
                            <div id="AdamW.giveInputsToBackprop" class="classattr">
                                <div class="attr variable">
            <span class="name">giveInputsToBackprop</span>

        
    </div>
    <a class="headerlink" href="#AdamW.giveInputsToBackprop"></a>
    
    

                            </div>
                            <div id="AdamW.weightDecay" class="classattr">
                                <div class="attr variable">
            <span class="name">weightDecay</span>

        
    </div>
    <a class="headerlink" href="#AdamW.weightDecay"></a>
    
    

                            </div>
                            <div id="AdamW.optimiser" class="classattr">
                                        <input id="AdamW.optimiser-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">optimiser</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="bp">self</span>,</span><span class="param">	<span class="n">inputData</span>,</span><span class="param">	<span class="n">labels</span>,</span><span class="param">	<span class="n">useBatches</span>,</span><span class="param">	<span class="n">batchSize</span>,</span><span class="param">	<span class="n">alpha</span>,</span><span class="param">	<span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span>,</span><span class="param">	<span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span>,</span><span class="param">	<span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-08</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="AdamW.optimiser-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#AdamW.optimiser"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="AdamW.optimiser-14"><a href="#AdamW.optimiser-14"><span class="linenos">14</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">optimiser</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">useBatches</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
</span><span id="AdamW.optimiser-15"><a href="#AdamW.optimiser-15"><span class="linenos">15</span></a>        <span class="k">if</span><span class="p">(</span><span class="n">useBatches</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
</span><span id="AdamW.optimiser-16"><a href="#AdamW.optimiser-16"><span class="linenos">16</span></a>            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AdamWOptimiserWithBatches</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
</span><span id="AdamW.optimiser-17"><a href="#AdamW.optimiser-17"><span class="linenos">17</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="AdamW.optimiser-18"><a href="#AdamW.optimiser-18"><span class="linenos">18</span></a>            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_AdamWOptimiserWithoutBatches</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
</span></pre></div>


    

                            </div>
                </section>
                <section id="SGD">
                            <input id="SGD-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">SGD</span>:

                <label class="view-source-button" for="SGD-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#SGD"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="SGD-4"><a href="#SGD-4"><span class="linenos"> 4</span></a><span class="k">class</span><span class="w"> </span><span class="nc">SGD</span><span class="p">:</span>
</span><span id="SGD-5"><a href="#SGD-5"><span class="linenos"> 5</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">forwardPropagationFunction</span><span class="p">,</span> <span class="n">backwardPropagationFunction</span><span class="p">,</span> <span class="n">giveInputsToBackprop</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="SGD-6"><a href="#SGD-6"><span class="linenos"> 6</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">forwardPropagationFunction</span> <span class="o">=</span> <span class="n">forwardPropagationFunction</span>
</span><span id="SGD-7"><a href="#SGD-7"><span class="linenos"> 7</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">backwardPropagationFunction</span> <span class="o">=</span> <span class="n">backwardPropagationFunction</span>
</span><span id="SGD-8"><a href="#SGD-8"><span class="linenos"> 8</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">giveInputsToBackprop</span> <span class="o">=</span> <span class="n">giveInputsToBackprop</span> 
</span><span id="SGD-9"><a href="#SGD-9"><span class="linenos"> 9</span></a>
</span><span id="SGD-10"><a href="#SGD-10"><span class="linenos">10</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">optimiser</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">useBatches</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">learningRate</span><span class="p">):</span>
</span><span id="SGD-11"><a href="#SGD-11"><span class="linenos">11</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainStochasticGradientDescent_WithoutBatches</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">learningRate</span><span class="p">)</span>
</span><span id="SGD-12"><a href="#SGD-12"><span class="linenos">12</span></a>
</span><span id="SGD-13"><a href="#SGD-13"><span class="linenos">13</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_trainStochasticGradientDescent_WithoutBatches</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">learningRate</span><span class="p">):</span>
</span><span id="SGD-14"><a href="#SGD-14"><span class="linenos">14</span></a>        <span class="n">allNodes</span> <span class="o">=</span> <span class="p">[]</span>   
</span><span id="SGD-15"><a href="#SGD-15"><span class="linenos">15</span></a>        <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputData</span><span class="p">)):</span>
</span><span id="SGD-16"><a href="#SGD-16"><span class="linenos">16</span></a>            <span class="n">batchData</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">inputData</span><span class="p">[</span><span class="n">data</span><span class="p">]])</span>
</span><span id="SGD-17"><a href="#SGD-17"><span class="linenos">17</span></a>            <span class="n">batchLabel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">labels</span><span class="p">[</span><span class="n">data</span><span class="p">]])</span>
</span><span id="SGD-18"><a href="#SGD-18"><span class="linenos">18</span></a>
</span><span id="SGD-19"><a href="#SGD-19"><span class="linenos">19</span></a>            <span class="n">layerNodes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forwardPropagationFunction</span><span class="p">(</span><span class="n">batchData</span><span class="p">)</span>
</span><span id="SGD-20"><a href="#SGD-20"><span class="linenos">20</span></a>            <span class="n">allNodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layerNodes</span><span class="p">)</span>
</span><span id="SGD-21"><a href="#SGD-21"><span class="linenos">21</span></a>
</span><span id="SGD-22"><a href="#SGD-22"><span class="linenos">22</span></a>            <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">giveInputsToBackprop</span> <span class="o">==</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="SGD-23"><a href="#SGD-23"><span class="linenos">23</span></a>                <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backwardPropagationFunction</span><span class="p">(</span><span class="n">layerNodes</span><span class="p">,</span> <span class="n">batchLabel</span><span class="p">)</span>
</span><span id="SGD-24"><a href="#SGD-24"><span class="linenos">24</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="SGD-25"><a href="#SGD-25"><span class="linenos">25</span></a>                <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backwardPropagationFunction</span><span class="p">(</span><span class="n">batchData</span><span class="p">,</span> <span class="n">layerNodes</span><span class="p">,</span> <span class="n">batchLabel</span><span class="p">)</span>
</span><span id="SGD-26"><a href="#SGD-26"><span class="linenos">26</span></a>
</span><span id="SGD-27"><a href="#SGD-27"><span class="linenos">27</span></a>            <span class="n">Parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_updateWeightsBiases</span><span class="p">(</span><span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span><span class="p">,</span> <span class="n">learningRate</span><span class="p">)</span>
</span><span id="SGD-28"><a href="#SGD-28"><span class="linenos">28</span></a>        <span class="k">return</span> <span class="n">allNodes</span><span class="p">,</span> <span class="n">Parameters</span>
</span><span id="SGD-29"><a href="#SGD-29"><span class="linenos">29</span></a>    
</span><span id="SGD-30"><a href="#SGD-30"><span class="linenos">30</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_updateWeightsBiases</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span><span class="p">,</span> <span class="n">learningRate</span><span class="p">):</span>
</span><span id="SGD-31"><a href="#SGD-31"><span class="linenos">31</span></a>        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">Gradients</span><span class="p">:</span>
</span><span id="SGD-32"><a href="#SGD-32"><span class="linenos">32</span></a>            <span class="k">if</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="nb">list</span><span class="p">)):</span> <span class="c1"># if the Gradient is a inhomengous list (jagged array, which numpy doesnt like)</span>
</span><span id="SGD-33"><a href="#SGD-33"><span class="linenos">33</span></a>                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">])):</span>
</span><span id="SGD-34"><a href="#SGD-34"><span class="linenos">34</span></a>                    <span class="n">Parameters</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="n">learningRate</span> <span class="o">*</span> <span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
</span><span id="SGD-35"><a href="#SGD-35"><span class="linenos">35</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="SGD-36"><a href="#SGD-36"><span class="linenos">36</span></a>                <span class="n">Parameters</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">-=</span> <span class="n">learningRate</span> <span class="o">*</span> <span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
</span><span id="SGD-37"><a href="#SGD-37"><span class="linenos">37</span></a>
</span><span id="SGD-38"><a href="#SGD-38"><span class="linenos">38</span></a>        <span class="k">return</span> <span class="n">Parameters</span>
</span></pre></div>


    

                            <div id="SGD.__init__" class="classattr">
                                        <input id="SGD.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">SGD</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">forwardPropagationFunction</span>,</span><span class="param">	<span class="n">backwardPropagationFunction</span>,</span><span class="param">	<span class="n">giveInputsToBackprop</span><span class="o">=</span><span class="kc">False</span></span>)</span>

                <label class="view-source-button" for="SGD.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#SGD.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="SGD.__init__-5"><a href="#SGD.__init__-5"><span class="linenos">5</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">forwardPropagationFunction</span><span class="p">,</span> <span class="n">backwardPropagationFunction</span><span class="p">,</span> <span class="n">giveInputsToBackprop</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="SGD.__init__-6"><a href="#SGD.__init__-6"><span class="linenos">6</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">forwardPropagationFunction</span> <span class="o">=</span> <span class="n">forwardPropagationFunction</span>
</span><span id="SGD.__init__-7"><a href="#SGD.__init__-7"><span class="linenos">7</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">backwardPropagationFunction</span> <span class="o">=</span> <span class="n">backwardPropagationFunction</span>
</span><span id="SGD.__init__-8"><a href="#SGD.__init__-8"><span class="linenos">8</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">giveInputsToBackprop</span> <span class="o">=</span> <span class="n">giveInputsToBackprop</span> 
</span></pre></div>


    

                            </div>
                            <div id="SGD.forwardPropagationFunction" class="classattr">
                                <div class="attr variable">
            <span class="name">forwardPropagationFunction</span>

        
    </div>
    <a class="headerlink" href="#SGD.forwardPropagationFunction"></a>
    
    

                            </div>
                            <div id="SGD.backwardPropagationFunction" class="classattr">
                                <div class="attr variable">
            <span class="name">backwardPropagationFunction</span>

        
    </div>
    <a class="headerlink" href="#SGD.backwardPropagationFunction"></a>
    
    

                            </div>
                            <div id="SGD.giveInputsToBackprop" class="classattr">
                                <div class="attr variable">
            <span class="name">giveInputsToBackprop</span>

        
    </div>
    <a class="headerlink" href="#SGD.giveInputsToBackprop"></a>
    
    

                            </div>
                            <div id="SGD.optimiser" class="classattr">
                                        <input id="SGD.optimiser-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">optimiser</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">inputData</span>, </span><span class="param"><span class="n">labels</span>, </span><span class="param"><span class="n">useBatches</span>, </span><span class="param"><span class="n">batchSize</span>, </span><span class="param"><span class="n">learningRate</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="SGD.optimiser-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#SGD.optimiser"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="SGD.optimiser-10"><a href="#SGD.optimiser-10"><span class="linenos">10</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">optimiser</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">useBatches</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">learningRate</span><span class="p">):</span>
</span><span id="SGD.optimiser-11"><a href="#SGD.optimiser-11"><span class="linenos">11</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainStochasticGradientDescent_WithoutBatches</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">learningRate</span><span class="p">)</span>
</span></pre></div>


    

                            </div>
                </section>
                <section id="GD">
                            <input id="GD-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">GD</span>:

                <label class="view-source-button" for="GD-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#GD"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="GD-4"><a href="#GD-4"><span class="linenos"> 4</span></a><span class="k">class</span><span class="w"> </span><span class="nc">GD</span><span class="p">:</span>
</span><span id="GD-5"><a href="#GD-5"><span class="linenos"> 5</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">forwardPropagationFunction</span><span class="p">,</span> <span class="n">backwardPropagationFunction</span><span class="p">,</span> <span class="n">giveInputsToBackprop</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="GD-6"><a href="#GD-6"><span class="linenos"> 6</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">forwardPropagationFunction</span> <span class="o">=</span> <span class="n">forwardPropagationFunction</span>
</span><span id="GD-7"><a href="#GD-7"><span class="linenos"> 7</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">backwardPropagationFunction</span> <span class="o">=</span> <span class="n">backwardPropagationFunction</span>
</span><span id="GD-8"><a href="#GD-8"><span class="linenos"> 8</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">giveInputsToBackprop</span> <span class="o">=</span> <span class="n">giveInputsToBackprop</span> 
</span><span id="GD-9"><a href="#GD-9"><span class="linenos"> 9</span></a>
</span><span id="GD-10"><a href="#GD-10"><span class="linenos">10</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">optimiser</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">useBatches</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">learningRate</span><span class="p">):</span>
</span><span id="GD-11"><a href="#GD-11"><span class="linenos">11</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainGradientDescent_Batching</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">learningRate</span><span class="p">)</span>
</span><span id="GD-12"><a href="#GD-12"><span class="linenos">12</span></a>
</span><span id="GD-13"><a href="#GD-13"><span class="linenos">13</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_trainGradientDescent_Batching</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">learningRate</span><span class="p">):</span>     
</span><span id="GD-14"><a href="#GD-14"><span class="linenos">14</span></a>        <span class="n">AllOutputs</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="GD-15"><a href="#GD-15"><span class="linenos">15</span></a>        <span class="n">numBatches</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputData</span><span class="p">)</span> <span class="o">//</span> <span class="n">batchSize</span>
</span><span id="GD-16"><a href="#GD-16"><span class="linenos">16</span></a>        <span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
</span><span id="GD-17"><a href="#GD-17"><span class="linenos">17</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">numBatches</span><span class="p">):</span>
</span><span id="GD-18"><a href="#GD-18"><span class="linenos">18</span></a>            <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span><span id="GD-19"><a href="#GD-19"><span class="linenos">19</span></a>            <span class="n">batchData</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputData</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">batchSize</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">batchSize</span><span class="p">])</span>
</span><span id="GD-20"><a href="#GD-20"><span class="linenos">20</span></a>            <span class="n">batchLabels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">batchSize</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">batchSize</span><span class="p">])</span>
</span><span id="GD-21"><a href="#GD-21"><span class="linenos">21</span></a>            <span class="n">Parameters</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="GD-22"><a href="#GD-22"><span class="linenos">22</span></a>
</span><span id="GD-23"><a href="#GD-23"><span class="linenos">23</span></a>            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forwardPropagationFunction</span><span class="p">(</span><span class="n">batchData</span><span class="p">)</span>
</span><span id="GD-24"><a href="#GD-24"><span class="linenos">24</span></a>            <span class="n">AllOutputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</span><span id="GD-25"><a href="#GD-25"><span class="linenos">25</span></a>
</span><span id="GD-26"><a href="#GD-26"><span class="linenos">26</span></a>            <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">giveInputsToBackprop</span> <span class="o">==</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="GD-27"><a href="#GD-27"><span class="linenos">27</span></a>                <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backwardPropagationFunction</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">batchLabels</span><span class="p">)</span>
</span><span id="GD-28"><a href="#GD-28"><span class="linenos">28</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="GD-29"><a href="#GD-29"><span class="linenos">29</span></a>                <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backwardPropagationFunction</span><span class="p">(</span><span class="n">batchData</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">batchLabels</span><span class="p">)</span>
</span><span id="GD-30"><a href="#GD-30"><span class="linenos">30</span></a>
</span><span id="GD-31"><a href="#GD-31"><span class="linenos">31</span></a>            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">Gradients</span><span class="p">:</span>
</span><span id="GD-32"><a href="#GD-32"><span class="linenos">32</span></a>                <span class="k">if</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="nb">list</span><span class="p">)):</span> <span class="c1"># inhomengous array</span>
</span><span id="GD-33"><a href="#GD-33"><span class="linenos">33</span></a>                    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">])):</span>
</span><span id="GD-34"><a href="#GD-34"><span class="linenos">34</span></a>                        <span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">j</span><span class="p">])</span> <span class="o">/</span> <span class="n">batchSize</span>
</span><span id="GD-35"><a href="#GD-35"><span class="linenos">35</span></a>                <span class="k">else</span><span class="p">:</span>
</span><span id="GD-36"><a href="#GD-36"><span class="linenos">36</span></a>                    <span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">/</span> <span class="n">batchSize</span>
</span><span id="GD-37"><a href="#GD-37"><span class="linenos">37</span></a>
</span><span id="GD-38"><a href="#GD-38"><span class="linenos">38</span></a>            <span class="n">Parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_updateWeightsBiases</span><span class="p">(</span><span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span><span class="p">,</span> <span class="n">learningRate</span><span class="p">)</span>
</span><span id="GD-39"><a href="#GD-39"><span class="linenos">39</span></a>            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;epoch: </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> / </span><span class="si">{</span><span class="n">numBatches</span><span class="si">}</span><span class="s2">, took </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="GD-40"><a href="#GD-40"><span class="linenos">40</span></a>        <span class="k">return</span> <span class="n">AllOutputs</span><span class="p">,</span> <span class="n">Parameters</span>
</span><span id="GD-41"><a href="#GD-41"><span class="linenos">41</span></a>    
</span><span id="GD-42"><a href="#GD-42"><span class="linenos">42</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_updateWeightsBiases</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span><span class="p">,</span> <span class="n">learningRate</span><span class="p">):</span>
</span><span id="GD-43"><a href="#GD-43"><span class="linenos">43</span></a>        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">Gradients</span><span class="p">:</span>
</span><span id="GD-44"><a href="#GD-44"><span class="linenos">44</span></a>            <span class="k">if</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="nb">list</span><span class="p">)):</span> <span class="c1"># if the Gradient is a inhomengous list (jagged array, which numpy doesnt like)</span>
</span><span id="GD-45"><a href="#GD-45"><span class="linenos">45</span></a>                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">])):</span>
</span><span id="GD-46"><a href="#GD-46"><span class="linenos">46</span></a>                    <span class="n">Parameters</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="n">learningRate</span> <span class="o">*</span> <span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
</span><span id="GD-47"><a href="#GD-47"><span class="linenos">47</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="GD-48"><a href="#GD-48"><span class="linenos">48</span></a>                <span class="n">Parameters</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">-=</span> <span class="n">learningRate</span> <span class="o">*</span> <span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
</span><span id="GD-49"><a href="#GD-49"><span class="linenos">49</span></a>        <span class="k">return</span> <span class="n">Parameters</span>
</span></pre></div>


    

                            <div id="GD.__init__" class="classattr">
                                        <input id="GD.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">GD</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">forwardPropagationFunction</span>,</span><span class="param">	<span class="n">backwardPropagationFunction</span>,</span><span class="param">	<span class="n">giveInputsToBackprop</span><span class="o">=</span><span class="kc">False</span></span>)</span>

                <label class="view-source-button" for="GD.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#GD.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="GD.__init__-5"><a href="#GD.__init__-5"><span class="linenos">5</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">forwardPropagationFunction</span><span class="p">,</span> <span class="n">backwardPropagationFunction</span><span class="p">,</span> <span class="n">giveInputsToBackprop</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span><span id="GD.__init__-6"><a href="#GD.__init__-6"><span class="linenos">6</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">forwardPropagationFunction</span> <span class="o">=</span> <span class="n">forwardPropagationFunction</span>
</span><span id="GD.__init__-7"><a href="#GD.__init__-7"><span class="linenos">7</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">backwardPropagationFunction</span> <span class="o">=</span> <span class="n">backwardPropagationFunction</span>
</span><span id="GD.__init__-8"><a href="#GD.__init__-8"><span class="linenos">8</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">giveInputsToBackprop</span> <span class="o">=</span> <span class="n">giveInputsToBackprop</span> 
</span></pre></div>


    

                            </div>
                            <div id="GD.forwardPropagationFunction" class="classattr">
                                <div class="attr variable">
            <span class="name">forwardPropagationFunction</span>

        
    </div>
    <a class="headerlink" href="#GD.forwardPropagationFunction"></a>
    
    

                            </div>
                            <div id="GD.backwardPropagationFunction" class="classattr">
                                <div class="attr variable">
            <span class="name">backwardPropagationFunction</span>

        
    </div>
    <a class="headerlink" href="#GD.backwardPropagationFunction"></a>
    
    

                            </div>
                            <div id="GD.giveInputsToBackprop" class="classattr">
                                <div class="attr variable">
            <span class="name">giveInputsToBackprop</span>

        
    </div>
    <a class="headerlink" href="#GD.giveInputsToBackprop"></a>
    
    

                            </div>
                            <div id="GD.optimiser" class="classattr">
                                        <input id="GD.optimiser-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">optimiser</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">inputData</span>, </span><span class="param"><span class="n">labels</span>, </span><span class="param"><span class="n">useBatches</span>, </span><span class="param"><span class="n">batchSize</span>, </span><span class="param"><span class="n">learningRate</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="GD.optimiser-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#GD.optimiser"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="GD.optimiser-10"><a href="#GD.optimiser-10"><span class="linenos">10</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">optimiser</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">useBatches</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">learningRate</span><span class="p">):</span>
</span><span id="GD.optimiser-11"><a href="#GD.optimiser-11"><span class="linenos">11</span></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainGradientDescent_Batching</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">learningRate</span><span class="p">)</span>
</span></pre></div>


    

                            </div>
                </section>
                <section id="RMSProp">
                            <input id="RMSProp-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">RMSProp</span>:

                <label class="view-source-button" for="RMSProp-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#RMSProp"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="RMSProp-4"><a href="#RMSProp-4"><span class="linenos"> 4</span></a><span class="k">class</span><span class="w"> </span><span class="nc">RMSProp</span><span class="p">:</span>
</span><span id="RMSProp-5"><a href="#RMSProp-5"><span class="linenos"> 5</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">forwardPropagationFunction</span><span class="p">,</span> <span class="n">backwardPropagationFunction</span><span class="p">,</span> <span class="n">giveInputsToBackprop</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
</span><span id="RMSProp-6"><a href="#RMSProp-6"><span class="linenos"> 6</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">squaredAvg</span> <span class="o">=</span> <span class="p">{}</span>  
</span><span id="RMSProp-7"><a href="#RMSProp-7"><span class="linenos"> 7</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">forwardPropagationFunction</span> <span class="o">=</span> <span class="n">forwardPropagationFunction</span>
</span><span id="RMSProp-8"><a href="#RMSProp-8"><span class="linenos"> 8</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">backwardPropagationFunction</span> <span class="o">=</span> <span class="n">backwardPropagationFunction</span>
</span><span id="RMSProp-9"><a href="#RMSProp-9"><span class="linenos"> 9</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">giveInputsToBackprop</span> <span class="o">=</span> <span class="n">giveInputsToBackprop</span>
</span><span id="RMSProp-10"><a href="#RMSProp-10"><span class="linenos">10</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">decay</span> <span class="o">=</span> <span class="n">decay</span>
</span><span id="RMSProp-11"><a href="#RMSProp-11"><span class="linenos">11</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
</span><span id="RMSProp-12"><a href="#RMSProp-12"><span class="linenos">12</span></a>
</span><span id="RMSProp-13"><a href="#RMSProp-13"><span class="linenos">13</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">optimiser</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">useBatches</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
</span><span id="RMSProp-14"><a href="#RMSProp-14"><span class="linenos">14</span></a>        <span class="k">if</span> <span class="n">useBatches</span><span class="p">:</span>
</span><span id="RMSProp-15"><a href="#RMSProp-15"><span class="linenos">15</span></a>            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_RMSPropOptimiserWithBatches</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
</span><span id="RMSProp-16"><a href="#RMSProp-16"><span class="linenos">16</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="RMSProp-17"><a href="#RMSProp-17"><span class="linenos">17</span></a>            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_RMSPropOptimiserWithoutBatches</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
</span><span id="RMSProp-18"><a href="#RMSProp-18"><span class="linenos">18</span></a>
</span><span id="RMSProp-19"><a href="#RMSProp-19"><span class="linenos">19</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_RMSPropOptimiserWithBatches</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
</span><span id="RMSProp-20"><a href="#RMSProp-20"><span class="linenos">20</span></a>        <span class="n">AllOutputs</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="RMSProp-21"><a href="#RMSProp-21"><span class="linenos">21</span></a>        <span class="n">numBatches</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputData</span><span class="p">)</span> <span class="o">//</span> <span class="n">batchSize</span>
</span><span id="RMSProp-22"><a href="#RMSProp-22"><span class="linenos">22</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">numBatches</span><span class="p">):</span>
</span><span id="RMSProp-23"><a href="#RMSProp-23"><span class="linenos">23</span></a>            <span class="n">batchData</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputData</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">batchSize</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">batchSize</span><span class="p">])</span>
</span><span id="RMSProp-24"><a href="#RMSProp-24"><span class="linenos">24</span></a>            <span class="n">batchLabels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">batchSize</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">batchSize</span><span class="p">])</span>
</span><span id="RMSProp-25"><a href="#RMSProp-25"><span class="linenos">25</span></a>            <span class="n">Parameters</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="RMSProp-26"><a href="#RMSProp-26"><span class="linenos">26</span></a>
</span><span id="RMSProp-27"><a href="#RMSProp-27"><span class="linenos">27</span></a>            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forwardPropagationFunction</span><span class="p">(</span><span class="n">batchData</span><span class="p">)</span>
</span><span id="RMSProp-28"><a href="#RMSProp-28"><span class="linenos">28</span></a>            <span class="n">AllOutputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</span><span id="RMSProp-29"><a href="#RMSProp-29"><span class="linenos">29</span></a>
</span><span id="RMSProp-30"><a href="#RMSProp-30"><span class="linenos">30</span></a>            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">giveInputsToBackprop</span><span class="p">:</span>
</span><span id="RMSProp-31"><a href="#RMSProp-31"><span class="linenos">31</span></a>                <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backwardPropagationFunction</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">batchLabels</span><span class="p">)</span>
</span><span id="RMSProp-32"><a href="#RMSProp-32"><span class="linenos">32</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="RMSProp-33"><a href="#RMSProp-33"><span class="linenos">33</span></a>                <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backwardPropagationFunction</span><span class="p">(</span><span class="n">batchData</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">batchLabels</span><span class="p">)</span>
</span><span id="RMSProp-34"><a href="#RMSProp-34"><span class="linenos">34</span></a>
</span><span id="RMSProp-35"><a href="#RMSProp-35"><span class="linenos">35</span></a>            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">Gradients</span><span class="p">:</span>
</span><span id="RMSProp-36"><a href="#RMSProp-36"><span class="linenos">36</span></a>                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="nb">list</span><span class="p">):</span>
</span><span id="RMSProp-37"><a href="#RMSProp-37"><span class="linenos">37</span></a>                    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">])):</span>
</span><span id="RMSProp-38"><a href="#RMSProp-38"><span class="linenos">38</span></a>                        <span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">j</span><span class="p">])</span> <span class="o">/</span> <span class="n">batchSize</span>
</span><span id="RMSProp-39"><a href="#RMSProp-39"><span class="linenos">39</span></a>                <span class="k">else</span><span class="p">:</span>
</span><span id="RMSProp-40"><a href="#RMSProp-40"><span class="linenos">40</span></a>                    <span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">/</span> <span class="n">batchSize</span>
</span><span id="RMSProp-41"><a href="#RMSProp-41"><span class="linenos">41</span></a>
</span><span id="RMSProp-42"><a href="#RMSProp-42"><span class="linenos">42</span></a>            <span class="n">Parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_RMSPropUpdate</span><span class="p">(</span><span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
</span><span id="RMSProp-43"><a href="#RMSProp-43"><span class="linenos">43</span></a>        <span class="k">return</span> <span class="n">AllOutputs</span><span class="p">,</span> <span class="n">Parameters</span>
</span><span id="RMSProp-44"><a href="#RMSProp-44"><span class="linenos">44</span></a>
</span><span id="RMSProp-45"><a href="#RMSProp-45"><span class="linenos">45</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_RMSPropOptimiserWithoutBatches</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
</span><span id="RMSProp-46"><a href="#RMSProp-46"><span class="linenos">46</span></a>        <span class="n">AllOutputs</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="RMSProp-47"><a href="#RMSProp-47"><span class="linenos">47</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputData</span><span class="p">)):</span>
</span><span id="RMSProp-48"><a href="#RMSProp-48"><span class="linenos">48</span></a>            <span class="nb">input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">inputData</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span>
</span><span id="RMSProp-49"><a href="#RMSProp-49"><span class="linenos">49</span></a>            <span class="n">lab</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span>
</span><span id="RMSProp-50"><a href="#RMSProp-50"><span class="linenos">50</span></a>            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forwardPropagationFunction</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span><span id="RMSProp-51"><a href="#RMSProp-51"><span class="linenos">51</span></a>            <span class="n">AllOutputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</span><span id="RMSProp-52"><a href="#RMSProp-52"><span class="linenos">52</span></a>
</span><span id="RMSProp-53"><a href="#RMSProp-53"><span class="linenos">53</span></a>            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">giveInputsToBackprop</span><span class="p">:</span>
</span><span id="RMSProp-54"><a href="#RMSProp-54"><span class="linenos">54</span></a>                <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backwardPropagationFunction</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">lab</span><span class="p">)</span>
</span><span id="RMSProp-55"><a href="#RMSProp-55"><span class="linenos">55</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="RMSProp-56"><a href="#RMSProp-56"><span class="linenos">56</span></a>                <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backwardPropagationFunction</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">lab</span><span class="p">)</span>
</span><span id="RMSProp-57"><a href="#RMSProp-57"><span class="linenos">57</span></a>
</span><span id="RMSProp-58"><a href="#RMSProp-58"><span class="linenos">58</span></a>            <span class="n">Parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_RMSPropUpdate</span><span class="p">(</span><span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
</span><span id="RMSProp-59"><a href="#RMSProp-59"><span class="linenos">59</span></a>        <span class="k">return</span> <span class="n">AllOutputs</span><span class="p">,</span> <span class="n">Parameters</span>
</span><span id="RMSProp-60"><a href="#RMSProp-60"><span class="linenos">60</span></a>
</span><span id="RMSProp-61"><a href="#RMSProp-61"><span class="linenos">61</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_RMSPropUpdate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
</span><span id="RMSProp-62"><a href="#RMSProp-62"><span class="linenos">62</span></a>        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">Gradients</span><span class="p">:</span>
</span><span id="RMSProp-63"><a href="#RMSProp-63"><span class="linenos">63</span></a>            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="nb">list</span><span class="p">):</span>
</span><span id="RMSProp-64"><a href="#RMSProp-64"><span class="linenos">64</span></a>                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">grad</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]):</span>
</span><span id="RMSProp-65"><a href="#RMSProp-65"><span class="linenos">65</span></a>                    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
</span><span id="RMSProp-66"><a href="#RMSProp-66"><span class="linenos">66</span></a>                    <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">squaredAvg</span><span class="p">:</span>
</span><span id="RMSProp-67"><a href="#RMSProp-67"><span class="linenos">67</span></a>                        <span class="bp">self</span><span class="o">.</span><span class="n">squaredAvg</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">g</span><span class="p">)</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]]</span>
</span><span id="RMSProp-68"><a href="#RMSProp-68"><span class="linenos">68</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">squaredAvg</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decay</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">squaredAvg</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">decay</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">grad</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="RMSProp-69"><a href="#RMSProp-69"><span class="linenos">69</span></a>                    <span class="n">Parameters</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">squaredAvg</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span>
</span><span id="RMSProp-70"><a href="#RMSProp-70"><span class="linenos">70</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="RMSProp-71"><a href="#RMSProp-71"><span class="linenos">71</span></a>                <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">squaredAvg</span><span class="p">:</span>
</span><span id="RMSProp-72"><a href="#RMSProp-72"><span class="linenos">72</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">squaredAvg</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
</span><span id="RMSProp-73"><a href="#RMSProp-73"><span class="linenos">73</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">squaredAvg</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decay</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">squaredAvg</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">decay</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="RMSProp-74"><a href="#RMSProp-74"><span class="linenos">74</span></a>                <span class="n">Parameters</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">squaredAvg</span><span class="p">[</span><span class="n">key</span><span class="p">])</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span>
</span><span id="RMSProp-75"><a href="#RMSProp-75"><span class="linenos">75</span></a>        <span class="k">return</span> <span class="n">Parameters</span>
</span></pre></div>


    

                            <div id="RMSProp.__init__" class="classattr">
                                        <input id="RMSProp.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">RMSProp</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">forwardPropagationFunction</span>,</span><span class="param">	<span class="n">backwardPropagationFunction</span>,</span><span class="param">	<span class="n">giveInputsToBackprop</span><span class="o">=</span><span class="kc">False</span>,</span><span class="param">	<span class="n">decay</span><span class="o">=</span><span class="mf">0.9</span>,</span><span class="param">	<span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-08</span></span>)</span>

                <label class="view-source-button" for="RMSProp.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#RMSProp.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="RMSProp.__init__-5"><a href="#RMSProp.__init__-5"><span class="linenos"> 5</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">forwardPropagationFunction</span><span class="p">,</span> <span class="n">backwardPropagationFunction</span><span class="p">,</span> <span class="n">giveInputsToBackprop</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
</span><span id="RMSProp.__init__-6"><a href="#RMSProp.__init__-6"><span class="linenos"> 6</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">squaredAvg</span> <span class="o">=</span> <span class="p">{}</span>  
</span><span id="RMSProp.__init__-7"><a href="#RMSProp.__init__-7"><span class="linenos"> 7</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">forwardPropagationFunction</span> <span class="o">=</span> <span class="n">forwardPropagationFunction</span>
</span><span id="RMSProp.__init__-8"><a href="#RMSProp.__init__-8"><span class="linenos"> 8</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">backwardPropagationFunction</span> <span class="o">=</span> <span class="n">backwardPropagationFunction</span>
</span><span id="RMSProp.__init__-9"><a href="#RMSProp.__init__-9"><span class="linenos"> 9</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">giveInputsToBackprop</span> <span class="o">=</span> <span class="n">giveInputsToBackprop</span>
</span><span id="RMSProp.__init__-10"><a href="#RMSProp.__init__-10"><span class="linenos">10</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">decay</span> <span class="o">=</span> <span class="n">decay</span>
</span><span id="RMSProp.__init__-11"><a href="#RMSProp.__init__-11"><span class="linenos">11</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
</span></pre></div>


    

                            </div>
                            <div id="RMSProp.squaredAvg" class="classattr">
                                <div class="attr variable">
            <span class="name">squaredAvg</span>

        
    </div>
    <a class="headerlink" href="#RMSProp.squaredAvg"></a>
    
    

                            </div>
                            <div id="RMSProp.forwardPropagationFunction" class="classattr">
                                <div class="attr variable">
            <span class="name">forwardPropagationFunction</span>

        
    </div>
    <a class="headerlink" href="#RMSProp.forwardPropagationFunction"></a>
    
    

                            </div>
                            <div id="RMSProp.backwardPropagationFunction" class="classattr">
                                <div class="attr variable">
            <span class="name">backwardPropagationFunction</span>

        
    </div>
    <a class="headerlink" href="#RMSProp.backwardPropagationFunction"></a>
    
    

                            </div>
                            <div id="RMSProp.giveInputsToBackprop" class="classattr">
                                <div class="attr variable">
            <span class="name">giveInputsToBackprop</span>

        
    </div>
    <a class="headerlink" href="#RMSProp.giveInputsToBackprop"></a>
    
    

                            </div>
                            <div id="RMSProp.decay" class="classattr">
                                <div class="attr variable">
            <span class="name">decay</span>

        
    </div>
    <a class="headerlink" href="#RMSProp.decay"></a>
    
    

                            </div>
                            <div id="RMSProp.epsilon" class="classattr">
                                <div class="attr variable">
            <span class="name">epsilon</span>

        
    </div>
    <a class="headerlink" href="#RMSProp.epsilon"></a>
    
    

                            </div>
                            <div id="RMSProp.optimiser" class="classattr">
                                        <input id="RMSProp.optimiser-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">optimiser</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">inputData</span>, </span><span class="param"><span class="n">labels</span>, </span><span class="param"><span class="n">useBatches</span>, </span><span class="param"><span class="n">batchSize</span>, </span><span class="param"><span class="n">alpha</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="RMSProp.optimiser-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#RMSProp.optimiser"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="RMSProp.optimiser-13"><a href="#RMSProp.optimiser-13"><span class="linenos">13</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">optimiser</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">useBatches</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
</span><span id="RMSProp.optimiser-14"><a href="#RMSProp.optimiser-14"><span class="linenos">14</span></a>        <span class="k">if</span> <span class="n">useBatches</span><span class="p">:</span>
</span><span id="RMSProp.optimiser-15"><a href="#RMSProp.optimiser-15"><span class="linenos">15</span></a>            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_RMSPropOptimiserWithBatches</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
</span><span id="RMSProp.optimiser-16"><a href="#RMSProp.optimiser-16"><span class="linenos">16</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="RMSProp.optimiser-17"><a href="#RMSProp.optimiser-17"><span class="linenos">17</span></a>            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_RMSPropOptimiserWithoutBatches</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
</span></pre></div>


    

                            </div>
                </section>
                <section id="Lion">
                            <input id="Lion-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">Lion</span>:

                <label class="view-source-button" for="Lion-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Lion"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Lion-4"><a href="#Lion-4"><span class="linenos"> 4</span></a><span class="k">class</span><span class="w"> </span><span class="nc">Lion</span><span class="p">:</span>
</span><span id="Lion-5"><a href="#Lion-5"><span class="linenos"> 5</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">forward</span><span class="p">,</span> <span class="n">backward</span><span class="p">,</span> <span class="n">giveInputsToBackprop</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
</span><span id="Lion-6"><a href="#Lion-6"><span class="linenos"> 6</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">forward</span> <span class="o">=</span> <span class="n">forward</span>
</span><span id="Lion-7"><a href="#Lion-7"><span class="linenos"> 7</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">backward</span> <span class="o">=</span> <span class="n">backward</span>
</span><span id="Lion-8"><a href="#Lion-8"><span class="linenos"> 8</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">giveInputsToBackprop</span> <span class="o">=</span> <span class="n">giveInputsToBackprop</span>
</span><span id="Lion-9"><a href="#Lion-9"><span class="linenos"> 9</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span> <span class="o">=</span> <span class="n">beta1</span>
</span><span id="Lion-10"><a href="#Lion-10"><span class="linenos">10</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="p">{}</span>  
</span><span id="Lion-11"><a href="#Lion-11"><span class="linenos">11</span></a>
</span><span id="Lion-12"><a href="#Lion-12"><span class="linenos">12</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">optimiser</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">useBatches</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batchSize</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.001</span><span class="p">):</span>
</span><span id="Lion-13"><a href="#Lion-13"><span class="linenos">13</span></a>        <span class="k">if</span> <span class="n">useBatches</span><span class="p">:</span>
</span><span id="Lion-14"><a href="#Lion-14"><span class="linenos">14</span></a>            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimiserWithBatches</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
</span><span id="Lion-15"><a href="#Lion-15"><span class="linenos">15</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="Lion-16"><a href="#Lion-16"><span class="linenos">16</span></a>            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimiserWithoutBatches</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
</span><span id="Lion-17"><a href="#Lion-17"><span class="linenos">17</span></a>
</span><span id="Lion-18"><a href="#Lion-18"><span class="linenos">18</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_optimiserWithBatches</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
</span><span id="Lion-19"><a href="#Lion-19"><span class="linenos">19</span></a>        <span class="n">allOutputs</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="Lion-20"><a href="#Lion-20"><span class="linenos">20</span></a>        <span class="n">numBatches</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputData</span><span class="p">)</span> <span class="o">//</span> <span class="n">batchSize</span>
</span><span id="Lion-21"><a href="#Lion-21"><span class="linenos">21</span></a>        <span class="n">Parameters</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="Lion-22"><a href="#Lion-22"><span class="linenos">22</span></a>
</span><span id="Lion-23"><a href="#Lion-23"><span class="linenos">23</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">numBatches</span><span class="p">):</span>
</span><span id="Lion-24"><a href="#Lion-24"><span class="linenos">24</span></a>            <span class="n">batchData</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputData</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">batchSize</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">batchSize</span><span class="p">])</span>
</span><span id="Lion-25"><a href="#Lion-25"><span class="linenos">25</span></a>            <span class="n">batchLabels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">batchSize</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">batchSize</span><span class="p">])</span>
</span><span id="Lion-26"><a href="#Lion-26"><span class="linenos">26</span></a>
</span><span id="Lion-27"><a href="#Lion-27"><span class="linenos">27</span></a>            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">batchData</span><span class="p">)</span>
</span><span id="Lion-28"><a href="#Lion-28"><span class="linenos">28</span></a>            <span class="n">allOutputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</span><span id="Lion-29"><a href="#Lion-29"><span class="linenos">29</span></a>
</span><span id="Lion-30"><a href="#Lion-30"><span class="linenos">30</span></a>            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">giveInputsToBackprop</span><span class="p">:</span>
</span><span id="Lion-31"><a href="#Lion-31"><span class="linenos">31</span></a>                <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">batchLabels</span><span class="p">)</span>
</span><span id="Lion-32"><a href="#Lion-32"><span class="linenos">32</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="Lion-33"><a href="#Lion-33"><span class="linenos">33</span></a>                <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">batchData</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">batchLabels</span><span class="p">)</span>
</span><span id="Lion-34"><a href="#Lion-34"><span class="linenos">34</span></a>
</span><span id="Lion-35"><a href="#Lion-35"><span class="linenos">35</span></a>            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">Gradients</span><span class="p">:</span>
</span><span id="Lion-36"><a href="#Lion-36"><span class="linenos">36</span></a>                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="nb">list</span><span class="p">):</span>
</span><span id="Lion-37"><a href="#Lion-37"><span class="linenos">37</span></a>                    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">])):</span>
</span><span id="Lion-38"><a href="#Lion-38"><span class="linenos">38</span></a>                        <span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">j</span><span class="p">])</span> <span class="o">/</span> <span class="n">batchSize</span>
</span><span id="Lion-39"><a href="#Lion-39"><span class="linenos">39</span></a>                <span class="k">else</span><span class="p">:</span>
</span><span id="Lion-40"><a href="#Lion-40"><span class="linenos">40</span></a>                    <span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">/</span> <span class="n">batchSize</span>
</span><span id="Lion-41"><a href="#Lion-41"><span class="linenos">41</span></a>
</span><span id="Lion-42"><a href="#Lion-42"><span class="linenos">42</span></a>            <span class="n">Parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lionUpdate</span><span class="p">(</span><span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
</span><span id="Lion-43"><a href="#Lion-43"><span class="linenos">43</span></a>
</span><span id="Lion-44"><a href="#Lion-44"><span class="linenos">44</span></a>        <span class="k">return</span> <span class="n">allOutputs</span><span class="p">,</span> <span class="n">Parameters</span>
</span><span id="Lion-45"><a href="#Lion-45"><span class="linenos">45</span></a>
</span><span id="Lion-46"><a href="#Lion-46"><span class="linenos">46</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_optimiserWithoutBatches</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
</span><span id="Lion-47"><a href="#Lion-47"><span class="linenos">47</span></a>        <span class="n">allOutputs</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="Lion-48"><a href="#Lion-48"><span class="linenos">48</span></a>        <span class="n">Parameters</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="Lion-49"><a href="#Lion-49"><span class="linenos">49</span></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputData</span><span class="p">)):</span>
</span><span id="Lion-50"><a href="#Lion-50"><span class="linenos">50</span></a>            <span class="n">inp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">inputData</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span>
</span><span id="Lion-51"><a href="#Lion-51"><span class="linenos">51</span></a>            <span class="n">lab</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span>
</span><span id="Lion-52"><a href="#Lion-52"><span class="linenos">52</span></a>
</span><span id="Lion-53"><a href="#Lion-53"><span class="linenos">53</span></a>            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
</span><span id="Lion-54"><a href="#Lion-54"><span class="linenos">54</span></a>            <span class="n">allOutputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</span><span id="Lion-55"><a href="#Lion-55"><span class="linenos">55</span></a>
</span><span id="Lion-56"><a href="#Lion-56"><span class="linenos">56</span></a>            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">giveInputsToBackprop</span><span class="p">:</span>
</span><span id="Lion-57"><a href="#Lion-57"><span class="linenos">57</span></a>                <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">lab</span><span class="p">)</span>
</span><span id="Lion-58"><a href="#Lion-58"><span class="linenos">58</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="Lion-59"><a href="#Lion-59"><span class="linenos">59</span></a>                <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">lab</span><span class="p">)</span>
</span><span id="Lion-60"><a href="#Lion-60"><span class="linenos">60</span></a>
</span><span id="Lion-61"><a href="#Lion-61"><span class="linenos">61</span></a>            <span class="n">Parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lionUpdate</span><span class="p">(</span><span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
</span><span id="Lion-62"><a href="#Lion-62"><span class="linenos">62</span></a>        <span class="k">return</span> <span class="n">allOutputs</span><span class="p">,</span> <span class="n">Parameters</span>
</span><span id="Lion-63"><a href="#Lion-63"><span class="linenos">63</span></a>
</span><span id="Lion-64"><a href="#Lion-64"><span class="linenos">64</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_lionUpdate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Parameters</span><span class="p">,</span> <span class="n">Gradients</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
</span><span id="Lion-65"><a href="#Lion-65"><span class="linenos">65</span></a>        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">Gradients</span><span class="p">:</span>
</span><span id="Lion-66"><a href="#Lion-66"><span class="linenos">66</span></a>            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="nb">list</span><span class="p">):</span>
</span><span id="Lion-67"><a href="#Lion-67"><span class="linenos">67</span></a>                <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">:</span>
</span><span id="Lion-68"><a href="#Lion-68"><span class="linenos">68</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">g</span><span class="p">)</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]]</span>
</span><span id="Lion-69"><a href="#Lion-69"><span class="linenos">69</span></a>                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">grad</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]):</span>
</span><span id="Lion-70"><a href="#Lion-70"><span class="linenos">70</span></a>                    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
</span><span id="Lion-71"><a href="#Lion-71"><span class="linenos">71</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span><span class="p">)</span> <span class="o">*</span> <span class="n">grad</span>
</span><span id="Lion-72"><a href="#Lion-72"><span class="linenos">72</span></a>                    <span class="n">Parameters</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">i</span><span class="p">])</span>
</span><span id="Lion-73"><a href="#Lion-73"><span class="linenos">73</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="Lion-74"><a href="#Lion-74"><span class="linenos">74</span></a>                <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">:</span>
</span><span id="Lion-75"><a href="#Lion-75"><span class="linenos">75</span></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
</span><span id="Lion-76"><a href="#Lion-76"><span class="linenos">76</span></a>                <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span><span class="p">)</span> <span class="o">*</span> <span class="n">Gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
</span><span id="Lion-77"><a href="#Lion-77"><span class="linenos">77</span></a>                <span class="n">Parameters</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
</span><span id="Lion-78"><a href="#Lion-78"><span class="linenos">78</span></a>        <span class="k">return</span> <span class="n">Parameters</span>
</span></pre></div>


    

                            <div id="Lion.__init__" class="classattr">
                                        <input id="Lion.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">Lion</span><span class="signature pdoc-code condensed">(<span class="param"><span class="n">forward</span>, </span><span class="param"><span class="n">backward</span>, </span><span class="param"><span class="n">giveInputsToBackprop</span><span class="o">=</span><span class="kc">False</span>, </span><span class="param"><span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span></span>)</span>

                <label class="view-source-button" for="Lion.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Lion.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Lion.__init__-5"><a href="#Lion.__init__-5"><span class="linenos"> 5</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">forward</span><span class="p">,</span> <span class="n">backward</span><span class="p">,</span> <span class="n">giveInputsToBackprop</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
</span><span id="Lion.__init__-6"><a href="#Lion.__init__-6"><span class="linenos"> 6</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">forward</span> <span class="o">=</span> <span class="n">forward</span>
</span><span id="Lion.__init__-7"><a href="#Lion.__init__-7"><span class="linenos"> 7</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">backward</span> <span class="o">=</span> <span class="n">backward</span>
</span><span id="Lion.__init__-8"><a href="#Lion.__init__-8"><span class="linenos"> 8</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">giveInputsToBackprop</span> <span class="o">=</span> <span class="n">giveInputsToBackprop</span>
</span><span id="Lion.__init__-9"><a href="#Lion.__init__-9"><span class="linenos"> 9</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span> <span class="o">=</span> <span class="n">beta1</span>
</span><span id="Lion.__init__-10"><a href="#Lion.__init__-10"><span class="linenos">10</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="p">{}</span>  
</span></pre></div>


    

                            </div>
                            <div id="Lion.forward" class="classattr">
                                <div class="attr variable">
            <span class="name">forward</span>

        
    </div>
    <a class="headerlink" href="#Lion.forward"></a>
    
    

                            </div>
                            <div id="Lion.backward" class="classattr">
                                <div class="attr variable">
            <span class="name">backward</span>

        
    </div>
    <a class="headerlink" href="#Lion.backward"></a>
    
    

                            </div>
                            <div id="Lion.giveInputsToBackprop" class="classattr">
                                <div class="attr variable">
            <span class="name">giveInputsToBackprop</span>

        
    </div>
    <a class="headerlink" href="#Lion.giveInputsToBackprop"></a>
    
    

                            </div>
                            <div id="Lion.beta1" class="classattr">
                                <div class="attr variable">
            <span class="name">beta1</span>

        
    </div>
    <a class="headerlink" href="#Lion.beta1"></a>
    
    

                            </div>
                            <div id="Lion.momentum" class="classattr">
                                <div class="attr variable">
            <span class="name">momentum</span>

        
    </div>
    <a class="headerlink" href="#Lion.momentum"></a>
    
    

                            </div>
                            <div id="Lion.optimiser" class="classattr">
                                        <input id="Lion.optimiser-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">optimiser</span><span class="signature pdoc-code condensed">(<span class="param"><span class="bp">self</span>, </span><span class="param"><span class="n">inputData</span>, </span><span class="param"><span class="n">labels</span>, </span><span class="param"><span class="n">useBatches</span><span class="o">=</span><span class="kc">True</span>, </span><span class="param"><span class="n">batchSize</span><span class="o">=</span><span class="mi">32</span>, </span><span class="param"><span class="n">alpha</span><span class="o">=</span><span class="mf">0.001</span></span><span class="return-annotation">):</span></span>

                <label class="view-source-button" for="Lion.optimiser-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Lion.optimiser"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Lion.optimiser-12"><a href="#Lion.optimiser-12"><span class="linenos">12</span></a>    <span class="k">def</span><span class="w"> </span><span class="nf">optimiser</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">useBatches</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batchSize</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.001</span><span class="p">):</span>
</span><span id="Lion.optimiser-13"><a href="#Lion.optimiser-13"><span class="linenos">13</span></a>        <span class="k">if</span> <span class="n">useBatches</span><span class="p">:</span>
</span><span id="Lion.optimiser-14"><a href="#Lion.optimiser-14"><span class="linenos">14</span></a>            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimiserWithBatches</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">batchSize</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
</span><span id="Lion.optimiser-15"><a href="#Lion.optimiser-15"><span class="linenos">15</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="Lion.optimiser-16"><a href="#Lion.optimiser-16"><span class="linenos">16</span></a>            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimiserWithoutBatches</span><span class="p">(</span><span class="n">inputData</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
</span></pre></div>


    

                            </div>
                </section>
    </main>
<script>
    function escapeHTML(html) {
        return document.createElement('div').appendChild(document.createTextNode(html)).parentNode.innerHTML;
    }

    const originalContent = document.querySelector("main.pdoc");
    let currentContent = originalContent;

    function setContent(innerHTML) {
        let elem;
        if (innerHTML) {
            elem = document.createElement("main");
            elem.classList.add("pdoc");
            elem.innerHTML = innerHTML;
        } else {
            elem = originalContent;
        }
        if (currentContent !== elem) {
            currentContent.replaceWith(elem);
            currentContent = elem;
        }
    }

    function getSearchTerm() {
        return (new URL(window.location)).searchParams.get("search");
    }

    const searchBox = document.querySelector(".pdoc input[type=search]");
    searchBox.addEventListener("input", function () {
        let url = new URL(window.location);
        if (searchBox.value.trim()) {
            url.hash = "";
            url.searchParams.set("search", searchBox.value);
        } else {
            url.searchParams.delete("search");
        }
        history.replaceState("", "", url.toString());
        onInput();
    });
    window.addEventListener("popstate", onInput);


    let search, searchErr;

    async function initialize() {
        try {
            search = await new Promise((resolve, reject) => {
                const script = document.createElement("script");
                script.type = "text/javascript";
                script.async = true;
                script.onload = () => resolve(window.pdocSearch);
                script.onerror = (e) => reject(e);
                script.src = "search.js";
                document.getElementsByTagName("head")[0].appendChild(script);
            });
        } catch (e) {
            console.error("Cannot fetch pdoc search index");
            searchErr = "Cannot fetch search index.";
        }
        onInput();

        document.querySelector("nav.pdoc").addEventListener("click", e => {
            if (e.target.hash) {
                searchBox.value = "";
                searchBox.dispatchEvent(new Event("input"));
            }
        });
    }

    function onInput() {
        setContent((() => {
            const term = getSearchTerm();
            if (!term) {
                return null
            }
            if (searchErr) {
                return `<h3>Error: ${searchErr}</h3>`
            }
            if (!search) {
                return "<h3>Searching...</h3>"
            }

            window.scrollTo({top: 0, left: 0, behavior: 'auto'});

            const results = search(term);

            let html;
            if (results.length === 0) {
                html = `No search results for '${escapeHTML(term)}'.`
            } else {
                html = `<h4>${results.length} search result${results.length > 1 ? "s" : ""} for '${escapeHTML(term)}'.</h4>`;
            }
            for (let result of results.slice(0, 10)) {
                let doc = result.doc;
                let url = `${doc.modulename.replaceAll(".", "/")}.html`;
                if (doc.qualname) {
                    url += `#${doc.qualname}`;
                }

                let heading;
                switch (result.doc.kind) {
                    case "function":
                        if (doc.fullname.endsWith(".__init__")) {
                            heading = `<span class="name">${doc.fullname.replace(/\.__init__$/, "")}</span>${doc.signature}`;
                        } else {
                            heading = `<span class="def">${doc.funcdef}</span> <span class="name">${doc.fullname}</span>${doc.signature}`;
                        }
                        break;
                    case "class":
                        heading = `<span class="def">class</span> <span class="name">${doc.fullname}</span>`;
                        if (doc.bases)
                            heading += `<wbr>(<span class="base">${doc.bases}</span>)`;
                        heading += `:`;
                        break;
                    case "variable":
                        heading = `<span class="name">${doc.fullname}</span>`;
                        if (doc.annotation)
                            heading += `<span class="annotation">${doc.annotation}</span>`;
                        if (doc.default_value)
                            heading += `<span class="default_value"> = ${doc.default_value}</span>`;
                        break;
                    default:
                        heading = `<span class="name">${doc.fullname}</span>`;
                        break;
                }
                html += `
                        <section class="search-result">
                        <a href="${url}" class="attr ${doc.kind}">${heading}</a>
                        <div class="docstring">${doc.doc}</div>
                        </section>
                    `;

            }
            return html;
        })());
    }

    if (getSearchTerm()) {
        initialize();
        searchBox.value = getSearchTerm();
        onInput();
    } else {
        searchBox.addEventListener("focus", initialize, {once: true});
    }

    searchBox.addEventListener("keydown", e => {
        if (["ArrowDown", "ArrowUp", "Enter"].includes(e.key)) {
            let focused = currentContent.querySelector(".search-result.focused");
            if (!focused) {
                currentContent.querySelector(".search-result").classList.add("focused");
            } else if (
                e.key === "ArrowDown"
                && focused.nextElementSibling
                && focused.nextElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.nextElementSibling.classList.add("focused");
                focused.nextElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "ArrowUp"
                && focused.previousElementSibling
                && focused.previousElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.previousElementSibling.classList.add("focused");
                focused.previousElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "Enter"
            ) {
                focused.querySelector("a").click();
            }
        }
    });
</script></body>
</html>